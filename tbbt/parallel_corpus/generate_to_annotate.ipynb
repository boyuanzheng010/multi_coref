{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import spacy\n",
    "import csv\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load Parsed Corpus\n",
    "sm_parser = spacy.load('en_core_web_sm')\n",
    "\n",
    "with open('tbbt_en_zh.pkl', 'rb') as f_zh:\n",
    "    with open('tbbt_en_fa.pkl', 'rb') as f_fa:\n",
    "        zh = pkl.load(f_zh)\n",
    "        fa = pkl.load(f_fa)\n",
    "        inter_keys = set(zh.keys()) & set(fa.keys())\n",
    "\n",
    "data = {}\n",
    "with open('parsed_corpus.pkl', 'rb') as f:\n",
    "    parsed = pkl.load(f)\n",
    "    for item in inter_keys:\n",
    "        data[item] = parsed[item]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:01<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Regular Candidate Spans\n",
    "\n",
    "output = []\n",
    "for epi_key in data:\n",
    "    if epi_key != (1,1):\n",
    "        continue\n",
    "    episode = data[epi_key]\n",
    "    # Each scene contain on episode\n",
    "    for scene in tqdm(episode):\n",
    "        # Collect data to annotate\n",
    "        all_sentences = []\n",
    "        all_query_spans = []\n",
    "        all_candidate_spans = []\n",
    "\n",
    "        for i, utt in enumerate(scene):\n",
    "            if \"en_subtitles\" in utt:\n",
    "                utterance = \" \".join([x.strip().lstrip('-').lstrip().lstrip('.').lstrip() for x in utt['en_subtitles']])\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "                speaker = utt['speaker']\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "\n",
    "                all_sentences.append(sentence_tokens)\n",
    "\n",
    "                spans = list(set(utt['sm_noun_chunk']) | set(utt['berkeley_noun_chunk']) | set(utt['trf_noun_chunk']))\n",
    "                spans.sort(key=lambda x: x[1])\n",
    "\n",
    "                for span in spans:\n",
    "                    all_candidate_spans.append({\n",
    "                        \"sentenceIndex\": i,\n",
    "                        \"startToken\": span[1] + len(speaker_tokens) + 1,\n",
    "                        \"endToken\": span[2] + len(speaker_tokens) + 1\n",
    "                    })\n",
    "                    all_query_spans.append({\n",
    "                        \"sentenceIndex\": i,\n",
    "                        \"startToken\": span[1] + len(speaker_tokens) + 1,\n",
    "                        \"endToken\": span[2] + len(speaker_tokens) + 1\n",
    "                    })\n",
    "            else:\n",
    "                utterance = utt['utterance']\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "                speaker = utt['speaker']\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "\n",
    "                all_sentences.append(sentence_tokens)\n",
    "                all_candidate_spans.append({\n",
    "                        \"sentenceIndex\": i,\n",
    "                        \"startToken\": 0,\n",
    "                        \"endToken\": len(speaker) + 1\n",
    "                })\n",
    "        output.append({\n",
    "            \"sentences\": all_sentences,\n",
    "            \"querySpans\": all_query_spans,\n",
    "            \"candidateSpans\": all_candidate_spans,\n",
    "            \"clickSpans\": all_query_spans,\n",
    "        })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_all_possible_spans(sentIdx, sentLen, window_size):\n",
    "    all_possible_spans = []\n",
    "    for i in range(sentLen-window_size):\n",
    "        all_possible_spans.append({\n",
    "            \"sentenceIndex\": sentIdx,\n",
    "            \"startToken\": i,\n",
    "            \"endToken\": i+window_size\n",
    "        })\n",
    "    return all_possible_spans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3485\n",
      "106\n",
      "====================================================================================================\n",
      "4484\n",
      "123\n",
      "====================================================================================================\n",
      "12527\n",
      "379\n",
      "====================================================================================================\n",
      "821\n",
      "23\n",
      "====================================================================================================\n",
      "2506\n",
      "86\n",
      "====================================================================================================\n",
      "388\n",
      "14\n",
      "====================================================================================================\n",
      "1378\n",
      "47\n",
      "====================================================================================================\n",
      "417\n",
      "17\n",
      "====================================================================================================\n",
      "168\n",
      "6\n",
      "====================================================================================================\n",
      "1270\n",
      "41\n",
      "====================================================================================================\n",
      "Buttons\n",
      "1934\n",
      "58\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# All Spans\n",
    "# Use Sliding Window to gather all potential spans\n",
    "\n",
    "output = []\n",
    "for epi_key in data:\n",
    "    if epi_key != (1,1):\n",
    "        continue\n",
    "    episode = data[epi_key]\n",
    "    # Each scene contain on episode\n",
    "    for scene in episode:\n",
    "        # Collect data to annotate\n",
    "        all_sentences = []\n",
    "        all_query_spans = []\n",
    "        all_candidate_spans = []\n",
    "\n",
    "        for i, utt in enumerate(scene):\n",
    "            if \"en_subtitles\" in utt:\n",
    "                # Fetch parse Noun Phrases from former parsing result\n",
    "                utterance = \" \".join([x.strip().lstrip('-').lstrip().lstrip('.').lstrip() for x in utt['en_subtitles']])\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "                speaker = utt['speaker']\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "                all_sentences.append(sentence_tokens)\n",
    "                spans = list(set(utt['sm_noun_chunk']) | set(utt['berkeley_noun_chunk']) | set(utt['trf_noun_chunk']))\n",
    "                spans.sort(key=lambda x: x[1])\n",
    "\n",
    "                # Split NPs with Poesstive Pronoun into two parts\n",
    "                all_new_spans = []\n",
    "                for j, token in enumerate(sm_parser(utterance)):\n",
    "                    if token.tag_==\"PRP$\":\n",
    "                        for k, (word, start_idx, end_idx) in enumerate(spans):\n",
    "                            if start_idx <= j < end_idx:\n",
    "                                spans.pop(k)\n",
    "                                new_span_1 = (token.text, j, j+1)\n",
    "                                new_span_2 = (\" \".join(utterance_tokens[j+1: end_idx]), j+1, end_idx)\n",
    "                                all_new_spans.extend([new_span_1, new_span_2])\n",
    "                    if token.tag_==\"NNPS\":\n",
    "                        print(token)\n",
    "\n",
    "                for item in all_new_spans:\n",
    "                    spans.append(item)\n",
    "                spans.sort(key=lambda x: x[1])\n",
    "\n",
    "                # Merge overlapping spans into one maximum logical span\n",
    "                to_pop = []\n",
    "                for j, (word_0, start_idx_0, end_idx_0) in enumerate(spans):\n",
    "                    for k, (word_1, start_idx_1, end_idx_1) in enumerate(spans):\n",
    "                        if k==j:\n",
    "                            continue\n",
    "                        if (start_idx_1 >= start_idx_0) and (end_idx_1 <= end_idx_0):\n",
    "                            to_pop.append(spans[k])\n",
    "                for item in to_pop:\n",
    "                    spans.remove(item)\n",
    "\n",
    "                for span in spans:\n",
    "                    all_query_spans.append({\n",
    "                        \"sentenceIndex\": i,\n",
    "                        \"startToken\": span[1] + len(speaker_tokens) + 1,\n",
    "                        \"endToken\": span[2] + len(speaker_tokens) + 1\n",
    "                    })\n",
    "                # Gather all possible candidate spans\n",
    "                temp = []\n",
    "                for window_size in range(10):\n",
    "                    temp += get_all_possible_spans(i, len(sentence_tokens), window_size)\n",
    "                all_candidate_spans.extend(temp)\n",
    "            else:\n",
    "                utterance = utt['utterance']\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "                speaker = utt['speaker']\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "\n",
    "                all_sentences.append(sentence_tokens)\n",
    "\n",
    "                # Gather all possible candidate spans\n",
    "                temp = []\n",
    "                for window_size in range(10):\n",
    "                    temp += get_all_possible_spans(i, len(sentence_tokens), window_size)\n",
    "                all_candidate_spans.extend(temp)\n",
    "\n",
    "        print(len(all_candidate_spans))\n",
    "        print(len(all_query_spans))\n",
    "        # print(all_candidate_spans)\n",
    "        print(\"==\"*50)\n",
    "        output.append({\n",
    "            \"sentences\": all_sentences,\n",
    "            \"querySpans\": all_query_spans,\n",
    "            \"candidateSpans\": all_candidate_spans,\n",
    "            \"clickSpans\": all_query_spans,\n",
    "        })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "with open('win_15_no_overlap_mar_20.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}