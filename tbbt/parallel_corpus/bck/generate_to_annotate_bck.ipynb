{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import spacy\n",
    "import csv\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load Parsed Corpus\n",
    "sm_parser = spacy.load('en_core_web_sm')\n",
    "\n",
    "with open('tbbt_en_zh.pkl', 'rb') as f_zh:\n",
    "    with open('tbbt_en_fa.pkl', 'rb') as f_fa:\n",
    "        zh = pkl.load(f_zh)\n",
    "        fa = pkl.load(f_fa)\n",
    "        inter_keys = set(zh.keys()) & set(fa.keys())\n",
    "\n",
    "data = {}\n",
    "with open('parsed_corpus.pkl', 'rb') as f:\n",
    "    parsed = pkl.load(f)\n",
    "    for item in inter_keys:\n",
    "        data[item] = parsed[item]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leonard  There you go, Pad Thai, no peanuts.\n",
      "\n",
      "Howard  But does it have peanut oil?\n",
      "\n",
      "Leonard  Uh, I’m not sure, everyone keep an eye on Howard in case he starts to swell up.\n",
      "['Everyone keep an eye on Howard in case he starts to swell up.']\n",
      "\n",
      "Sheldon  Since it’s not bee season, you can have my epinephrine.\n",
      "[\"Since it's not bee season, you can have my epinephrine.\"]\n",
      "\n",
      "Raj  Are there any chopsticks?\n",
      "['- Any chopsticks?']\n",
      "\n",
      "Sheldon  You don’t need chopsticks, this is Thai food.\n",
      "[\"- Don't need chopsticks, this is Thai food.\"]\n",
      "\n",
      "Leonard  Here we go.\n",
      "['Here we go.']\n",
      "\n",
      "Sheldon  Thailand has had the fork since the latter half of the nineteenth century. Interestingly they don’t actually put the fork in their mouth, they use it to put the food on a spoon which then goes into their mouth.\n",
      "['Thailand has had the fork since the latter half of the 19th century.', \"They don't put the fork in their mouth, they use it to put the food on a spoon which then goes into their mouth.\"]\n",
      "\n",
      "Leonard  Ask him for a napkin, I dare you. (There is a knock on the door.) I’ll get it.\n",
      "['Ask him for a napkin, I dare you.']\n",
      "\n",
      "Howard  Do I look puffy? I feel puffy.\n",
      "['Do I look puffy?', 'I feel puffy.']\n",
      "\n",
      "Penny  Hey Leonard.\n",
      "\n",
      "Leonard  Oh, hi Penny.\n",
      "['- Hey, Leonard. - Oh, hi, Penny.']\n",
      "\n",
      "Penny  Am I interrupting.\n",
      "['- Am I interrupting?']\n",
      "\n",
      "Leonard  No.\n",
      "\n",
      "(off)  You’re not swelling, Howard.\n",
      "[\"You're not swelling, Howard.\"]\n",
      "\n",
      "(off)  No, no, look at my fingers, they’re like Vienna sausages.\n",
      "[\"No, look at my fingers, they're like Vienna sausages.\"]\n",
      "\n",
      "Penny  Sounds like you have company.\n",
      "['- Sounds like you have company.']\n",
      "\n",
      "Leonard  They’re not going anywhere. (Closes door, staying in hallway.) So, you’re coming home from work. That’s great. How was work.\n",
      "[\"- They're not going anywhere.\", \"So you're coming home from work. That's great. How was work?\"]\n",
      "\n",
      "Penny  Well, you know, it’s the Cheesecake Factory. People order cheesecake, and I bring it to them.\n",
      "[\"You know, it's a Cheesecake Factory people order cheesecake and I bring it to them.\"]\n",
      "\n",
      "Leonard  So, you sort of act as a carbohydrate delivery system.\n",
      "['So you kind of act as like a carbohydrate delivery system?']\n",
      "\n",
      "Penny  Yeah, call it whatever you want, I get minimum wage. Yeah, anyways, I was wondering if you could help me out with something, I was….\n",
      "['yeah, call it whatever you want, I get minimum wage.', 'Yeah. I was wondering if you could help me out-']\n",
      "\n",
      "Leonard  Yes.\n",
      "\n",
      "Penny  Oh. Okay, great, I’m having some furniture delivered tomorrow, and I may not be here, so…. (apartment door opens, Sheldon, Raj and Howard appear) Oh! Hel…hello!\n",
      "[\"Okay, great, I'm having some furniture delivered and I may not be here, so, oh-\", 'Hello.']\n",
      "\n",
      "Howard  (speaks a phrase in Russian).\n",
      "\n",
      "Penny  I’m sorry?\n",
      "[\"I'm sorry?\"]\n",
      "\n",
      "Howard  Haven’t you ever been told how beautiful you are in flawless Russian?\n",
      "[\"Haven't you ever been told how beautiful you are in flawless Russian?\"]\n",
      "\n",
      "Penny  No, I haven’t.\n",
      "[\"- No, I haven't.\"]\n",
      "\n",
      "Howard  Get used to it.\n",
      "['- Get used to it.']\n",
      "\n",
      "Penny  Yeah, I probably won’t, but… Hey Sheldon.\n",
      "[\"yeah, I probably won't. - Hey, Sheldon.\"]\n",
      "\n",
      "Sheldon  Hi.\n",
      "\n",
      "Penny  Hey Raj! (Raj looks uncomfortable) Still not talking to me, huh?\n",
      "['Still not talking to me, huh?']\n",
      "\n",
      "Sheldon  Don’t take it personally, it’s his pathology, he can’t talk to women.\n",
      "[\"Don't take it personally, it's his pathology. - He can't talk to women.\"]\n",
      "\n",
      "Howard  He can’t talk to attractive women, or in your case a cheesecake–scented Goddess!\n",
      "[\"- He can't talk to attractive women. Or in your case, a cheesecake-scented goddess.\"]\n",
      "\n",
      "Leonard  So, there’s gonna be some furniture delivered?\n",
      "[\"- There's gonna be some furniture delivered? - Yeah.\"]\n",
      "\n",
      "Penny  Yeah, yeah, if it gets here and I’m not here tomorrow could you just sign for it and have them put it in my apartment.\n",
      "[\"If it gets here and I'm not here, could you sign for it and put it in?\"]\n",
      "\n",
      "Leonard  Yeah, no problem.\n",
      "['- Yeah, no problem.']\n",
      "\n",
      "Penny  Great, here’s my spare key. Thank you.\n",
      "[\"- Great, here's my spare key.\", 'Thank you.']\n",
      "\n",
      "Leonard  Penny, wait.\n",
      "\n",
      "Penny  Yeah?\n",
      "\n",
      "Leonard  Um, if you don’t have any other plans, do you want to join us for Thai food and a Superman movie marathon?\n",
      "[\"If you don't have any other plans do you wanna join us for Thai food and a Superman movie marathon?\"]\n",
      "\n",
      "Penny  A marathon? Wow, how many Superman movies are there?\n",
      "['A marathon?', 'Wow, how many Superman movies are there?']\n",
      "\n",
      "Sheldon  You’re kidding, right?\n",
      "[\"You're kidding, right?\"]\n",
      "\n",
      "Penny  Yeah, I do like the one where Lois Lane falls from the helicopter and Superman swooshes down and catches her, which one was that? \n",
      "['I do like the one where Lois Lane falls from the helicopter and Superman catches her.', '- Which one was that?', '- One.']\n",
      "\n",
      "together  One. (Raj raises one finger). \n",
      "\n",
      "Sheldon  You realise that scene was rife with scientific inaccuracy.\n",
      "['You realize that scene was rife with scientific inaccuracy?']\n",
      "\n",
      "Penny  Yes, I know, men can’t fly.\n",
      "[\"Yes, I know, men can't fly.\"]\n",
      "\n",
      "Sheldon  Oh no, let’s assume that they can. Lois Lane is falling, accelerating at an initial rate of 32 feet per second per second. Superman swoops down to save her by reaching out two arms of steel. Miss Lane, who is now travelling at approximately 120 miles per hour, hits them, and is immediately sliced into three equal pieces. \n",
      "[\"No, let's assume that they can.\", 'Lois Lane is falling. Accelerating at an initial rate of 32 feet per second.', 'Superman swoops down to save her by reaching out two arms of steel.', 'Miss Lane, who is now traveling at approximately 120 miles an hour, hits them and is immediately sliced into three equal pieces.']\n",
      "\n",
      "Leonard  Unless, Superman matches her speed and decelerates. \n",
      "['Unless Superman matches her speed and decelerates.']\n",
      "\n",
      "Sheldon  In what space, sir, in what space? She’s two feet above the ground. Frankly, if he really loved her, he’d let her hit the pavement. It would be a more merciful death.\n",
      "['In what space, sir? In what space?', \"She's 2 feet above the ground.\", \"If he really loved her, he'd let her hit the pavement.\", \"- It would be a more merciful death. - That doesn't-\"]\n",
      "\n",
      "Leonard  Excuse me, your entire argument is predicated on the assumption that Superman’s flight is a feat of strength.\n",
      "[\"Excuse me, your entire argument is predicated on the assumption that Superman's flight is a feat of strength.\"]\n",
      "\n",
      "Sheldon  Are you listening to yourself, it is well established that Superman’s flight is a feat of strength, it is an extension of his ability to leap tall buildings, an ability he derives from Earth’s yellow Sun.\n",
      "['Are you listening to yourself?', \"Superman's flight is a feat of strength.\", 'An extension of his ability to leap buildings an ability he derives from exposure to sun-']\n",
      "\n",
      "Howard  Yeah, and you don’t have a problem with that, how does he fly at night.\n",
      "['How does he fly at night?']\n",
      "\n",
      "Sheldon  Uh, a combination of the moon’s solar reflection and the energy storage capacity of Kryptonian skin cells.\n",
      "[\"A combination of the moon's solar reflection and the energy storage capacity of Kryptonian skin cells.\"]\n",
      "\n",
      "Penny  I’m just going to go wash up.\n",
      "[\"I'm just gonna go wash up.\"]\n",
      "\n",
      "Leonard  I have 26 hundred comic books in there, I challenge you to find a single reference to Kryptonian skin cells.\n",
      "['I have 2600 comic books in there.', 'I challenge you to find a single reference to Kryptonian skin cells.']\n",
      "\n",
      "Sheldon  Challenge accepted. (Tries door.) We’re locked out.\n",
      "['Challenge accepted.', \"We're locked out.\"]\n",
      "\n",
      "Raj  Also, the pretty girl left.\n",
      "['Also, the pretty girl left.']\n",
      "\n",
      "Leonard  Okay, her apartment’s on the fourth floor but the elevator’s broken so you’re going to have to (delivery man leaves) oh, you’re just going to be done, okay, cool, thanks. I guess we’ll just bring it up ourselves.\n",
      "[\"Okay. Her apartment's on the fourth floor, elevator is broken, you're gonna-\", 'Okay, cool, thanks.', \"- I guess we'll just bring it up ourselves.\"]\n",
      "\n",
      "Sheldon  I hardly think so. \n",
      "['- I hardly think so.']\n",
      "\n",
      "Leonard  Why not?\n",
      "\n",
      "Sheldon  Well, we don’t have a dolly, or lifting belts, or any measurable upper body strength.\n",
      "[\"Well, we don't have a dolly or lifting belts, or any measurable upper-body strength.\"]\n",
      "\n",
      "Leonard  We don’t need strength, we’re physicists. We are the intellectual descendents of Archimedes. Give me a fulcrum and a lever and I can move the Earth, it’s just a matter… (starts to move package) I don’t have this… I don’t have this I don’t have this.\n",
      "[\"We don't need strength. We're physicists.\", 'We are the intellectual descendents of Archimedes.', 'Give me a fulcrum and a lever and I can move the Earth.', \"It's just a matter\", \"- I don't have this!\", 'I do not have this!']\n",
      "\n",
      "Sheldon  Archimedes would be so proud.\n",
      "['Archimedes would be so proud.']\n",
      "\n",
      "Leonard  Do you have any ideas?\n",
      "['Do you have any ideas?']\n",
      "\n",
      "Sheldon  Yes, but they all involve a green lantern and a power ring.\n",
      "['Yes, but they all involve a Green Lantern and a power ring.']\n",
      "\n",
      "Leonard  Easy, easy (package falls) Okay! Now we’ve got an inclined plane. The force required to lift is reduced by the sine of the angle of the stairs, call it thirty degrees, so about half. \n",
      "[\"Now we've got an inclined plane.\", 'Force required to lift is reduced by the sine of the angle of the stairs call it 30 degrees, so about half.']\n",
      "\n",
      "Sheldon  Exactly half.\n",
      "\n",
      "(snarkily)  Exactly half. Let’s push. Okay, see, it’s moving, this is easy, all in the math. \n",
      "[\"Okay. See, it's moving. This is easy.\", \"It's all in the math.\"]\n",
      "\n",
      "Sheldon  What’s your formula for the corner.\n",
      "[\"- What's your formula for the corner?\"]\n",
      "\n",
      "Leonard  What? Oh, okay, uh, okay, yeah, no problem, just come up here and help me pull and turn.\n",
      "['Oh, okay. Okay, yeah, no problem.', 'Just come up here, help me pull and turn.']\n",
      "\n",
      "Sheldon  Ah, gravity, thou art a heartless bitch.\n",
      "['Oh, gravity, thou art a heatless bitch.']\n",
      "\n",
      "Sheldon  You do understand that our efforts here will in no way increase the odds of you having sexual congress with this woman?\n",
      "['You do understand that our efforts here will in no way increase the odds of you having sexual congress with this woman?']\n",
      "\n",
      "Leonard  Men do things for women without expecting sex.\n",
      "['Men do things for women without expecting sex.']\n",
      "\n",
      "Sheldon  Yeah, those are men who just had sex.\n",
      "['Yeah, those would be men who just had sex.']\n",
      "\n",
      "Leonard  I’m doing this to be a good neighbour. In any case, there’s no way it could lower the odds.\n",
      "[\"I'm doing this to be a good neighbor.\", \"In any case, there's no way it could lower the odds.\"]\n",
      "\n",
      "Leonard  Almost there, almost there, almost there. (Lets go of package, it starts to slip down)\n",
      "\n",
      "Sheldon  No we’re not, no we’re not, no we’re not.\n",
      "[\"- No, we're not.\", \"- No, we're not.\"]\n",
      "\n",
      "Sheldon  Watch your fingers. Watch your fingers. Oh God, my fingers!\n",
      "['- Watch your fingers. - Yeah.', 'Oh, God, my fingers!']\n",
      "\n",
      "Leonard  You okay?\n",
      "\n",
      "Sheldon  No, it hurt… (looking around) Great Caesar’s Ghost, look at this place?\n",
      "[\"Great Caesar's ghost, look at this place.\"]\n",
      "\n",
      "Leonard  So Penny’s a little messy.\n",
      "['So Penny is a little messy.', 'A little messy?']\n",
      "\n",
      "Sheldon  A little messy? The Mandelbrot set of complex numbers is a little messy, this is chaos. Excuse me, explain to me an organisational system where a tray of flatware on a couch is valid. I’m just inferring that this is a couch, because the evidence suggests the coffee table’s having a tiny garage sale. \n",
      "['A little messy?', 'The Mandelbrot set of complex numbers is a little messy.', 'This is chaos.', 'Explain to me a system where a tray of flatware on a couch is valid?', \"I'm just inferring that this as a couch because the evidence suggests the coffee table's having a tiny garage sale.\"]\n",
      "\n",
      "Leonard  Did it ever occur to you that not everyone has the compulsive need to sort, organise and label the entire world around them?\n",
      "['Did it occur to you that not everyone has the compulsive need to sort organize and label the entire world around them?', 'No.']\n",
      "\n",
      "Sheldon  No.\n",
      "\n",
      "Leonard  Well they don’t. Hard as it may be for you to believe, most people don’t sort their breakfast cereal numerically by fibre content.\n",
      "[\"Well, they don't.\", \"Hard as it may be to believe most people don't sort their breakfast cereal numerically by fiber content.\", \"Excuse me, but I think we've both found that helpful at times.\"]\n",
      "\n",
      "Sheldon  Excuse me, but I think we’ve both found that helpful at times.\n",
      "\n",
      "Leonard  Come on, we should go.\n",
      "['- Come on, we should go.']\n",
      "\n",
      "Sheldon  Hang on.\n",
      "\n",
      "Leonard  What are you doing?\n",
      "[\"- I'm straightening up.\"]\n",
      "\n",
      "Sheldon  Straightening up.\n",
      "\n",
      "Leonard  Sheldon, this is not your home.\n",
      "['Sheldon, this is not your home.']\n",
      "\n",
      "Sheldon  This is not anyone’s home, this is a swirling vortex of entropy.\n",
      "[\"This is not anyone's home. This is a swirling vortex of entropy.\"]\n",
      "\n",
      "Leonard  When the transvestite lived here, you didn’t care how he kept the place.\n",
      "[\"When the transvestite lived here, you didn't care how he kept the place.\"]\n",
      "\n",
      "Sheldon  Because it was immaculate, I mean, you open that man’s closet, it was left to right, evening gowns, cocktail dresses, then his police uniforms.\n",
      "['Because it was immaculate.', \"I mean, you open that man's closet it was left to right, evening gowns, cocktail dresses and his police uniforms.\"]\n",
      "\n",
      "Leonard  What were you doing in his closet?\n",
      "['- What were you doing in his closet?']\n",
      "\n",
      "Sheldon  I helped run some cable for a webcam.\n",
      "\n",
      "(entering)  Hey guys.\n",
      "\n",
      "Leonard  Oh, hey Penny, this just arrived, we just brought this up, just now.\n",
      "['- I helped him run some cable for a webcam. - Hey, guys.', '- Oh, hey, Penny.']\n",
      "\n",
      "Penny  Great. Was it hard getting it up the stairs?\n",
      "['- Oh, hey, Penny.']\n",
      "\n",
      "Sheldon  (sucks in breath)\n",
      "['This just arrived. Brought this up. Just now.']\n",
      "\n",
      "Sheldon  No?\n",
      "['Great.']\n",
      "\n",
      "Leonard  No.\n",
      "\n",
      "Sheldon  No.\n",
      "\n",
      "Leonard  Well, we’ll get out of your hair.\n",
      "[\"- We'll get out of your hair. - Okay, great.\"]\n",
      "\n",
      "Penny  Oh, great, thank you again (she throws her jacket over the back of the sofa).\n",
      "\n",
      "Sheldon  Penny, I just want you to know that, you don’t have to live like this. I’m here for you.\n",
      "['Penny?', \"I just want you to know that you don't have to live like this.\"]\n",
      "\n",
      "Penny  What’s he talking about?\n",
      "\n",
      "Leonard  It’s a joke.\n",
      "\n",
      "Penny  I don’t get it.\n",
      "[\"I'm here for you.\"]\n",
      "\n",
      "Leonard  Yeah, he didn’t tell it right.\n",
      "[\"yeah, he didn't tell it right.\"]\n",
      "\n",
      "Leonard  Sheldon?\n",
      "\n",
      "Leonard  Sheldon? Hello? \n",
      "['Sheldon?']\n",
      "\n",
      "Leonard  Sheldon!\n",
      "['Sheldon?', '- Sheldon? - Shh!']\n",
      "\n",
      "Sheldon  Sssshhhh! Penny’s sleeping.\n",
      "[\"Penny's sleeping.\"]\n",
      "\n",
      "Leonard  Are you insane, you can’t just break into a woman’s apartment in the middle of the night and clean.\n",
      "[\"You can't break into a woman's apartment in the middle of the night and clean.\"]\n",
      "\n",
      "Sheldon  I had no choice. I couldn’t sleep knowing that just outside my bedroom was our living room, and just outside our living room was that hallway, and immediately adjacent to that hallway was… this.\n",
      "[\"I couldn't sleep knowing that just outside my bedroom was our living room and outside our living room was that hallway and immediately adjacent to that hallway was this.\"]\n",
      "\n",
      "Leonard  Do you realise that if Penny wakes up, there is no reasonable explanation as to why we’re here?\n",
      "[\"Do you realize that if Penny wakes up there is no reasonable explanation as to why we're here?\"]\n",
      "\n",
      "Sheldon  I just gave you a reasonable explanation.\n",
      "['I just gave you a reasonable explanation.']\n",
      "\n",
      "Leonard  No, no. You gave me an explanation, it’s reasonableness will be determined by a jury of your peers.\n",
      "['No, no. you gave me an explanation.', \"It's reasonableness will be determined by a jury of your peers.\"]\n",
      "\n",
      "Sheldon  Don’t be ridiculous. I have no peers.\n",
      "[\"Don't be ridiculous.\", 'I have no peers.']\n",
      "\n",
      "Leonard  Sheldon, we have to get out of here.\n",
      "['Sheldon, we have to get out of here.']\n",
      "\n",
      "Sheldon  You might want to speak in a lower register.\n",
      "['You might want to speak in a lower register.']\n",
      "\n",
      "Leonard  What?\n",
      "\n",
      "Sheldon  Evolution has made women sensitive to high pitched noises while they sleep, so that they’ll be roused by a crying baby. If you want to avoid waking her, speak in a lower register.\n",
      "[\"Evolution has made women sensitive to high-pitch noises while they sleep so that they'll be roused by a crying baby.\", 'If you want to avoid waking her, speak in a lower register.']\n",
      "\n",
      "Leonard  That’s ridiculous. (Penny snores again.)\n",
      "\n",
      "Sheldon  No, (lowering his voice dramatically,) that’s ridiculous.\n",
      "\n",
      "likewise)  Fine. I accept your premise, now please let’s go.\n",
      "\n",
      "Sheldon  I am not leaving until I’m done.\n",
      "\n",
      "Leonard  O-o-o-oh! (Collapses against wall).\n",
      "[\"I accept your premise. Now, please let's go.\"]\n",
      "\n",
      "Sheldon  If you have time to lean, you have time to clean.\n",
      "['If you have time to lean, you have time to clean.']\n",
      "\n",
      "Leonard  Oh, what the hell.\n",
      "\n",
      "Sheldon  Morning.\n",
      "\n",
      "Leonard  Morning. \n",
      "['Oh, what the hell.']\n",
      "\n",
      "Sheldon  I have to say, I slept splendidly. Granted, not long, but just deeply and well.\n",
      "['I have to say I slept splendidly.', 'Granted not long, but just deeply and well.']\n",
      "\n",
      "Leonard  I’m not surprised. A well known folk cure for insomnia is to break into your neighbour’s apartment and clean.\n",
      "[\"I'm not surprised.\", \"A well-known folk cure for insomnia is to break into your neighbor's apartment and clean.\"]\n",
      "\n",
      "Sheldon  Sarcasm?\n",
      "\n",
      "Leonard  You think?\n",
      "['Sarcasm?']\n",
      "\n",
      "Sheldon  Granted, my methods may have been somewhat unorthodox, but I think the end result will be a measurable enhancement of Penny’s quality of life.\n",
      "[\"Granted, my methods may have been somewhat unorthodox but the end result will be a measurable enhancement to Penny's quality of life.\"]\n",
      "\n",
      "Leonard  You know what, you’ve convinced me, maybe tonight we should sneak in and shampoo her carpet.\n",
      "[\"You've convinced me, maybe we should sneak in and shampoo her carpet.\"]\n",
      "\n",
      "Sheldon  You don’t think that crosses a line?\n",
      "[\"- You don't think that crosses a line?\"]\n",
      "\n",
      "Leonard  Yes! For God’s sake, Sheldon, do I have to hold up a sarcasm sign every time I open my mouth.\n",
      "[\"For God's sake, Sheldon do I have to hold up a sarcasm sign every time I open my mouth?\"]\n",
      "\n",
      "Sheldon  You have a sarcasm sign?\n",
      "['You have a sarcasm sign?']\n",
      "\n",
      "Leonard  No, I do not have a sarcasm sign.\n",
      "['No, I do not have a sarcasm sign.']\n",
      "\n",
      "Sheldon  Do you want some cereal. I’m feeling so good today I’m going to choose from the low fibre end of the shelf. Hello, Honey Puffs.\n",
      "['You want some cereal?', \"I feel so good today, I'm gonna choose from the low-fiber end of the shelf.\", 'Hello, Honey Puffs.', \"Penny's up.\"]\n",
      "\n",
      "off)  Son of a Bitch!\n",
      "\n",
      "Leonard  Penny’s up.\n",
      "\n",
      "off)  You sick, geeky bastards!\n",
      "['You sick, geeky bastards!']\n",
      "\n",
      "Leonard  How did she know it was us?\n",
      "['How did she know it was us?']\n",
      "\n",
      "Sheldon  I may have left a suggested organisational schematic for her bedroom closet.\n",
      "['I may have left a suggested organizational schematic for her bedroom closet.']\n",
      "\n",
      "off)  Leonard!\n",
      "\n",
      "Leonard  God, this is going to be bad.\n",
      "\n",
      "Sheldon  Goodbye, Honey Puffs, hello Big Bran.\n",
      "['Goodbye, Honey Puffs. Hello, Big Bran.']\n",
      "\n",
      "(entering)  You came into my apartment last night when I was sleeping?\n",
      "['You came into my apartment while I was sleeping?']\n",
      "\n",
      "Leonard  Yes, but, only to clean.\n",
      "['Yes, but only to clean.']\n",
      "\n",
      "Sheldon  Really more to organise, you’re not actually dirty, per se.\n",
      "[\"Really more to organize, you're not actually dirty, per se.\"]\n",
      "\n",
      "Penny  Give me back my key.\n",
      "['Give me back my key.']\n",
      "\n",
      "Leonard  I’m very, very sorry.\n",
      "\n",
      "Penny  Do you understand how creepy this is.\n",
      "['Do you understand how creepy this is?']\n",
      "\n",
      "Leonard  Oh, yes, we discussed it at length last night.\n",
      "['Yes, we discussed it at length last night.']\n",
      "\n",
      "Penny  In my apartment, while I was sleeping.\n",
      "['In my apartment? While I was sleeping?']\n",
      "\n",
      "Sheldon  And snoring. And that’s probably just a sinus infection, but it could be sleep apnoea, you might want to see an otolaryngologist. It’s a throat doctor.\n",
      "[\"And that's probably just a sinus infection.\", 'But it could be sleep apnea. You might wanna see an otorhinolaryngologist.']\n",
      "\n",
      "Penny  And what kind of doctor removes shoes from asses?\n",
      "['And what kind of doctor removes shoes from asses?']\n",
      "\n",
      "Sheldon  Depending on the depth, that’s either a proctologist or a general surgeon. (Leonard holds up a sign reading “Sarcasm”) Oh!\n",
      "[\"Depending on the depth that's either a proctologist, or a general surgeon.\"]\n",
      "\n",
      "Penny  God!\n",
      "\n",
      "Leonard  Okay, look, no Penny, I think what you’re feeling is perfectly valid, and maybe a little bit later today when you’re feeling a little bit less, for lack of a better word, violated, maybe we could talk about this some more.\n",
      "['- Okay, look.', 'No, Penny-', \"I think what you're feeling is valid and maybe later today when you're feeling less, for lack of a better word, violated maybe we can talk about this.\"]\n",
      "\n",
      "Penny  Stay away from me.\n",
      "\n",
      "Leonard  Sure, that’s another way to go.\n",
      "[\"- Sure. That's another way to go.\"]\n",
      "\n",
      "Sheldon  Penny, Penny, just to clarify because there will be a discussion when you leave, is your objection solely to our presence in the apartment while you were sleeping, or do you also object to the imposition of a new organisational paradigm. (Penny stares in disbelief, then leaves.) Well that was a little non-responsive.\n",
      "['Penny, Penny, hold on.', 'Just to clarify because there will be a discussion when you leave.', 'Is your objection solely to our presence in the apartment while you were sleeping or do you also object to the imposition of a new organizational paradigm?', 'Well, that was a little non-responsive.']\n",
      "\n",
      "Leonard  You are going to march yourself over there right now and apologise. (Sheldon laughs.) What’s funny?\n",
      "['You are going to march yourself over there right now and apologize.']\n",
      "\n",
      "Sheldon  That wasn’t sarcasm?\n",
      "[\"That wasn't sarcasm?\"]\n",
      "\n",
      "Leonard  No.\n",
      "\n",
      "Sheldon  Wooh, boy, you are all over the place this morning. (Knocks on Penny’s door.) I have a masters and two PhD’s, I should not have to do this.\n",
      "['Boy, you are all over the place this morning.', \"I have a Master's and two Ph. Ds, I should not have to do this.\"]\n",
      "\n",
      "door)  What?\n",
      "\n",
      "Sheldon  I am truly sorry for what happened last night, I take full responsibility. And I hope that it won’t colour your opinion of Leonard, who is not only a wonderful guy, but also, I hear, a gentle and thorough lover. (Penny closes door in his face.) I did what I could.\n",
      "[\"I'm truly sorry for what happened last night.\", 'I take full responsibility.', \"I hope that it won't color your opinion of Leonard who is not only a wonderful guy, but also, I hear, a gentle and thorough lover.\"]\n",
      "\n",
      "Penny  Hey Raj. (Raj stands looking uncomfortable.) Hey, listen, I don’t know if you heard about what happened last night with Leonard and Sheldon, but I’m really upset about it, I mean they just, they let themselves into my place, and then they cleaned it, I mean can you even believe that? How weird is that?\n",
      "[\"Hey, I don't know if you heard about what happened with Leonard and Sheldon but I'm really upset about it.\", 'I mean, they let themselves into my place, and then they cleaned it.', 'Can you even believe that? How weird is that?']\n",
      "\n",
      "talk)  Ooh, she’s standing very close to me. Oh my, she does smell good. What is that, vanilla?\n",
      "[\"She's standing very close to me. Oh, my.\", 'She does smell good.', 'What is that? Vanilla?']\n",
      "\n",
      "Penny  You know, where I come from, someone comes into your house at night, you shoot, okay? And you don’t shoot to wound. I mean, alright, my sister shot her husband, but it was an accident, they were drunk. What was I saying?\n",
      "['Where I come from, someone comes into your house at night, you shoot. Okay?', \"And you don't shoot to wound.\", 'I mean, all right, my sister shot her husband, but it was an accident, they were drunk.', 'What was I saying?']\n",
      "\n",
      "(internally)  She’s so chatty. Maybe my parents are right. Maybe I’d be better off with an Indian girl. We’d have the same cultural background, and my wife would sing to my children the same lullabies my mother sang to me. \n",
      "[\"She's so chatty.\", 'Maybe my parents were right.', \"Maybe I'd be better off with an Indian girl.\", \"We'd have the same cultural background and my wife could sing to my children the lullabies my mother sang to me.\"]\n",
      "\n",
      "Penny  It’s obvious that they meant well, but I’m just, I’m having a really rough time, like I said, I broke up with my boyfriend, and it’s just freaking me out.\n",
      "[\"It's obvious they meant well.\", \"I'm having a really rough time. Like I said I broke up with my boyfriend and it's freaking me out.\"]\n",
      "\n",
      "Penny  I mean, just because most of the men I’ve known in my life happen to be jerks, doesn’t mean I should just assume Leonard and Sheldon are. Right?\n",
      "[\"I mean, just because most of the men I've known happened to be jerks doesn't mean I should just assume Leonard and Sheldon are.\"]\n",
      "\n",
      "(internally)  She asked me a question. I should probably nod. (Does.)\n",
      "['She asked me a question. I should probably nod.']\n",
      "\n",
      "Penny  That’s exactly what I thought. Thank you for listening. You’re a doll. (She hugs him.)\n",
      "[\"That's exactly what I thought. Thank you for listening. You're a doll.\"]\n",
      "\n",
      "(internally)  Oh-oh. Turn your pelvis. (Does.)\n",
      "['Turn your pelvis.']\n",
      "\n",
      "mat)  Grab a napkin, homie. You just got served. \n",
      "['Grab a napkin, homey, you just got served.']\n",
      "\n",
      "Leonard  It’s fine. You win.\n",
      "[\"It's fine. you win.\"]\n",
      "\n",
      "Howard  What’s his problem?\n",
      "[\"What's his problem?\"]\n",
      "\n",
      "Sheldon  His imaginary girlfriend broke up with him.\n",
      "['His imaginary girlfriend broke up with him.']\n",
      "\n",
      "Howard  Been there.\n",
      "\n",
      "(entering)  Hello. Sorry I’m late. But I was in the hallway, chatting up Penny.\n",
      "[\"Sorry I'm late, but I was in the hallway, chatting up Penny.\"]\n",
      "\n",
      "Howard  Really? You? Rajesh Koothrapali, spoke to Penny?\n",
      "['you? Rajesh Koothrappali spoke to Penny?']\n",
      "\n",
      "Raj  Actually, I was less the chatter than the chattee.\n",
      "['Actually, I was less the chatter than the chattee.']\n",
      "\n",
      "Leonard  What did she say? Is she still mad at me?\n",
      "['What did she say?', 'Is she still mad at me?']\n",
      "\n",
      "Raj  Well, she was upset at first, but, probably because her sister shot somebody. Then there was something about you and… then she hugged me.\n",
      "['Well, she was upset at first but probably because her sister shot somebody.', 'Then there was something about you, and then she hugged me.']\n",
      "\n",
      "Howard  She hugged you? How did she hug you? (Raj hugs Howard.) Is that her perfume I smell?\n",
      "['She hugged you?', 'How did she hug you?', 'Is that her perfume I smell?']\n",
      "\n",
      "Raj  intoxicating, isn’t it?\n",
      "[\"Intoxicating, isn't it?\"]\n",
      "\n",
      "Penny  Hi.\n",
      "\n",
      "Leonard  Oh.\n",
      "\n",
      "Penny  What’s going on?\n",
      "[\"What's going on? Uh...\"]\n",
      "\n",
      "Leonard  Um, here’s the thing. (Reads from note.) Penny. Just as Oppenheimer came to regret his contributions to the first atomic bomb, so too I regret my participation in what was, at the very least, an error in judgement. The hallmark of the great human experiment is the willingness to recognise one’s mistakes. Some mistakes, such as Madame Curie’s discovery of Radium turned out to have great scientific potential even though she would later die a slow, painful death from radiation poisoning. Another example, from the field of ebola research….\n",
      "[\"Here's the thing:\", '\"Penny, just as Oppenheimer came to regret his contributions to the first atomic bomb so too I regret my participation in what was at the very least, an error in judgment.', \"The hallmark of the great human experiment is the willingness to recognize one's mistakes.\", \"Madam Curie's discovery of radium turned out to have great potential even though she would later die a slow, painful death from radiation poisoning.\", 'Another example, from the field of Ebola research-\"']\n",
      "\n",
      "Penny  Leonard.\n",
      "\n",
      "Leonard  Yeah.\n",
      "\n",
      "him)  We’re okay. (Kisses him on cheek. Closes door. Leonard looks happy, walks back across hallway and straight into the apartment door.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for scene in zh[(1,2)]:\n",
    "    for utt in scene:\n",
    "        print(utt['speaker'], utt['utterance'])\n",
    "        if \"en_subtitles\" in utt:\n",
    "            print(utt['en_subtitles'])\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(zh[(1,2)]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:01<00:00,  5.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Regular Candidate Spans\n",
    "\n",
    "output = []\n",
    "for epi_key in data:\n",
    "    if epi_key != (1,1):\n",
    "        continue\n",
    "    episode = data[epi_key]\n",
    "    # Each scene contain on episode\n",
    "    for scene in tqdm(episode):\n",
    "        # Collect data to annotate\n",
    "        all_sentences = []\n",
    "        all_query_spans = []\n",
    "        all_candidate_spans = []\n",
    "\n",
    "        for i, utt in enumerate(scene):\n",
    "            if \"en_subtitles\" in utt:\n",
    "                utterance = \" \".join([x.strip().lstrip('-').lstrip().lstrip('.').lstrip() for x in utt['en_subtitles']])\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "                speaker = utt['speaker']\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "\n",
    "                all_sentences.append(sentence_tokens)\n",
    "\n",
    "                spans = list(set(utt['sm_noun_chunk']) | set(utt['berkeley_noun_chunk']) | set(utt['trf_noun_chunk']))\n",
    "                spans.sort(key=lambda x: x[1])\n",
    "\n",
    "                for span in spans:\n",
    "                    all_candidate_spans.append({\n",
    "                        \"sentenceIndex\": i,\n",
    "                        \"startToken\": span[1] + len(speaker_tokens) + 1,\n",
    "                        \"endToken\": span[2] + len(speaker_tokens) + 1\n",
    "                    })\n",
    "                    all_query_spans.append({\n",
    "                        \"sentenceIndex\": i,\n",
    "                        \"startToken\": span[1] + len(speaker_tokens) + 1,\n",
    "                        \"endToken\": span[2] + len(speaker_tokens) + 1\n",
    "                    })\n",
    "            else:\n",
    "                utterance = utt['utterance']\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "                speaker = utt['speaker']\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "\n",
    "                all_sentences.append(sentence_tokens)\n",
    "                all_candidate_spans.append({\n",
    "                        \"sentenceIndex\": i,\n",
    "                        \"startToken\": 0,\n",
    "                        \"endToken\": len(speaker) + 1\n",
    "                })\n",
    "        output.append({\n",
    "            \"sentences\": all_sentences,\n",
    "            \"querySpans\": all_query_spans,\n",
    "            \"candidateSpans\": all_candidate_spans,\n",
    "            \"clickSpans\": all_query_spans,\n",
    "        })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_all_possible_spans(sentIdx, sentLen, window_size):\n",
    "    all_possible_spans = []\n",
    "    for i in range(sentLen-window_size):\n",
    "        all_possible_spans.append({\n",
    "            \"sentenceIndex\": sentIdx,\n",
    "            \"startToken\": i,\n",
    "            \"endToken\": i+window_size\n",
    "        })\n",
    "    return all_possible_spans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FYI, secret-keeping. Hand-holding, not a fan. Hammerhead shark, I love that thing. Yeah, it's another fish with a tool on its head.\n",
      "\n",
      "PS4 or Xbox One?\n",
      "\n",
      "PENNY: Come on, Sheldon. There are plenty of smart people who don't have mental problems.\n",
      "\n",
      "PENNY: I don't know. Maybe it's the local cuisine. Okay, well, it's nice to meet you. Just gonna set you on down over here. And FYI, you'd be lucky to have me as a daughter-in-law.\n",
      "\n",
      "NASA called. The telescope mount I installed on the space station got damaged, and they want me to go back up and fix it.\n",
      "\n",
      "KEVIN: Oh, I'm hanging up now.\n",
      "\n",
      "PG. Some scenes may be too intense for younger viewers.\n",
      "\n",
      "MRS. WOLOWITZ: Howard, I found my girdle! It was in the dryer!\n",
      "\n",
      "FYI-- his toothbrush is the red one in the Plexiglas case under the UV light.\n",
      "\n",
      "FYI, I had a doughnut for breakfast, you jerk.\n",
      "\n",
      "PENNY: You know, they have DVDs over there.\n",
      "\n",
      "LEONARD: Anyway we can e-mail and I think the phone connections are good.\n",
      "\n",
      "DR. KOOTHRAPPALI: Who's that? Oh, my God! There's someone in your apartment. Call the police in America!\n",
      "\n",
      "WOLOWITZ: I'm not a baby! I'm a grown man, and I made the bed. Now where's my star?\n",
      "\n",
      "KOOTHRAPPALI: They're in my shirt!\n",
      "\n",
      "AMY: Wow, the store looks great.\n",
      "\n",
      "DEFCON 5? Well, there's no need to rush.\n",
      "\n",
      "DEFCON 5 means no danger. DEFCON 1 is a crisis.\n",
      "\n",
      "MRS. WOLOWITZ [SHOUTING O VER PHONE]: Tell her we're going to the Olive Garden.\n",
      "\n",
      "MRS. WOLOWITZ:\n",
      "\n",
      "SHELDON RAJESH: Hey.\n",
      "\n",
      "LEONARD: Right there, right there,\n",
      "\n",
      "MRS. WOLOWITZ Howard, dinner's ready! I just don't want to be yet another flip-flop fatality.\n",
      "\n",
      "MRS. WOLOWITZ Howard, I made cookies for you and your little friends!\n",
      "\n",
      "LEONARD: I know, I'm familiar with you.\n",
      "\n",
      "SHELDON: This is great. I'm in the real world of ordinary people just living their ordinary, colorless, work-a-day lives.\n",
      "\n",
      "PMS? - The eta meson.\n",
      "\n",
      "AA? - And, of course, the answer is technetium.\n",
      "\n",
      "AA, I need your answer.\n",
      "\n",
      "OK, how about we toast your newfound freedom?\n",
      "\n",
      "OK, I'll say it. (CLEARS THROAT) I was on the way to pick her up. My stomach felt a little funny. When I got there, I asked if I could use her bathroom.\n",
      "\n",
      "LEONARD: Oh, come on. Some battles you win, some battles you lose.\n",
      "\n",
      "HOWARD: Yes, but you don't have to lose to Kyle Bernstein's bar mitzvah party.\n",
      "\n",
      "TV, film, DD, manga. Greek, Roman, Norse gods...\n",
      "\n",
      "SHELDON: Keep it up. I got nowhere else to be.\n",
      "\n",
      "LEONARD: Wait a minute. Sheldon spent a whole day with James Earl Jones and never told any of us.\n",
      "\n",
      "R2-D2 and C-3PO.\n",
      "\n",
      "SHELDON: It was a compromise.\n",
      "\n",
      "MRS. WOLOWITZ: Holy Moses, how much liquid can be in one tokus?\n",
      "\n",
      "EZ Aquarii B... EZ Aquarii C...\n",
      "\n",
      "SHELDON: Wait. Whoa, whoa. Is \"placed\" right?\n",
      "\n",
      "MRS. WOLOWITZ: Oh, good, I got that new stain stick to try out!\n",
      "\n",
      "SHELDON: It has been some time since we've had a woman take her clothes off in our apartment.\n",
      "\n",
      "LEONARD: This is it. I'll do the talking.\n",
      "\n",
      "SHELDON: Well, you got me out of my pants.\n",
      "\n",
      "SHELDON: I think you're high on paint fumes. And, boy, that's a lot of Band-Aids.\n",
      "\n",
      "PENNY: Get away from me, or I swear to God, I will rip out what's left of your pubes.\n",
      "\n",
      "PENNY: Guys, I think I need to go to the emergency room.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epi_key in data:\n",
    "    episode = data[epi_key]\n",
    "    # Each scene contain on episode\n",
    "    # print(epi_key)\n",
    "    for scene in episode:\n",
    "        # Collect data to annotate\n",
    "        all_sentences = []\n",
    "        all_query_spans = []\n",
    "        all_candidate_spans = []\n",
    "\n",
    "        for i, utt in enumerate(scene):\n",
    "            if \"en_subtitles\" in utt:\n",
    "                # Fetch parse Noun Phrases from former parsing result\n",
    "                utterance = \" \".join([x.strip().lstrip('-').lstrip().lstrip('.').lstrip() for x in utt['en_subtitles']])\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "\n",
    "                if (utterance_tokens[0].isupper() and len(utterance_tokens[0])!=1) or (utterance_tokens[1]==\":\"):\n",
    "                    print(utterance)\n",
    "                    print()\n",
    "\n",
    "                speaker = utt['speaker'].strip().strip(\"(\").strip(\")\").strip().strip(\".\").strip().strip(\":\")\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENNY: Come on, Sheldon. There are plenty of smart people who don't have mental problems.\n",
      "\n",
      "PENNY: I don't know. Maybe it's the local cuisine. Okay, well, it's nice to meet you. Just gonna set you on down over here. And FYI, you'd be lucky to have me as a daughter-in-law.\n",
      "\n",
      "KEVIN: Oh, I'm hanging up now.\n",
      "\n",
      "A: surprised you know that. B: I wanted to look like a sexy graduate for you.\n",
      "\n",
      "PENNY: You know, they have DVDs over there.\n",
      "\n",
      "LEONARD: Anyway we can e-mail and I think the phone connections are good.\n",
      "\n",
      "WOLOWITZ: I'm not a baby! I'm a grown man, and I made the bed. Now where's my star?\n",
      "\n",
      "KOOTHRAPPALI: They're in my shirt!\n",
      "\n",
      "AMY: Wow, the store looks great.\n",
      "\n",
      "LEONARD: Right there, right there,\n",
      "\n",
      "LEONARD: I know, I'm familiar with you.\n",
      "\n",
      "SHELDON: This is great. I'm in the real world of ordinary people just living their ordinary, colorless, work-a-day lives.\n",
      "\n",
      "LEONARD: Oh, come on. Some battles you win, some battles you lose.\n",
      "\n",
      "HOWARD: Yes, but you don't have to lose to Kyle Bernstein's bar mitzvah party.\n",
      "\n",
      "SHELDON: Keep it up. I got nowhere else to be.\n",
      "\n",
      "LEONARD: Wait a minute. Sheldon spent a whole day with James Earl Jones and never told any of us.\n",
      "\n",
      "SHELDON: It was a compromise.\n",
      "\n",
      "SHELDON: Wait. Whoa, whoa. Is \"placed\" right?\n",
      "\n",
      "Bernadette: So, your boyfriend's A fixer-upper. Most of them are. I mean, look at this guy. You think he came like this? When I met him, he was a hot, goofy mess. Now, he's been to space.\n",
      "\n",
      "SHELDON: It has been some time since we've had a woman take her clothes off in our apartment.\n",
      "\n",
      "LEONARD: This is it. I'll do the talking.\n",
      "\n",
      "SHELDON: Well, you got me out of my pants.\n",
      "\n",
      "SHELDON: I think you're high on paint fumes. And, boy, that's a lot of Band-Aids.\n",
      "\n",
      "PENNY: Get away from me, or I swear to God, I will rip out what's left of your pubes.\n",
      "\n",
      "PENNY: Guys, I think I need to go to the emergency room.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epi_key in data:\n",
    "    episode = data[epi_key]\n",
    "    # Each scene contain on episode\n",
    "    # print(epi_key)\n",
    "    for scene in episode:\n",
    "        # Collect data to annotate\n",
    "        all_sentences = []\n",
    "        all_query_spans = []\n",
    "        all_candidate_spans = []\n",
    "\n",
    "        for i, utt in enumerate(scene):\n",
    "            if \"en_subtitles\" in utt:\n",
    "                # Fetch parse Noun Phrases from former parsing result\n",
    "                utterance = \" \".join([x.strip().lstrip('-').lstrip().lstrip('.').lstrip() for x in utt['en_subtitles']])\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "\n",
    "                if (utterance_tokens[1]==\":\"):\n",
    "                    print(utterance)\n",
    "                    print()\n",
    "\n",
    "                speaker = utt['speaker'].strip().strip(\"(\").strip(\")\").strip().strip(\".\").strip().strip(\":\")\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "    # print('=='*50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENNY: Come on, Sheldon. There are plenty of smart people who don't have mental problems.\n",
      "['PENNY'] 1\n",
      "Come on , Sheldon . There are plenty of smart people who do n't have mental problems .\n",
      "\n",
      "PENNY: I don't know. Maybe it's the local cuisine. Okay, well, it's nice to meet you. Just gonna set you on down over here. And FYI, you'd be lucky to have me as a daughter-in-law.\n",
      "['PENNY'] 1\n",
      "I do n't know . Maybe it 's the local cuisine . Okay , well , it 's nice to meet you . Just gon na set you on down over here . And FYI , you 'd be lucky to have me as a daughter - in - law .\n",
      "\n",
      "KEVIN: Oh, I'm hanging up now.\n",
      "['KEVIN'] 1\n",
      "Oh , I 'm hanging up now .\n",
      "\n",
      "MRS. WOLOWITZ: Howard, I found my girdle! It was in the dryer!\n",
      "['MRS', '.', 'WOLOWITZ'] 3\n",
      "Howard , I found my girdle ! It was in the dryer !\n",
      "\n",
      "A: surprised you know that. B: I wanted to look like a sexy graduate for you.\n",
      "['A'] 1\n",
      "surprised you know that . B : I wanted to look like a sexy graduate for you .\n",
      "\n",
      "PENNY: You know, they have DVDs over there.\n",
      "['PENNY'] 1\n",
      "You know , they have DVDs over there .\n",
      "\n",
      "LEONARD: Anyway we can e-mail and I think the phone connections are good.\n",
      "['LEONARD'] 1\n",
      "Anyway we can e - mail and I think the phone connections are good .\n",
      "\n",
      "DR. KOOTHRAPPALI: Who's that? Oh, my God! There's someone in your apartment. Call the police in America!\n",
      "['DR', '.', 'KOOTHRAPPALI'] 3\n",
      "Who 's that ? Oh , my God ! There 's someone in your apartment . Call the police in America !\n",
      "\n",
      "WOLOWITZ: I'm not a baby! I'm a grown man, and I made the bed. Now where's my star?\n",
      "['WOLOWITZ'] 1\n",
      "I 'm not a baby ! I 'm a grown man , and I made the bed . Now where 's my star ?\n",
      "\n",
      "KOOTHRAPPALI: They're in my shirt!\n",
      "['KOOTHRAPPALI'] 1\n",
      "They 're in my shirt !\n",
      "\n",
      "AMY: Wow, the store looks great.\n",
      "['AMY'] 1\n",
      "Wow , the store looks great .\n",
      "\n",
      "MRS. WOLOWITZ [SHOUTING O VER PHONE]: Tell her we're going to the Olive Garden.\n",
      "['MRS', '.', 'WOLOWITZ', '[', 'SHOUTING', 'O', 'VER', 'PHONE', ']'] 9\n",
      "Tell her we 're going to the Olive Garden .\n",
      "\n",
      "MRS. WOLOWITZ:\n",
      "['MRS', '.', 'WOLOWITZ'] 3\n",
      "\n",
      "\n",
      "SHELDON RAJESH: Hey.\n",
      "['SHELDON', 'RAJESH'] 2\n",
      "Hey .\n",
      "\n",
      "LEONARD: Right there, right there,\n",
      "['LEONARD'] 1\n",
      "Right there , right there ,\n",
      "\n",
      "LEONARD: I know, I'm familiar with you.\n",
      "['LEONARD'] 1\n",
      "I know , I 'm familiar with you .\n",
      "\n",
      "SHELDON: This is great. I'm in the real world of ordinary people just living their ordinary, colorless, work-a-day lives.\n",
      "['SHELDON'] 1\n",
      "This is great . I 'm in the real world of ordinary people just living their ordinary , colorless , work - a - day lives .\n",
      "\n",
      "LEONARD: Oh, come on. Some battles you win, some battles you lose.\n",
      "['LEONARD'] 1\n",
      "Oh , come on . Some battles you win , some battles you lose .\n",
      "\n",
      "HOWARD: Yes, but you don't have to lose to Kyle Bernstein's bar mitzvah party.\n",
      "['HOWARD'] 1\n",
      "Yes , but you do n't have to lose to Kyle Bernstein 's bar mitzvah party .\n",
      "\n",
      "SHELDON: Keep it up. I got nowhere else to be.\n",
      "['SHELDON'] 1\n",
      "Keep it up . I got nowhere else to be .\n",
      "\n",
      "LEONARD: Wait a minute. Sheldon spent a whole day with James Earl Jones and never told any of us.\n",
      "['LEONARD'] 1\n",
      "Wait a minute . Sheldon spent a whole day with James Earl Jones and never told any of us .\n",
      "\n",
      "SHELDON: It was a compromise.\n",
      "['SHELDON'] 1\n",
      "It was a compromise .\n",
      "\n",
      "MRS. WOLOWITZ: Holy Moses, how much liquid can be in one tokus?\n",
      "['MRS', '.', 'WOLOWITZ'] 3\n",
      "Holy Moses , how much liquid can be in one tokus ?\n",
      "\n",
      "SHELDON: Wait. Whoa, whoa. Is \"placed\" right?\n",
      "['SHELDON'] 1\n",
      "Wait . Whoa , whoa . Is \" placed \" right ?\n",
      "\n",
      "MRS. WOLOWITZ: Oh, good, I got that new stain stick to try out!\n",
      "['MRS', '.', 'WOLOWITZ'] 3\n",
      "Oh , good , I got that new stain stick to try out !\n",
      "\n",
      "SHELDON: It has been some time since we've had a woman take her clothes off in our apartment.\n",
      "['SHELDON'] 1\n",
      "It has been some time since we 've had a woman take her clothes off in our apartment .\n",
      "\n",
      "LEONARD: This is it. I'll do the talking.\n",
      "['LEONARD'] 1\n",
      "This is it . I 'll do the talking .\n",
      "\n",
      "SHELDON: Well, you got me out of my pants.\n",
      "['SHELDON'] 1\n",
      "Well , you got me out of my pants .\n",
      "\n",
      "SHELDON: I think you're high on paint fumes. And, boy, that's a lot of Band-Aids.\n",
      "['SHELDON'] 1\n",
      "I think you 're high on paint fumes . And , boy , that 's a lot of Band - Aids .\n",
      "\n",
      "PENNY: Get away from me, or I swear to God, I will rip out what's left of your pubes.\n",
      "['PENNY'] 1\n",
      "Get away from me , or I swear to God , I will rip out what 's left of your pubes .\n",
      "\n",
      "PENNY: Guys, I think I need to go to the emergency room.\n",
      "['PENNY'] 1\n",
      "Guys , I think I need to go to the emergency room .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epi_key in data:\n",
    "    episode = data[epi_key]\n",
    "    # Each scene contain on episode\n",
    "    # print(epi_key)\n",
    "    for scene in episode:\n",
    "        # Collect data to annotate\n",
    "        all_sentences = []\n",
    "        all_query_spans = []\n",
    "        all_candidate_spans = []\n",
    "\n",
    "        for i, utt in enumerate(scene):\n",
    "            if \"en_subtitles\" in utt:\n",
    "                # Fetch parse Noun Phrases from former parsing result\n",
    "                utterance = \" \".join([x.strip().lstrip('-').lstrip().lstrip('.').lstrip() for x in utt['en_subtitles']])\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "\n",
    "                if \":\" in utterance_tokens:\n",
    "                    colon_idx = utterance_tokens.index(\":\")\n",
    "                    prefix = utterance_tokens[: colon_idx]\n",
    "                    if \" \".join(prefix).isupper():\n",
    "                        print(utterance)\n",
    "                        print(prefix, len(prefix))\n",
    "                        print(\" \".join(utterance_tokens[len(prefix)+1:]))\n",
    "                        print()\n",
    "\n",
    "                speaker = utt['speaker'].strip().strip(\"(\").strip(\")\").strip().strip(\".\").strip().strip(\":\")\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEONARD: This is it. I'll do the talking.\n",
      "['LEONARD', ':', 'This', 'is', 'it', '.', 'I', \"'ll\", 'do', 'the', 'talking', '.']\n",
      "True\n",
      ": True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "utterance = \"LEONARD: This is it. I'll do the talking.\"\n",
    "utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "\n",
    "print(utterance)\n",
    "print(utterance_tokens)\n",
    "print(utterance_tokens[0].isupper())\n",
    "print(utterance_tokens[1], utterance_tokens[1]==\":\")\n",
    "\n",
    "print(utterance_tokens[0].isupper() and utterance_tokens[1]==\":\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3485\n",
      "106\n",
      "====================================================================================================\n",
      "4484\n",
      "123\n",
      "====================================================================================================\n",
      "12481\n",
      "378\n",
      "====================================================================================================\n",
      "821\n",
      "23\n",
      "====================================================================================================\n",
      "2506\n",
      "86\n",
      "====================================================================================================\n",
      "378\n",
      "14\n",
      "====================================================================================================\n",
      "1369\n",
      "47\n",
      "====================================================================================================\n",
      "397\n",
      "17\n",
      "====================================================================================================\n",
      "168\n",
      "6\n",
      "====================================================================================================\n",
      "1250\n",
      "40\n",
      "====================================================================================================\n",
      "1934\n",
      "59\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# All Spans\n",
    "# Use Sliding Window to gather all potential spans\n",
    "\n",
    "output = []\n",
    "for epi_key in data:\n",
    "    if epi_key != (1,1):\n",
    "        continue\n",
    "    episode = data[epi_key]\n",
    "    # Each scene contain on episode\n",
    "    for scene in episode:\n",
    "        # Collect data to annotate\n",
    "        all_sentences = []\n",
    "        all_query_spans = []\n",
    "        all_candidate_spans = []\n",
    "\n",
    "        for i, utt in enumerate(scene):\n",
    "            if \"en_subtitles\" in utt:\n",
    "                prefix_length = None\n",
    "                # Fetch parse Noun Phrases from former parsing result\n",
    "                utterance = \" \".join([x.strip().lstrip('-').lstrip().lstrip('.').lstrip() for x in utt['en_subtitles']])\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "\n",
    "                if \":\" in utterance_tokens:\n",
    "                    colon_idx = utterance_tokens.index(\":\")\n",
    "                    prefix = utterance_tokens[: colon_idx]\n",
    "                    if \" \".join(prefix).isupper():\n",
    "                        prefix_length = len(prefix)+1\n",
    "                        utterance_tokens = utterance_tokens[prefix_length:]\n",
    "                        utterance = \" \".join(utterance_tokens)\n",
    "\n",
    "\n",
    "                speaker = utt['speaker'].strip().strip(\"(\").strip(\")\").strip().strip(\".\").strip().strip(\":\")\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "                all_sentences.append(sentence_tokens)\n",
    "                spans = list(set(utt['sm_noun_chunk']) | set(utt['berkeley_noun_chunk']) | set(utt['trf_noun_chunk']))\n",
    "\n",
    "                spans.sort(key=lambda x: x[1])\n",
    "                # Merge overlapping spans into one maximum logical span\n",
    "                to_pop = []\n",
    "                for j, (word_0, start_idx_0, end_idx_0) in enumerate(spans):\n",
    "                    for k, (word_1, start_idx_1, end_idx_1) in enumerate(spans):\n",
    "                        if k==j:\n",
    "                            continue\n",
    "                        if (start_idx_1 >= start_idx_0) and (end_idx_1 <= end_idx_0):\n",
    "                            to_pop.append(spans[k])\n",
    "                for item in to_pop:\n",
    "                    spans.remove(item)\n",
    "\n",
    "                # Split NPs with Poesstive Pronoun into two parts\n",
    "                poessives = []\n",
    "                for j, token in enumerate(sm_parser(utterance)):\n",
    "                    if token.tag_==\"PRP$\":\n",
    "                        for k, (word, start_idx, end_idx) in enumerate(spans):\n",
    "                            if start_idx <= j < end_idx:\n",
    "                                new_span_1 = (token.text, j, j+1)\n",
    "                                poessives.append(new_span_1)\n",
    "                    if token.tag_==\"NNPS\":\n",
    "                        pass\n",
    "\n",
    "                for item in poessives:\n",
    "                    spans.append(item)\n",
    "                spans.sort(key=lambda x: x[1])\n",
    "\n",
    "                for span in spans:\n",
    "                    all_query_spans.append({\n",
    "                        \"sentenceIndex\": i,\n",
    "                        \"startToken\": span[1] + len(speaker_tokens) + 1,\n",
    "                        \"endToken\": span[2] + len(speaker_tokens) + 1\n",
    "                    })\n",
    "                    # if prefix_length:\n",
    "                    #     all_query_spans.append({\n",
    "                    #     \"sentenceIndex\": i,\n",
    "                    #     \"startToken\": span[1] + len(speaker_tokens) + 1 - prefix_length,\n",
    "                    #     \"endToken\": span[2] + len(speaker_tokens) + 1 - prefix_length\n",
    "                    # })\n",
    "                    # else:\n",
    "                    #     all_query_spans.append({\n",
    "                    #     \"sentenceIndex\": i,\n",
    "                    #     \"startToken\": span[1] + len(speaker_tokens) + 1,\n",
    "                    #     \"endToken\": span[2] + len(speaker_tokens) + 1\n",
    "                    # })\n",
    "\n",
    "                # Gather all possible candidate spans\n",
    "                temp = []\n",
    "                for window_size in range(10):\n",
    "                    temp += get_all_possible_spans(i, len(sentence_tokens), window_size)\n",
    "                all_candidate_spans.extend(temp)\n",
    "            else:\n",
    "                prefix_length = None\n",
    "                utterance = utt['utterance']\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "\n",
    "                if \":\" in utterance_tokens:\n",
    "                    colon_idx = utterance_tokens.index(\":\")\n",
    "                    prefix = utterance_tokens[: colon_idx]\n",
    "                    if \" \".join(prefix).isupper():\n",
    "                        prefix_length = len(prefix)+1\n",
    "\n",
    "                speaker = utt['speaker'].strip().strip(\"(\").strip(\")\").strip().strip(\".\").strip().strip(\":\")\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "\n",
    "                all_sentences.append(sentence_tokens)\n",
    "\n",
    "                # Gather all possible candidate spans\n",
    "                temp = []\n",
    "                for window_size in range(10):\n",
    "                    temp += get_all_possible_spans(i, len(sentence_tokens), window_size)\n",
    "                all_candidate_spans.extend(temp)\n",
    "\n",
    "\n",
    "        print(len(all_candidate_spans))\n",
    "        print(len(all_query_spans))\n",
    "        # print(all_candidate_spans)\n",
    "        print(\"==\"*50)\n",
    "        output.append({\n",
    "            \"sentences\": all_sentences,\n",
    "            \"querySpans\": all_query_spans,\n",
    "            \"candidateSpans\": all_candidate_spans,\n",
    "            \"clickSpans\": all_query_spans,\n",
    "        })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:02<00:00, 36.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for epi_key in tqdm(data):\n",
    "    if epi_key != (1,8):\n",
    "        continue\n",
    "    episode = data[epi_key]\n",
    "    # Each scene contain on episode\n",
    "    for scene in episode:\n",
    "        # Collect data to annotate\n",
    "        all_sentences = []\n",
    "        all_query_spans = []\n",
    "        all_candidate_spans = []\n",
    "\n",
    "        for i, utt in enumerate(scene):\n",
    "            if \"en_subtitles\" in utt:\n",
    "                prefix_length = None\n",
    "                # Fetch parse Noun Phrases from former parsing result\n",
    "                utterance = \" \".join([x.strip().lstrip('-').lstrip().lstrip('.').lstrip() for x in utt['en_subtitles']])\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "\n",
    "                if \":\" in utterance_tokens:\n",
    "                    colon_idx = utterance_tokens.index(\":\")\n",
    "                    prefix = utterance_tokens[: colon_idx]\n",
    "                    if \" \".join(prefix).isupper():\n",
    "                        prefix_length = len(prefix)+1\n",
    "                        utterance_tokens = utterance_tokens[prefix_length:]\n",
    "                        utterance = \" \".join(utterance_tokens)\n",
    "\n",
    "\n",
    "                speaker = utt['speaker'].strip().strip(\"(\").strip(\")\").strip().strip(\".\").strip().strip(\":\")\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "                all_sentences.append(sentence_tokens)\n",
    "                spans = list(set(utt['sm_noun_chunk']) | set(utt['berkeley_noun_chunk']) | set(utt['trf_noun_chunk']))\n",
    "\n",
    "                spans.sort(key=lambda x: x[1])\n",
    "                # Merge overlapping spans into one maximum logical span\n",
    "                to_pop = []\n",
    "                for j, (word_0, start_idx_0, end_idx_0) in enumerate(spans):\n",
    "                    for k, (word_1, start_idx_1, end_idx_1) in enumerate(spans):\n",
    "                        if k==j:\n",
    "                            continue\n",
    "                        if (start_idx_1 >= start_idx_0) and (end_idx_1 <= end_idx_0):\n",
    "                            to_pop.append(spans[k])\n",
    "                for item in to_pop:\n",
    "                    if item in spans:\n",
    "                        spans.remove(item)\n",
    "\n",
    "                # Split NPs with Poesstive Pronoun into two parts\n",
    "                poessives = []\n",
    "                for j, token in enumerate(sm_parser(utterance)):\n",
    "                    if token.tag_==\"PRP$\":\n",
    "                        for k, (word, start_idx, end_idx) in enumerate(spans):\n",
    "                            if start_idx <= j < end_idx:\n",
    "                                new_span_1 = (token.text, j, j+1)\n",
    "                                poessives.append(new_span_1)\n",
    "                    if token.tag_==\"NNPS\":\n",
    "                        pass\n",
    "\n",
    "                for item in poessives:\n",
    "                    spans.append(item)\n",
    "                spans.sort(key=lambda x: x[1])\n",
    "\n",
    "                for span in spans:\n",
    "                    all_query_spans.append({\n",
    "                        \"sentenceIndex\": i,\n",
    "                        \"startToken\": span[1] + len(speaker_tokens) + 1,\n",
    "                        \"endToken\": span[2] + len(speaker_tokens) + 1\n",
    "                    })\n",
    "\n",
    "                # Gather all possible candidate spans\n",
    "                temp = []\n",
    "                for window_size in range(1):\n",
    "                    temp += get_all_possible_spans(i, len(sentence_tokens), window_size)\n",
    "                all_candidate_spans.extend(temp)\n",
    "            else:\n",
    "                prefix_length = None\n",
    "                utterance = utt['utterance']\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "\n",
    "                if \":\" in utterance_tokens:\n",
    "                    colon_idx = utterance_tokens.index(\":\")\n",
    "                    prefix = utterance_tokens[: colon_idx]\n",
    "                    if \" \".join(prefix).isupper():\n",
    "                        prefix_length = len(prefix)+1\n",
    "\n",
    "                speaker = utt['speaker'].strip().strip(\"(\").strip(\")\").strip().strip(\".\").strip().strip(\":\")\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "\n",
    "                all_sentences.append(sentence_tokens)\n",
    "\n",
    "                # Gather all possible candidate spans\n",
    "                temp = []\n",
    "                for window_size in range(1):\n",
    "                    temp += get_all_possible_spans(i, len(sentence_tokens), window_size)\n",
    "                all_candidate_spans.extend(temp)\n",
    "        output.append({\n",
    "            \"sentences\": all_sentences,\n",
    "            \"querySpans\": all_query_spans,\n",
    "            \"candidateSpans\": all_candidate_spans,\n",
    "            \"clickSpans\": all_query_spans,\n",
    "        })\n",
    "\n",
    "\n",
    "with open('sample_annotate_epi_1_8.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "temp = output[0]\n",
    "print(len(temp['clickSpans']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 187\n",
      "33 159\n",
      "41 149\n",
      "68 233\n",
      "44 138\n"
     ]
    }
   ],
   "source": [
    "for item in output:\n",
    "    print(len(item['sentences']), len(item['querySpans']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "with open('all_to_annotate.pkl', 'wb') as f:\n",
    "    pkl.dump(output, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "with open('all_to_annotate.pkl', 'rb') as f:\n",
    "    all_output = pkl.load(f)\n",
    "    output = all_output[11:21]\n",
    "\n",
    "with open('sample_annotate_epi_1_2.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate samples to annotate by utterance number"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "with open('all_to_annotate.pkl', 'rb') as f:\n",
    "    all_output = pkl.load(f)\n",
    "    random.shuffle(all_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "output = []\n",
    "random.shuffle(all_output)\n",
    "for scene in all_output:\n",
    "    sent_num = len(scene['sentences'])\n",
    "    if 5 < sent_num < 10:\n",
    "        output.append(scene)\n",
    "        break\n",
    "\n",
    "for scene in all_output:\n",
    "    sent_num = len(scene['sentences'])\n",
    "    if 10 < sent_num < 15:\n",
    "        output.append(scene)\n",
    "        break\n",
    "\n",
    "for scene in all_output:\n",
    "    sent_num = len(scene['sentences'])\n",
    "    if 30 < sent_num < 35:\n",
    "        output.append(scene)\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "with open('sample_pilot_study.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Penny', ':', 'Where', \"'d\", 'you', 'go', '?', 'I', 'ca', \"n't\", 'tell', 'if', 'the', 'turkey', \"'s\", 'done', '!']\n",
      "['Leonard', ':', 'Be', 'right', 'there', '!', 'Hi', ',', 'lover', '.']\n",
      "['Penny', ':', ' ', 'What', 'are', 'you', 'doing', '?']\n",
      "['Leonard', ':', 'I', \"'m\", 'sorry', 'about', 'the', 'journal', '.', 'I', 'want', 'to', 'make', 'it', 'up', 'to', 'you', '.', 'So', 'I', \"'m\", 'gon', 'na', 'let', 'you', 'post', 'a', 'shame', 'photo', 'of', 'me', 'on', 'Facebook', '.']\n",
      "['Penny', ':', 'I', 'am', 'not', 'putting', 'that', 'on', 'the', 'Internet', '!', 'I', 'do', \"n't\", 'want', 'people', 'to', 'see', 'this', '.', 'I', 'do', \"n't\", 'want', 'to', 'see', 'it', '!']\n",
      "['Leonard', ':', 'Do', \"n't\", 'want', 'people', 'to', 'see', 'what', ',', 'huh', '?', 'A', 'little', 'bit', 'of', 'this', '?']\n",
      "['Penny', ':', ' ', 'Oh', '.']\n",
      "['Leonard', ':', 'Some', 'of', 'this', '?', 'And', ',', 'since', 'it', \"'s\", 'Thanksgiving', ',', 'an', 'extra', 'helping', 'of', 'this', '?']\n",
      "['Bernadette', ':', ' ', 'Happy', 'Thanks', '…', 'Yikes', '.']\n",
      "====================================================================================================\n",
      "['Penny', ':', 'Oh', ',', 'yeah', ',', 'no', ',', 'this', 'thing', \"'s\", 'majorly', 'out', 'of', 'order', '.', 'See', '?']\n",
      "['Ramona', ':', 'Sorry', '.', 'That', \"'s\", 'okay', '.', 'Guess', 'I', \"'m\", 'taking', 'the', 'stairs', '.']\n",
      "['Penny', ':', 'Where', 'you', 'going', '?']\n",
      "['Ramona', ':', ' ', '4', '-', 'A.']\n",
      "['Penny', ':', 'Oh', ',', 'are', 'you', 'here', 'to', 'see', 'Leonard', '?']\n",
      "['Ramona', ':', ' ', 'No', ',', 'Dr.', 'Cooper', '.']\n",
      "['Penny', ':', 'Dr.', 'Sheldon', 'Cooper', '?']\n",
      "['Ramona', ':', 'We', \"'re\", 'having', 'dinner', '.']\n",
      "['Penny', ':', 'Sheldon', 'Cooper', '?', 'Tall', ',', 'thin', ',', 'looks', 'a', 'little', 'like', 'a', 'giant', 'praying', 'mantis', '?']\n",
      "['Ramona', ':', 'He', 'is', 'cute', ',', \"isn't\", 'he', '?']\n",
      "['Penny', ':', ' ', 'Sheldon', 'Cooper', '?']\n",
      "====================================================================================================\n",
      "['Leonard', ':', 'We', 'just', 'wanted', 'to', 'see', 'how', 'your', 'class', 'was', 'going', '.', 'Where', 'is', 'everybody', '?']\n",
      "['Sheldon', ':', 'There', 'is', 'no', 'class', '.']\n",
      "['Howard', ':', 'Did', 'you', 'send', 'everyone', 'to', 'the', 'principal', \"'s\", 'office', 'already', '?']\n",
      "['Sheldon', ':', 'No', 'one', 'signed', 'up', '.']\n",
      "['Leonard', ':', 'Well', ',', 'that', \"'s\", 'not', 'your', 'fault', '.']\n",
      "['Sheldon', ':', 'I', 'called', 'the', 'department', 'secretary', 'to', 'see', 'what', 'happened', '.', 'Apparently', ',', 'I', 'have', 'a', 'reputation', 'for', 'being', 'obnoxious', '.', 'What', '?']\n",
      "['Leonard', ':', ' ', 'What', '?']\n",
      "['Raj', ':', 'Hey', ',', 'Sheldon', ',', 'I', \"'m\", 'sorry', '.']\n",
      "['Sheldon', ':', 'No', ',', 'it', \"'s\", 'fine', '.', 'Now', 'I', 'can', 'devote', 'all', 'my', 'time', 'to', 'dark', 'matter', '.']\n",
      "['Raj', ':', 'Aw', ',', 'you', 'brought', 'cookies', 'for', 'everyone', '?']\n",
      "['Sheldon', ':', 'Oh', ',', 'yes', '.', 'Fig', 'Newtons', '.', 'which', 'scientist', 'both', 'helped', 'to', 'develop', 'calculus', 'and', 'had', 'a', 'famous', 'cookie', 'named', 'after', 'him', '?', 'And', 'then', 'after', 'someone', 'said', '\"', 'Newton', ',', '\"', 'I', 'was', 'going', 'to', 'tell', 'them', 'they', \"'re\", 'wrong', '.', 'The', 'cookies', 'are', 'named', 'after', 'a', 'town', 'in', 'Massachusetts', '.', 'And', 'then', 'I', \"'d\", 'throw', 'the', 'cookies', 'away', '.']\n",
      "['Howard', ':', 'Hey', ',', 'what', 'if', 'I', 'took', 'your', 'class', '?']\n",
      "['Sheldon', ':', ' ', 'Why', 'would', 'you', 'do', 'that', '?']\n",
      "['Leonard', ':', 'Why', 'would', 'you', 'do', 'that', '?', 'Yeah', ',', 'why', 'would', 'you', 'do', 'that', '?']\n",
      "['Raj', ':', ' ', 'What', '’s', 'wrong', 'with', 'you', '?']\n",
      "['Howard', ':', 'I', \"'m\", 'thinking', 'about', 'getting', 'my', 'doctorate', ',', 'and', 'he', 'wants', 'to', 'teach--', 'why', 'not', '?']\n",
      "['Sheldon', ':', 'Oh', ',', 'Howard', '.', 'I', 'appreciate', 'the', 'gesture', ',', 'but', 'this', 'is', 'a', 'graduate', '-', 'level', 'physics', 'class', '.', 'I', 'do', \"n't\", 'think', 'you', \"'d\", 'understand', 'a', 'single', 'thing', 'I', 'was', 'talking', 'about', '.']\n",
      "['Raj', ':', 'Ask', 'why', 'not', 'again', ';']\n",
      "['Howard', ':', 'Sheldon', ',', 'I', \"'m\", 'more', 'than', 'smart', 'enough', 'to', 'take', 'your', 'class', '.', 'No', '.']\n",
      "['Sheldon', ':', ' ', 'No', '.']\n",
      "['Howard', ':', ' ', 'Yes', '.']\n",
      "['Sheldon', ':', 'How', 'would', 'you', 'determine', 'the', 'ground', 'state', 'of', 'a', 'quantum', 'system', 'with', 'no', 'exact', 'solution', '?']\n",
      "['Howard', ':', 'I', 'would', 'guess', 'a', 'wave', '-', 'function', 'and', 'then', 'vary', 'its', 'parameters', 'until', 'I', 'found', 'the', 'lowest', 'energy', 'solution', '.']\n",
      "['Sheldon', ':', 'Do', 'you', 'know', 'how', 'to', 'integrate', 'X', 'squared', 'times', 'E', 'to', 'the', 'minus', 'X', ',', 'without', 'looking', 'it', 'up', '?']\n",
      "['Howard', ':', 'I', \"'d\", 'use', 'Feynman', \"'s\", 'trick--', 'differentiate', 'under', 'the', 'integral', 'sign', '.']\n",
      "['Sheldon', ':', 'Okay', '.', 'Um', '...', 'What', 'is', 'the', 'correct', 'interpretation', 'of', 'quantum', 'mechanics', '?']\n",
      "['Howard', ':', 'Since', 'every', 'interpretation', 'gives', 'exactly', 'the', 'same', 'answer', 'to', 'every', 'measurement', ',', 'they', 'are', 'all', 'equally', 'correct', '.', 'However', ',', 'I', 'know', 'you', 'believe', 'in', 'the', 'Many', 'Worlds', 'Interpretation', ',', 'so', 'I', \"'ll\", 'say', 'that', '.', 'Now', 'do', 'you', 'think', 'I', \"'m\", 'smart', 'enough', '?']\n",
      "['Sheldon', ':', ' ', 'No', '.']\n",
      "['Howard', ':', 'Oh', ',', 'come', 'on', '.', 'You', 'might', \"'ve\", 'gone', 'to', 'school', 'for', 'a', 'couple', 'more', 'years', 'than', 'me', ',', 'but', 'guess', 'what--', 'engineers', 'are', 'just', 'as', 'smart', 'as', 'physicists', '.']\n",
      "['Sheldon', ':', 'You', 'take', 'that', 'back', '!']\n",
      "['Howard', ':', 'No', '.']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for scene in output:\n",
    "    for sent in scene['sentences']:\n",
    "        print(sent)\n",
    "    print('=='*50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "with open('all_to_annotate.pkl', 'rb') as f:\n",
    "    all_output = pkl.load(f)\n",
    "    output = all_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "print(len(output[0]['querySpans']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1070\n",
      "73411\n",
      "68.60841121495326\n"
     ]
    }
   ],
   "source": [
    "query_nums = []\n",
    "for item in output:\n",
    "    query_nums.append(len(item['querySpans']))\n",
    "print(len(query_nums))\n",
    "print(sum(query_nums))\n",
    "print(sum(query_nums)/len(query_nums))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = []\n",
    "for epi_key in tqdm(data):\n",
    "    if epi_key != (1,8):\n",
    "        continue\n",
    "    episode = data[epi_key]\n",
    "    # Each scene contain on episode\n",
    "    for scene in episode:\n",
    "        # Collect data to annotate\n",
    "        all_sentences = []\n",
    "        all_query_spans = []\n",
    "        all_candidate_spans = []\n",
    "\n",
    "        for i, utt in enumerate(scene):\n",
    "            if \"en_subtitles\" in utt:\n",
    "                prefix_length = None\n",
    "                # Fetch parse Noun Phrases from former parsing result\n",
    "                utterance = \" \".join([x.strip().lstrip('-').lstrip().lstrip('.').lstrip() for x in utt['en_subtitles']])\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "\n",
    "                if \":\" in utterance_tokens:\n",
    "                    colon_idx = utterance_tokens.index(\":\")\n",
    "                    prefix = utterance_tokens[: colon_idx]\n",
    "                    if \" \".join(prefix).isupper():\n",
    "                        prefix_length = len(prefix)+1\n",
    "                        utterance_tokens = utterance_tokens[prefix_length:]\n",
    "                        utterance = \" \".join(utterance_tokens)\n",
    "\n",
    "\n",
    "                speaker = utt['speaker'].strip().strip(\"(\").strip(\")\").strip().strip(\".\").strip().strip(\":\")\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "                all_sentences.append(sentence_tokens)\n",
    "                spans = list(set(utt['sm_noun_chunk']) | set(utt['berkeley_noun_chunk']) | set(utt['trf_noun_chunk']))\n",
    "\n",
    "                spans.sort(key=lambda x: x[1])\n",
    "                # Merge overlapping spans into one maximum logical span\n",
    "                to_pop = []\n",
    "                for j, (word_0, start_idx_0, end_idx_0) in enumerate(spans):\n",
    "                    for k, (word_1, start_idx_1, end_idx_1) in enumerate(spans):\n",
    "                        if k==j:\n",
    "                            continue\n",
    "                        if (start_idx_1 >= start_idx_0) and (end_idx_1 <= end_idx_0):\n",
    "                            to_pop.append(spans[k])\n",
    "                for item in to_pop:\n",
    "                    if item in spans:\n",
    "                        spans.remove(item)\n",
    "\n",
    "                # Split NPs with Poesstive Pronoun into two parts\n",
    "                poessives = []\n",
    "                for j, token in enumerate(sm_parser(utterance)):\n",
    "                    if token.tag_==\"PRP$\":\n",
    "                        for k, (word, start_idx, end_idx) in enumerate(spans):\n",
    "                            if start_idx <= j < end_idx:\n",
    "                                new_span_1 = (token.text, j, j+1)\n",
    "                                poessives.append(new_span_1)\n",
    "                    if token.tag_==\"NNPS\":\n",
    "                        pass\n",
    "\n",
    "                for item in poessives:\n",
    "                    spans.append(item)\n",
    "                spans.sort(key=lambda x: x[1])\n",
    "\n",
    "                for span in spans:\n",
    "                    all_query_spans.append({\n",
    "                        \"sentenceIndex\": i,\n",
    "                        \"startToken\": span[1] + len(speaker_tokens) + 1,\n",
    "                        \"endToken\": span[2] + len(speaker_tokens) + 1\n",
    "                    })\n",
    "\n",
    "                # Gather all possible candidate spans\n",
    "                temp = []\n",
    "                for window_size in range(1):\n",
    "                    temp += get_all_possible_spans(i, len(sentence_tokens), window_size)\n",
    "                all_candidate_spans.extend(temp)\n",
    "            else:\n",
    "                prefix_length = None\n",
    "                utterance = utt['utterance']\n",
    "                utterance_tokens = [item.text for item in sm_parser(utterance)]\n",
    "\n",
    "                if \":\" in utterance_tokens:\n",
    "                    colon_idx = utterance_tokens.index(\":\")\n",
    "                    prefix = utterance_tokens[: colon_idx]\n",
    "                    if \" \".join(prefix).isupper():\n",
    "                        prefix_length = len(prefix)+1\n",
    "\n",
    "                speaker = utt['speaker'].strip().strip(\"(\").strip(\")\").strip().strip(\".\").strip().strip(\":\")\n",
    "                speaker_tokens = [item.text for item in sm_parser(speaker)]\n",
    "                sentence_tokens = speaker_tokens + [\":\"] + utterance_tokens\n",
    "\n",
    "                all_sentences.append(sentence_tokens)\n",
    "\n",
    "                # Gather all possible candidate spans\n",
    "                temp = []\n",
    "                for window_size in range(1):\n",
    "                    temp += get_all_possible_spans(i, len(sentence_tokens), window_size)\n",
    "                all_candidate_spans.extend(temp)\n",
    "        output.append({\n",
    "            \"sentences\": all_sentences,\n",
    "            \"querySpans\": all_query_spans,\n",
    "            \"candidateSpans\": all_candidate_spans,\n",
    "            \"clickSpans\": all_query_spans,\n",
    "        })\n",
    "\n",
    "\n",
    "with open('sample_annotate_epi_1_8.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}