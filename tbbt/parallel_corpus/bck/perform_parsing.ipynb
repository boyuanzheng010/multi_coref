{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import spacy\n",
    "import csv\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import spacy_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "with open('tbbt_en_zh.pkl', 'rb') as f:\n",
    "    data = pkl.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.\n",
      " Agreed, what’s your point?\n",
      " There’s no point, I just think it’s a good idea for a tee-shirt. \n",
      " Excuse me?\n",
      " Hang on. \n",
      " One across is Aegean, eight down is Nabakov, twenty-six across is MCM, fourteen down is… move your finger… phylum, which makes fourteen across Port-au-Prince. See, Papa Doc’s capital idea, that’s Port-au-Prince. Haiti. \n",
      " Can I help you?\n",
      " Yes. Um, is this the High IQ sperm bank?\n",
      " If you have to ask, maybe you shouldn’t be here.\n",
      " I think this is the place.\n",
      " Fill these out.\n",
      " Thank-you. We’ll be right back.\n",
      " Oh, take your time. I’ll just finish my crossword puzzle. Oh wait.\n",
      " Leonard, I don’t think I can do this.\n",
      " What, are you kidding? You’re a semi-pro. \n",
      " No. We are committing genetic fraud. There’s no guarantee that our sperm is going to generate high IQ offspring, think about that. I have a sister with the same basic DNA mix who hostesses at Fuddruckers.\n",
      " Sheldon, this was your idea. A little extra money to get fractional T1 bandwidth in the apartment.\n",
      " I know, and I do yearn for faster downloads, but there’s some poor woman is going to pin her hopes on my sperm, what if she winds up with a toddler who doesn’t know if he should use an integral or a differential to solve the area under a curve.\n",
      " I’m sure she’ll still love him.\n",
      " I wouldn’t.\n",
      " Well, what do you want to do?\n",
      " I want to leave.\n",
      " What’s the protocol for leaving?\n",
      " I don’t know, I’ve never reneged on a proffer of sperm before.\n",
      " Let’s try just walking out.\n",
      " Okay.\n",
      " Bye-bye\n",
      " See you.\n",
      "====================================================================================================\n",
      "1\n",
      " Are you still mad about the sperm bank?\n",
      " No.\n",
      " You want to hear an interesting thing about stairs?\n",
      " Not really.\n",
      " If the height of a single step is off by as little as two millimetres, most people will trip.\n",
      " I don’t care. Two millimetres? That doesn’t seem right.\n",
      " No, it’s true, I did a series of experiments when I was twelve, my father broke his clavicle.\n",
      " Is that why they sent you to boarding school?\n",
      " No, that was the result of my work with lasers.\n",
      " New neighbour?\n",
      " Evidently.\n",
      " Significant improvement over the old neighbour.\n",
      " Two hundred pound transvestite with a skin condition, yes she is.\n",
      " Oh, hi!\n",
      " Hi.\n",
      " Hi.\n",
      " Hi?\n",
      " We don’t mean to interrupt, we live across the hall.\n",
      " Oh, that’s nice.\n",
      " Oh… uh… no… we don’t live together… um… we live together but in separate, heterosexual bedrooms.\n",
      " Oh, okay, well, guess I’m your new neighbour, Penny.\n",
      " Leonard, Sheldon.\n",
      " Hi.\n",
      " Hi.\n",
      " Hi.\n",
      " Hi. \n",
      " Hi. Well, uh, oh, welcome to the building.\n",
      " Thankyou, maybe we can have coffee sometime.\n",
      " Oh, great.\n",
      " Great. \n",
      " Great.\n",
      " Great. Well, bye.\n",
      " Bye.\n",
      " Bye.\n",
      " Bye. \n",
      " Should we have invited her for lunch?\n",
      " No. We’re going to start Season Two of Battlestar Galactica.\n",
      " We already watched the Season Two DVDs.\n",
      " Not with commentary.\n",
      " I think we should be good neighbours, invite her over, make her feel welcome.\n",
      " We never invited Louis-slash-Louise over.\n",
      " Well, then that was wrong of us. We need to widen our circle.\n",
      " I have a very wide circle. I have 212 friends on myspace. \n",
      " Yes, and you’ve never met one of them.\n",
      " That’s the beauty of it.\n",
      " I’m going to invite her over. We’ll have a nice meal and chat.\n",
      " Chat? We don’t chat. At least not offline.\n",
      " Well it’s not difficult, you just listen to what she says and then you say something appropriate in response.\n",
      " To what end?\n",
      " Hi. Again.\n",
      " Hi.\n",
      " Hi.\n",
      " Hi.\n",
      " Hi. \n",
      " Anyway, um. We brought home Indian food. And, um. I know that moving can be stressful, and I find that when I’m undergoing stress, that good food and company can have a comforting effect. Also, curry is a natural laxative, and I don’t have to tell you that, uh, a clean colon is just one less thing to worry about.\n",
      " Leonard, I’m not expert here but I believe in the context of a luncheon invitation, you might want to skip the reference to bowel movements.\n",
      " Oh, you’re inviting me over to eat?\n",
      " Uh, yes.\n",
      " Oh, that’s so nice, I’d love to.\n",
      " Great.\n",
      " So, what do you guys do for fun around here?\n",
      " Well, today we tried masturbating for money.\n",
      "====================================================================================================\n",
      "2\n",
      " Okay, well, make yourself at home.\n",
      " Okay, thankyou.\n",
      " You’re very welcome.\n",
      " This looks like some serious stuff, Leonard, did you do this?\n",
      " Actually that’s my work.\n",
      " Wow.\n",
      " Yeah, well, it’s just some quantum mechanics, with a little string theory doodling around the edges. That part there, that’s just a joke, it’s a spoof of the Bourne-Oppenheimer approximation.\n",
      " So you’re like, one of those, beautiful mind genius guys.\n",
      " Yeah. \n",
      " This is really impressive.\n",
      " I have a board. If you like boards, this is my board.\n",
      " Holy smokes.\n",
      " If by holy smokes you mean a derivative restatement of the kind of stuff you can find scribbled on the wall of any men’s room at MIT, sure.\n",
      " What?\n",
      " Oh, come on. Who hasn’t seen this differential below “here I sit broken hearted?”\n",
      " At least I didn’t have to invent twenty-six dimensions just to make the math come out.\n",
      " I didn’t invent them, they’re there.\n",
      " In what universe?\n",
      " In all of them, that is the point.\n",
      " Uh, do you guys mind if I start?\n",
      " Um, Penny, that’s where I sit.\n",
      " So, sit next to me. \n",
      " No, I sit there.\n",
      " What’s the difference?\n",
      " What’s the difference?\n",
      " Here we go.\n",
      " In the winter that seat is close enough to the radiator to remain warm, and yet not so close as to cause perspiration. In the summer it’s directly in the path of a cross breeze created by open windows there, and there. It faces the television at an angle that is neither direct, thus discouraging conversation, nor so far wide to create a parallax distortion, I could go on, but I think I’ve made my point. \n",
      " Do you want me to move?\n",
      " Well.\n",
      " Just sit somewhere else.\n",
      " Fine. (Wanders in circles, looking lost.)\n",
      " Sheldon, sit!\n",
      " Aaah!\n",
      " Well this is nice. We don’t have a lot of company over.\n",
      " That’s not true. Koothrapali and Wolowitz come over all the time. \n",
      " Yes I now, but…\n",
      " Tuesday night we played Klingon boggle until one in the morning.\n",
      " Yes, I remember.\n",
      " I resent you saying we don’t have company.\n",
      " I’m sorry.\n",
      " That is an antisocial implication.\n",
      " I said I’m sorry.\n",
      " So, Klingon boggle?\n",
      " Yeah, it’s like regular boggle but, in Klingon. That’s probably enough about us, tell us about you.\n",
      " Um, me, okay, I’m Sagittarius, which probably tells you way more than you need to know.\n",
      " Yes, it tells us that you participate in the mass cultural delusion that the Sun’s apparent position relative to arbitrarily defined constellations and the time of your birth somehow effects your personality.\n",
      " Participate in the what?\n",
      " I think what Sheldon’s trying to say, is that Sagittarius wouldn’t have been our first guess.\n",
      " Oh, yeah, a lot of people think I’m a water sign. Okay, let’s see, what else, oh, I’m a vegetarian, oh, except for fish, and the occasional steak, I love steak. \n",
      " That’s interesting. Leonard can’t process corn.\n",
      " Wu-uh, do you have some sort of a job?\n",
      " Oh, yeah, I’m a waitress at the Cheesecake Factory.\n",
      " Oh, okay. I love cheesecake.\n",
      " You’re lactose intolerant. \n",
      " I don’t eat it, I just think it’s a good idea.\n",
      " Oh, anyways, I’m also writing a screenplay. It’s about this sensitive girl who comes to L.A. from Lincoln Nebraska to be an actress, and winds up a waitress at the Cheesecake Factory.\n",
      " So it’s based on your life?\n",
      " No, I’m from Omaha. \n",
      " Well, if that was a movie I would go see it.\n",
      " I know, right? Okay, let’s see, what else? Um, that’s about it. That’s the story of Penny.\n",
      " Well it sounds wonderful.\n",
      " It was. Until I fell in love with a jerk. \n",
      " What’s happening.\n",
      " I don’t know.\n",
      " Oh God, you know, four years I lived with him, four years, that’s like as long as High School. \n",
      " It took you four years to get through High School?\n",
      " Don’t.\n",
      " I just, I can’t believe I trusted him.\n",
      " Should I say something? I feel like I should say something.\n",
      " You? No, you’ll only make it worse.\n",
      " You want to know the most pathetic part? Even though I hate his lying, cheating guts, I still love him. Is that crazy?\n",
      " No, it’s not crazy it’s, uh, uh, it’s a paradox. And paradoxes are part of nature, think about light. Now if you look at Huygens, light is a wave, as confirmed by the double slit experiments, but then, along comes Albert Einstein and discovers that light behaves like particles too. Well, I didn’t make it worse.\n",
      " Oh, I’m so sorry, I’m such a mess, and on top of everything else I’m all gross from moving and my stupid shower doesn’t even work.\n",
      " Our shower works.\n",
      " Really? Would it be totally weird if I used it?\n",
      " Yes. \n",
      " No?\n",
      " No.\n",
      " No.\n",
      " It’s right down the hall.\n",
      " Thanks. You guys are really sweet.\n",
      " Well this is an interesting development. \n",
      " How so?\n",
      " It has been some time since we’ve had a woman take her clothes off in our apartment.\n",
      " That’s not true, remember at Thanksgiving my grandmother with Alzheimer’s had that episode.\n",
      " Point taken. It has been some time since we’ve had a woman take her clothes off after which we didn’t want to rip our eyes out. \n",
      " The worst part was watching her carve that turkey.\n",
      " So, what exactly are you trying to accomplish here?\n",
      " Excuse me?\n",
      " That woman in there’s not going to have sex with you.\n",
      " Well I’m not trying to have sex with her.\n",
      " Oh, good. Then you won’t be disappointed.\n",
      " What makes you think she wouldn’t have sex with me, I’m a male and she’s a female?\n",
      " Yes, but not of the same species.\n",
      " I’m not going to engage in hypotheticals here, I’m just trying to be a good neighbour.\n",
      " Oh, of course.\n",
      " That’s not to say that if a carnal relationship were to develop that I wouldn’t participate. However briefly.\n",
      " Do you think this possibility will be helped or hindered when she discovers your Luke Skywalker no-more-tears shampoo?\n",
      " It’s Darth Vader shampoo. (There is a knock on the door.) Luke Skywalker’s the conditioner.\n",
      " Wait till you see this.\n",
      " It’s fantastic. Unbelievable.\n",
      " See what?\n",
      " It’s a Stephen Hawking lecture from MIT in 1974.\n",
      " This is not a good time.\n",
      " It’s before he became a creepy computer voice:.\n",
      " That’s great, you guys have to go.\n",
      " Why?\n",
      " It’s just not a good time.\n",
      " Leonard has a lady over.\n",
      " Yeah, right, your grandmother back in town?\n",
      " No. And she’s not a lady, she’s just a new neighbour.\n",
      " Hang on, there really is a lady here?\n",
      " Uh-huh.\n",
      " And you want us out because you’re anticipating coitus?\n",
      " I’m not anticipating coitus.\n",
      " So she’s available for coitus?\n",
      " Can we please stop saying coitus?\n",
      " Technically that would be coitus interruptus.\n",
      " Hey, is there a trick to getting it to switch from tub to shower. Oh. Hi, sorry. Hello!\n",
      " Enchante Madamoiselle. Howard Wolowitz, Cal-Tech department of Applied Physics. You may be familiar with some of my work, it’s currently orbiting Jupiter’s largest moon taking high-resolution digital photographs.\n",
      " Penny. I work at the Cheesecake Factory.\n",
      " Come on, I’ll show you the trick with the shower.\n",
      " Bon douche.\n",
      " I’m sorry?\n",
      " It’s French for good shower. It’s a sentiment I can express in six languages.\n",
      " Save it for your blog, Howard.\n",
      " See-ka-tong-guay-jow.\n",
      "====================================================================================================\n",
      "3\n",
      " Uh, there it goes, it sticks, I’m sorry.\n",
      " Okay. Thanks. \n",
      " You’re welcome, oh, you’re going to step right, okay, I’ll….\n",
      " Hey, Leonard?\n",
      " The hair products are Sheldon’s.\n",
      " Um, okay. Can I ask you a favour.\n",
      " A favour? Sure, you can ask me a favour, I would do you a favour for you.\n",
      " It’s okay if you say no.\n",
      " Oh, I’ll probably say yes.\n",
      " It’s just not the kind of thing you ask a guy you’ve just met.\n",
      " Wow.\n",
      "====================================================================================================\n",
      "4\n",
      " I really think we should examine the chain of causality here.\n",
      " Must we?\n",
      " Event A. A beautiful woman stands naked in our shower. Event B. We drive half way across town to retrieve a television set from the aforementioned woman’s ex-boyfriend. Query, on what plane of existence is there even a semi-rational link between these events?\n",
      " She asked me to do her a favour, Sheldon.\n",
      " Ah, yes, well that may be the proximal cause of our journey, but we both know it only exists in contradistinction to the higher level distal cause.\n",
      " Which is?\n",
      " You think with your penis.\n",
      " That’s a biological impossibility and you didn’t have to come.\n",
      " Oh, right, yes, I could have stayed behind and watched Wolowitz try to hit on Penny in Russian, Arabic and Farsi. Why can’t she get her own TV.\n",
      " Come on, you know how it is with break-ups.\n",
      " No I don’t. And neither do you.\n",
      " Wuh, I, I broke up with Joyce Kim.\n",
      " You did not break up with Joyce Kim, she defected to North Korea.\n",
      " To mend her broken heart. This situation is much less complicated. There’s some kind of dispute between Penny and her ex-boyfriend as to who gets custody of the TV. She just wanted to avoid having a scene with him.\n",
      " So we get to have a scene with him?\n",
      " No, Sheldon, there’s not going to be a scene. There’s two of us and one of him.\n",
      " Leonard, the two of us can’t even carry a TV. \n",
      "====================================================================================================\n",
      "5\n",
      " So, you guys work with Leonard and Sheldon at the University?\n",
      " Uh, I’m sorry, do you speak English?\n",
      " Oh, he speaks English, he just can’t speak to women.\n",
      " Really, why?\n",
      " He’s kind of a nerd. Juice box?\n",
      "====================================================================================================\n",
      "6\n",
      " I’ll do the talking.\n",
      " Yeah.\n",
      " Hi, I’m Leonard, this is Sheldon.\n",
      " Hello.\n",
      " What did I just…. Uh, we’re here to pick up Penny’s TV.\n",
      " Get lost.\n",
      " Okay, thanks for your time.\n",
      " We’re not going to give up just like that.\n",
      " Leonard, the TV is in the building, we’ve been denied access to the building, ergo we are done.\n",
      " Excuse me, if I were to give up at the first little hitch I never would have been able to identify the fingerprints of string theory in the aftermath of the big bang.\n",
      " My apologies. What’s your plan.\n",
      " It’s just a privilege to watch your mind at work.\n",
      " Come on, we have a combined IQ of 360, we should be able to figure out how to get into a stupid building.\n",
      " What do you think their combined IQ is?\n",
      " Just grab the door.\n",
      "====================================================================================================\n",
      "7\n",
      " This is it. (Knocks.) I’ll do the talking.\n",
      " Good thinking, I’ll just be the muscle.\n",
      " Yeah?\n",
      " I’m Leonard, this is Sheldon.\n",
      " From the intercom.\n",
      " How the hell did you get in the building?\n",
      " Oh. We’re scientists.\n",
      " Tell him about our IQ.\n",
      "====================================================================================================\n",
      "8\n",
      " Leonard.\n",
      " What?\n",
      " My mom bought me those pants.\n",
      " I’m sorry.\n",
      " You’re going to have to call her.\n",
      "====================================================================================================\n",
      "9\n",
      " Sheldon, I’m so sorry I dragged you through this.\n",
      " It’s okay. It wasn’t my first pantsing, and it won’t be my last.\n",
      " And you were right about my motives, I was hoping to establish a relationship with Penny that might have some day led to sex.\n",
      " Well you got me out of my pants.\n",
      " Anyway, I’ve learned my lesson. She’s out of my league, I’m done with her, I’ve got my work, one day I’ll win the Nobel Prize and then I’ll die alone.\n",
      " Don’t think like that, you’re not going to die alone.\n",
      " Thank you Sheldon, you’re a good friend.\n",
      " And you’re certainly not going to win a Nobel Prize.\n",
      "====================================================================================================\n",
      "10\n",
      " This is one of my favourite places to kick back after a quest, they have a great house ale.\n",
      " Wow, cool tiger.\n",
      " Yeah, I’ve had him since level ten. His name is Buttons. Anyway, if you had your own game character we could hang out, maybe go on a quest.\n",
      " Uh, sounds interesting.\n",
      " So you’ll think about it?\n",
      " Oh, I don’t think I’ll be able to stop thinking about it.\n",
      " Smooth.\n",
      " We’re home.\n",
      " Oh, my God, what happened?\n",
      " Well, your ex-boyfriend sends his regards and I think the rest is fairly self-explanatory.\n",
      " I’m so sorry, I really thought if you guys went instead of me he wouldn’t be such an ass.\n",
      " No, it was a valid hypothesis.\n",
      " That was a valid hypothesis? What is happening to you?\n",
      " Really, thank you so much for going and trying you’re, uh, you’re so terrific. Why don’t you put some clothes on, I’ll get my purse and dinner is on me, okay?\n",
      " Really? Great.\n",
      " Thank you. You’re not done with her, are you?\n",
      " Our babies will be smart and beautiful.\n",
      " Not to mention imaginary.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "scenes = data[(1,1)]\n",
    "for i, scene in enumerate(scenes):\n",
    "    print(i)\n",
    "    for item in scene:\n",
    "        print(item['utterance'])\n",
    "    print(\"==\"*50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "md_parser = spacy.load('en_core_web_md')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Agreed, what’s your point?\n",
      "what WP\n",
      " What, are you kidding? You’re a semi-pro. \n",
      "What WP\n",
      " No. We are committing genetic fraud. There’s no guarantee that our sperm is going to generate high IQ offspring, think about that. I have a sister with the same basic DNA mix who hostesses at Fuddruckers.\n",
      "who WP\n",
      " I know, and I do yearn for faster downloads, but there’s some poor woman is going to pin her hopes on my sperm, what if she winds up with a toddler who doesn’t know if he should use an integral or a differential to solve the area under a curve.\n",
      "what WP\n",
      " I know, and I do yearn for faster downloads, but there’s some poor woman is going to pin her hopes on my sperm, what if she winds up with a toddler who doesn’t know if he should use an integral or a differential to solve the area under a curve.\n",
      "who WP\n",
      " Well, what do you want to do?\n",
      "what WP\n",
      " What’s the protocol for leaving?\n",
      "What WP\n"
     ]
    }
   ],
   "source": [
    "for instance in scenes[0]:\n",
    "    sent = instance['utterance']\n",
    "    # print(sent)\n",
    "    for token in md_parser(sent):\n",
    "        if token.tag_==\"WP\":\n",
    "            print(sent)\n",
    "            print(token.text, token.tag_)\n",
    "    # print(\"==\"*50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform Parsing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Load Data\n",
    "scene = data[(1,1)][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'utterance': ' So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.', 'speaker': 'Sheldon', 'en_subtitles': ['If a photon is directed through a plane with two slits in it and either is observed it will not go through both.', 'If unobserved, it will.', \"If it's observed after it left the plane, before it hits its target...\", '- ...it will not have gone through both slits.'], 'zh_subtitles': ['将光子正对平面上的双缝 观察任意一个隙缝 它不会穿过那两个隙缝', '如果没被观察 那就会', '总之 如果观察它在离开平面到击中目标之前', '它就不会穿过那两个隙缝']}\n",
      "{'utterance': ' Agreed, what’s your point?', 'speaker': 'Leonard', 'en_subtitles': [\"- Agreed. What's your point?\"], 'zh_subtitles': ['没错 但你为什么要说这个?']}\n",
      "{'utterance': ' There’s no point, I just think it’s a good idea for a tee-shirt. ', 'speaker': 'Sheldon', 'en_subtitles': [\"There's no point, I just think it's a good idea for a T-shirt.\"], 'zh_subtitles': ['没什么 我只是觉得这个主意 可以用于设计T恤衫']}\n",
      "{'utterance': ' Excuse me?', 'speaker': 'Leonard'}\n",
      "{'utterance': ' Hang on. ', 'speaker': 'Receptionist'}\n",
      "{'utterance': ' One across is Aegean, eight down is Nabakov, twenty-six across is MCM, fourteen down is… move your finger… phylum, which makes fourteen across Port-au-Prince. See, Papa Doc’s capital idea, that’s Port-au-Prince. Haiti. ', 'speaker': 'Leonard', 'en_subtitles': ['One across is Aegean, eight down is Nabokov.', 'Twenty-six across is MCM.', 'Fourteen down is... Move your finger.', '...phylum, which makes 14 across Port-Au-Prince.', \"See, Papa Doc's capital idea, that's Port-Au-Prince.\", 'Haiti.'], 'zh_subtitles': ['横1是Aegean (爱情海)', '竖8是Nabokov (小说洛丽塔的作者) 横26是MCM', '竖14是...', '手指挪开点 Phylum (生物门类) 这样一来横14就是Port -au', '瞧 提示是\"Papa doc的首都\" (海地前总统) 所以是太子港', '海地的']}\n",
      "{'utterance': ' Can I help you?', 'speaker': 'Receptionist', 'en_subtitles': ['- Can I help you? - Yes.'], 'zh_subtitles': ['- 能为你效劳吗?']}\n",
      "{'utterance': ' Yes. Um, is this the High IQ sperm bank?', 'speaker': 'Leonard', 'en_subtitles': ['Um, is this the high-IQ sperm bank?'], 'zh_subtitles': ['这里是高智商精子银行吗?']}\n",
      "{'utterance': ' If you have to ask, maybe you shouldn’t be here.', 'speaker': 'Receptionist', 'en_subtitles': [\"If you have to ask, maybe you shouldn't be here.\"], 'zh_subtitles': ['如果你这么问 也许你不该来这']}\n",
      "{'utterance': ' I think this is the place.', 'speaker': 'Sheldon', 'en_subtitles': ['I think this is the place.'], 'zh_subtitles': ['我想就是这没错了']}\n",
      "{'utterance': ' Fill these out.', 'speaker': 'Receptionist', 'en_subtitles': ['- Fill these out. - Thank you.'], 'zh_subtitles': ['- 把这个填一填']}\n",
      "{'utterance': ' Thank-you. We’ll be right back.', 'speaker': 'Leonard', 'en_subtitles': [\"- We'll be right back.\"], 'zh_subtitles': ['- 谢谢 我们马上好']}\n",
      "{'utterance': ' Oh, take your time. I’ll just finish my crossword puzzle. Oh wait.', 'speaker': 'Receptionist', 'en_subtitles': [\"- Oh, take your time. I'll just finish my crossword puzzle.\", 'Oh, wait.'], 'zh_subtitles': ['慢慢来 我还要玩填字游戏', '噢 慢着']}\n",
      "{'utterance': ' Leonard, I don’t think I can do this.', 'speaker': 'Sheldon', 'en_subtitles': [\"Leonard, I don't think I can do this.\"], 'zh_subtitles': ['Leonard 我办不到']}\n",
      "{'utterance': ' What, are you kidding? You’re a semi-pro. ', 'speaker': 'Leonard', 'en_subtitles': ['What, are you kidding?'], 'zh_subtitles': ['开玩笑? 你可是半职业人士']}\n",
      "{'utterance': ' No. We are committing genetic fraud. There’s no guarantee that our sperm is going to generate high IQ offspring, think about that. I have a sister with the same basic DNA mix who hostesses at Fuddruckers.', 'speaker': 'Sheldon', 'en_subtitles': ['We are committing genetic fraud.', \"There's no guarantee our sperm's going to generate high-IQ offspring. Think about that.\", 'I have a sister with the same basic DNA mix who hostesses at Fuddruckers.'], 'zh_subtitles': ['不 我们这样是诈骗', '我们没法保证 生出来的一定是高智商小孩', '我姐姐跟我有一套相同的基本基因 她却在Fuddrucker餐厅当服务生']}\n",
      "{'utterance': ' Sheldon, this was your idea. A little extra money to get fractional T1 bandwidth in the apartment.', 'speaker': 'Leonard', 'en_subtitles': ['Sheldon, this was your idea.', 'A little extra money to get fractional T1 bandwidth in the apartment?'], 'zh_subtitles': ['Sheldon 这可是你的主意啊', '轻松赚点钱 就有钱能升级我们的网络带宽']}\n",
      "{'utterance': ' I know, and I do yearn for faster downloads, but there’s some poor woman is going to pin her hopes on my sperm, what if she winds up with a toddler who doesn’t know if he should use an integral or a differential to solve the area under a curve.', 'speaker': 'Sheldon', 'en_subtitles': ['I know. And I do yearn for faster downloads.', \"There's some poor woman who's gonna pin her hopes on my sperm.\", \"What if she winds up with a toddler who doesn't know if he should use a differential to solve for the area under a curve?\"], 'zh_subtitles': ['我知道 我确实很渴望高速下载', '但一些可怜的女人 会把希望寄托在我的精子上', '万一生出来一个 连曲线下部的面积用积分还是微分算 都搞不清楚的小屁孩怎么办?']}\n",
      "{'utterance': ' I’m sure she’ll still love him.', 'speaker': 'Leonard', 'en_subtitles': [\"- I'm sure she'll still love him.\"], 'zh_subtitles': ['我想她还是会爱那个宝宝的']}\n",
      "{'utterance': ' I wouldn’t.', 'speaker': 'Sheldon', 'en_subtitles': [\"- I wouldn't.\"], 'zh_subtitles': ['我不会']}\n",
      "{'utterance': ' Well, what do you want to do?', 'speaker': 'Leonard', 'en_subtitles': ['Well, what do you wanna do?'], 'zh_subtitles': ['你想要怎样?']}\n",
      "{'utterance': ' I want to leave.', 'speaker': 'Sheldon', 'en_subtitles': ['- I want to leave.'], 'zh_subtitles': ['- 我想要走']}\n",
      "{'utterance': ' What’s the protocol for leaving?', 'speaker': 'Sheldon', 'en_subtitles': [\"- What's the protocol for leaving? - I don't know.\"], 'zh_subtitles': ['离开时要怎么说呢?']}\n",
      "{'utterance': ' I don’t know, I’ve never reneged on a proffer of sperm before.', 'speaker': 'Leonard', 'en_subtitles': [\"I've never reneged on a proffer of sperm before.\"], 'zh_subtitles': ['我不知道 我可从没有拒绝过提供精子的要求']}\n",
      "{'utterance': ' Let’s try just walking out.', 'speaker': 'Sheldon', 'en_subtitles': [\"Let's try just walking out.\"], 'zh_subtitles': ['我们就直接走出去吧']}\n",
      "{'utterance': ' Okay.', 'speaker': 'Leonard'}\n",
      "{'utterance': ' Bye-bye', 'speaker': 'Sheldon'}\n",
      "{'utterance': ' See you.', 'speaker': 'Leonard'}\n"
     ]
    }
   ],
   "source": [
    "for x in scene:\n",
    "    print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "sentence = \" So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "trf_parser = spacy.load(\"en_core_web_trf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  _SP\n",
      "So RB\n",
      "if IN\n",
      "a DT\n",
      "photon NN\n",
      "is VBZ\n",
      "directed VBN\n",
      "through IN\n",
      "a DT\n",
      "plane NN\n",
      "with IN\n",
      "two CD\n",
      "slits NNS\n",
      "in IN\n",
      "it PRP\n",
      "and CC\n",
      "either DT\n",
      "slit NN\n",
      "is VBZ\n",
      "observed VBN\n",
      "it PRP\n",
      "will MD\n",
      "not RB\n",
      "go VB\n",
      "through IN\n",
      "both DT\n",
      "slits NNS\n",
      ". .\n",
      "If IN\n",
      "it PRP\n",
      "’s VBZ\n",
      "unobserved JJ\n",
      "it PRP\n",
      "will MD\n",
      ", ,\n",
      "however RB\n",
      ", ,\n",
      "if IN\n",
      "it PRP\n",
      "’s VBZ\n",
      "observed VBN\n",
      "after IN\n",
      "it PRP\n",
      "’s VBZ\n",
      "left VBN\n",
      "the DT\n",
      "plane NN\n",
      "but CC\n",
      "before IN\n",
      "it PRP\n",
      "hits VBZ\n",
      "its PRP$\n",
      "target NN\n",
      ", ,\n",
      "it PRP\n",
      "will MD\n",
      "not RB\n",
      "have VB\n",
      "gone VBN\n",
      "through IN\n",
      "both DT\n",
      "slits NNS\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for token in trf_parser(sentence):\n",
    "    print(token.text, token.tag_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "md_parser = spacy.load('en_core_web_md')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'benepar' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, spancat, textcat_multilabel, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/w9/673mfrb56v7dvx7hhvw7s6240000gn/T/ipykernel_30516/427763387.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Load Berkeley Parser\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mberkeley_parser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspacy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'en_core_web_md'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mberkeley_parser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_pipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"benepar\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m\"model\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"benepar_en3\"\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.conda/envs/multi_coref/lib/python3.8/site-packages/spacy/language.py\u001B[0m in \u001B[0;36madd_pipe\u001B[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001B[0m\n\u001B[1;32m    790\u001B[0m                     \u001B[0mlang_code\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlang\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    791\u001B[0m                 )\n\u001B[0;32m--> 792\u001B[0;31m             pipe_component = self.create_pipe(\n\u001B[0m\u001B[1;32m    793\u001B[0m                 \u001B[0mfactory_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    794\u001B[0m                 \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/multi_coref/lib/python3.8/site-packages/spacy/language.py\u001B[0m in \u001B[0;36mcreate_pipe\u001B[0;34m(self, factory_name, name, config, raw_config, validate)\u001B[0m\n\u001B[1;32m    653\u001B[0m                 \u001B[0mlang_code\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlang\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    654\u001B[0m             )\n\u001B[0;32m--> 655\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    656\u001B[0m         \u001B[0mpipe_meta\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_factory_meta\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfactory_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    657\u001B[0m         \u001B[0;31m# This is unideal, but the alternative would mean you always need to\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: [E002] Can't find factory for 'benepar' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, spancat, textcat_multilabel, en.lemmatizer"
     ]
    }
   ],
   "source": [
    "# Load Berkeley Parser\n",
    "berkeley_parser = spacy.load('en_core_web_md')\n",
    "berkeley_parser.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Stanza Parser\n",
    "stanza_parser = spacy_stanza.load_pipeline('en')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(len(stanza_parser(sentence)))\n",
    "# print(len(berkeley_parser(sentence)))\n",
    "# print(len(trf_parser(sentence)))\n",
    "# print(len(md_parser(sentence)))\n",
    "\n",
    "for a, b, c, d in zip(stanza_parser(sentence), berkeley_parser(sentence), trf_parser(sentence), md_parser(sentence)):\n",
    "    print(a, b, c, d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_spans(scene, parser):\n",
    "    all_sentences = []\n",
    "    all_speakers = []\n",
    "    all_query_spans = []\n",
    "    all_candidate_spans = []\n",
    "    temp = []\n",
    "\n",
    "    for i, utt in enumerate(scene):\n",
    "        speaker = utt['speaker']\n",
    "        transcript = speaker + \" : \" + utt['utterance'].replace(\"-\", \" \")\n",
    "\n",
    "\n",
    "        doc = parser(transcript)\n",
    "\n",
    "        # Load tokens and build instance\n",
    "        sent_tokens = []\n",
    "        for j, item in enumerate(doc):\n",
    "            sent_tokens.append(str(item))\n",
    "        all_sentences.append(sent_tokens)\n",
    "        all_speakers.append(speaker)\n",
    "\n",
    "        # Fetch Noun\n",
    "        noun = set()\n",
    "        for j, item in enumerate(doc):\n",
    "            pos = item.pos_\n",
    "            if pos==\"NOUN\":\n",
    "                noun.add((i, j, j+1))\n",
    "        noun.add((i, 0, 1))\n",
    "\n",
    "        # Fetch Prons\n",
    "        pron = set()\n",
    "        for j, item in enumerate(doc):\n",
    "            pos = item.pos_\n",
    "            if pos==\"PRON\":\n",
    "                pron.add((i, j, j+1))\n",
    "\n",
    "        # Check Noun Phrase\n",
    "        noun_phrases = set()\n",
    "        for item in doc.noun_chunks:\n",
    "            noun_phrases.add((i, item.start, item.end))\n",
    "\n",
    "        # Organize query source\n",
    "        query_source = list(noun_phrases | noun | pron)\n",
    "        query_source.sort(key=lambda x:x[1])\n",
    "\n",
    "        temp.append(query_source)\n",
    "\n",
    "        # Add into Candidate Spans (Noun Phrases + Prons + Nouns)\n",
    "        for (sentenceIndex, startToken, endToken) in query_source:\n",
    "            span = {\n",
    "                \"sentenceIndex\": sentenceIndex,\n",
    "                \"startToken\": startToken,\n",
    "                \"endToken\": endToken\n",
    "            }\n",
    "            all_candidate_spans.append(span)\n",
    "\n",
    "    print(temp)\n",
    "\n",
    "    output = []\n",
    "    for sent in all_sentences:\n",
    "        sent.append(\"\\n\")\n",
    "    temp = {\n",
    "        \"sentences\": all_sentences,\n",
    "        \"querySpans\": all_candidate_spans,\n",
    "        \"candidateSpans\": all_candidate_spans\n",
    "    }\n",
    "    output.append(temp)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def get_spans_multi_parsers(scene, parsers):\n",
    "    all_sentences = []\n",
    "    all_speakers = []\n",
    "    all_query_spans = []\n",
    "    all_candidate_spans = []\n",
    "    temp = []\n",
    "\n",
    "    for i, utt in enumerate(scene):\n",
    "        speaker = utt['speaker']\n",
    "        transcript = speaker + \" : \" + utt['utterance'].replace(\"-\", \" \")\n",
    "\n",
    "\n",
    "        # Load doc sentences and speakers\n",
    "        doc = parsers[0](transcript)\n",
    "        sent_tokens = []\n",
    "        for j, item in enumerate(doc):\n",
    "            sent_tokens.append(str(item))\n",
    "        all_sentences.append(sent_tokens)\n",
    "        all_speakers.append(speaker)\n",
    "\n",
    "        # Parse spans\n",
    "        noun = set()\n",
    "        pron = set()\n",
    "        noun_phrases = set()\n",
    "\n",
    "        for parser in parsers:\n",
    "            doc = parser(transcript)\n",
    "            # Fetch Noun\n",
    "            for j, item in enumerate(doc):\n",
    "                pos = item.pos_\n",
    "                if pos==\"NOUN\":\n",
    "                    noun.add((i, j, j+1))\n",
    "            noun.add((i, 0, 1))\n",
    "\n",
    "            # Fetch Prons\n",
    "            for j, item in enumerate(doc):\n",
    "                pos = item.pos_\n",
    "                if pos==\"PRON\":\n",
    "                    pron.add((i, j, j+1))\n",
    "\n",
    "            # Check Noun Phrase\n",
    "            for item in doc.noun_chunks:\n",
    "                noun_phrases.add((i, item.start, item.end))\n",
    "\n",
    "            print(len(noun), len(pron), len(noun_phrases))\n",
    "        print(\"==\"*50)\n",
    "\n",
    "        # Organize query source\n",
    "        query_source = list(noun_phrases | noun | pron)\n",
    "        query_source.sort(key=lambda x:x[1])\n",
    "\n",
    "        temp.append(query_source)\n",
    "\n",
    "        # Add into Candidate Spans (Noun Phrases + Prons + Nouns)\n",
    "        for (sentenceIndex, startToken, endToken) in query_source:\n",
    "            span = {\n",
    "                \"sentenceIndex\": sentenceIndex,\n",
    "                \"startToken\": startToken,\n",
    "                \"endToken\": endToken\n",
    "            }\n",
    "            all_candidate_spans.append(span)\n",
    "\n",
    "    print(len(temp))\n",
    "    print(temp)\n",
    "\n",
    "    output = []\n",
    "    for sent in all_sentences:\n",
    "        sent.append(\"\\n\")\n",
    "    temp = {\n",
    "        \"sentences\": all_sentences,\n",
    "        \"querySpans\": all_candidate_spans,\n",
    "        \"candidateSpans\": all_candidate_spans\n",
    "    }\n",
    "    output.append(temp)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuanzheng/.conda/envs/multi_coref/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9 17\n",
      "9 9 17\n",
      "====================================================================================================\n",
      "2 2 2\n",
      "2 2 2\n",
      "2 2 2\n",
      "====================================================================================================\n",
      "5 3 5\n",
      "5 3 5\n",
      "5 3 5\n",
      "====================================================================================================\n",
      "1 1 2\n",
      "1 1 2\n",
      "1 1 2\n",
      "====================================================================================================\n",
      "1 0 1\n",
      "1 0 1\n",
      "1 0 1\n",
      "====================================================================================================\n",
      "5 3 10\n",
      "5 3 11\n",
      "5 3 11\n",
      "====================================================================================================\n",
      "1 2 3\n",
      "1 2 3\n",
      "1 2 3\n",
      "====================================================================================================\n",
      "4 1 3\n",
      "4 1 3\n",
      "4 1 3\n",
      "====================================================================================================\n",
      "1 2 3\n",
      "1 2 3\n",
      "1 2 3\n",
      "====================================================================================================\n",
      "2 2 3\n",
      "2 2 3\n",
      "2 2 3\n",
      "====================================================================================================\n",
      "1 1 2\n",
      "1 1 2\n",
      "1 1 2\n",
      "====================================================================================================\n",
      "1 2 3\n",
      "1 2 3\n",
      "1 2 3\n",
      "====================================================================================================\n",
      "3 3 4\n",
      "4 3 4\n",
      "4 3 4\n",
      "====================================================================================================\n",
      "1 3 4\n",
      "1 3 4\n",
      "1 3 4\n",
      "====================================================================================================\n",
      "2 3 4\n",
      "3 3 4\n",
      "3 3 4\n",
      "====================================================================================================\n",
      "8 6 12\n",
      "9 6 12\n",
      "9 6 12\n",
      "====================================================================================================\n",
      "6 2 6\n",
      "6 2 6\n",
      "6 2 6\n",
      "====================================================================================================\n",
      "9 9 14\n",
      "10 9 15\n",
      "10 9 15\n",
      "====================================================================================================\n",
      "1 3 4\n",
      "1 3 4\n",
      "1 3 4\n",
      "====================================================================================================\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "====================================================================================================\n",
      "1 2 2\n",
      "1 2 2\n",
      "1 2 2\n",
      "====================================================================================================\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "====================================================================================================\n",
      "2 1 2\n",
      "2 1 2\n",
      "2 1 2\n",
      "====================================================================================================\n",
      "3 2 4\n",
      "3 2 4\n",
      "3 2 4\n",
      "====================================================================================================\n",
      "1 1 2\n",
      "1 1 2\n",
      "1 1 2\n",
      "====================================================================================================\n",
      "1 0 1\n",
      "1 0 1\n",
      "1 0 1\n",
      "====================================================================================================\n",
      "2 0 1\n",
      "2 0 1\n",
      "2 0 1\n",
      "====================================================================================================\n",
      "1 1 2\n",
      "1 1 2\n",
      "1 1 2\n",
      "====================================================================================================\n",
      "28\n",
      "[[(0, 0, 1), (0, 5, 7), (0, 6, 7), (0, 10, 12), (0, 11, 12), (0, 13, 15), (0, 14, 15), (0, 16, 17), (0, 18, 20), (0, 19, 20), (0, 22, 23), (0, 27, 29), (0, 28, 29), (0, 31, 32), (0, 34, 35), (0, 40, 41), (0, 44, 45), (0, 47, 49), (0, 48, 49), (0, 51, 52), (0, 53, 54), (0, 53, 55), (0, 54, 55), (0, 56, 57), (0, 62, 64), (0, 63, 64)], [(1, 0, 1), (1, 5, 6), (1, 7, 8), (1, 7, 9), (1, 8, 9)], [(2, 0, 1), (2, 3, 4), (2, 5, 7), (2, 6, 7), (2, 8, 9), (2, 11, 12), (2, 13, 16), (2, 15, 16), (2, 17, 20), (2, 18, 19), (2, 19, 20)], [(3, 0, 1), (3, 4, 5)], [(4, 0, 1)], [(5, 0, 1), (5, 6, 7), (5, 11, 12), (5, 17, 18), (5, 24, 25), (5, 24, 26), (5, 25, 26), (5, 27, 28), (5, 29, 30), (5, 33, 36), (5, 39, 44), (5, 42, 43), (5, 43, 44), (5, 45, 46), (5, 47, 50), (5, 51, 52)], [(6, 0, 1), (6, 4, 5), (6, 6, 7)], [(7, 0, 1), (7, 8, 9), (7, 9, 14), (7, 11, 12), (7, 12, 13), (7, 13, 14)], [(8, 0, 1), (8, 4, 5), (8, 10, 11)], [(9, 0, 1), (9, 3, 4), (9, 5, 6), (9, 7, 9), (9, 8, 9)], [(10, 0, 1), (10, 4, 5)], [(11, 0, 1), (11, 4, 5), (11, 6, 7)], [(12, 0, 1), (12, 6, 8), (12, 6, 7), (12, 7, 8), (12, 9, 10), (12, 13, 16), (12, 13, 14), (12, 14, 15), (12, 15, 16)], [(13, 0, 1), (13, 3, 4), (13, 5, 6), (13, 9, 10), (13, 12, 13)], [(14, 0, 1), (14, 3, 4), (14, 6, 7), (14, 9, 10), (14, 11, 14), (14, 12, 13), (14, 13, 14)], [(15, 0, 1), (15, 5, 6), (15, 8, 10), (15, 9, 10), (15, 11, 12), (15, 13, 15), (15, 14, 15), (15, 16, 17), (15, 16, 18), (15, 17, 18), (15, 22, 25), (15, 23, 24), (15, 24, 25), (15, 28, 29), (15, 30, 31), (15, 32, 34), (15, 33, 34), (15, 35, 40), (15, 38, 39), (15, 39, 40), (15, 40, 41), (15, 43, 44)], [(16, 0, 1), (16, 3, 4), (16, 5, 6), (16, 7, 9), (16, 7, 8), (16, 8, 9), (16, 10, 14), (16, 13, 14), (16, 16, 19), (16, 17, 18), (16, 18, 19), (16, 20, 22), (16, 21, 22)], [(17, 0, 1), (17, 3, 4), (17, 7, 8), (17, 11, 13), (17, 12, 13), (17, 15, 16), (17, 17, 20), (17, 19, 20), (17, 24, 26), (17, 24, 25), (17, 25, 26), (17, 27, 28), (17, 27, 29), (17, 28, 29), (17, 30, 31), (17, 32, 33), (17, 36, 38), (17, 37, 38), (17, 38, 39), (17, 43, 44), (17, 46, 48), (17, 47, 48), (17, 49, 51), (17, 50, 51), (17, 53, 55), (17, 54, 55), (17, 56, 58), (17, 57, 58)], [(18, 0, 1), (18, 3, 4), (18, 6, 7), (18, 10, 11)], [(19, 0, 1), (19, 3, 4)], [(20, 0, 1), (20, 5, 6), (20, 7, 8)], [(21, 0, 1), (21, 3, 4)], [(22, 0, 1), (22, 3, 4), (22, 5, 7), (22, 6, 7)], [(23, 0, 1), (23, 3, 4), (23, 8, 9), (23, 13, 15), (23, 14, 15), (23, 16, 17)], [(24, 0, 1), (24, 4, 5)], [(25, 0, 1)], [(26, 0, 1), (26, 4, 5)], [(27, 0, 1), (27, 4, 5)]]\n"
     ]
    }
   ],
   "source": [
    "multi_output = get_spans_multi_parsers(scene, [berkeley_parser, trf_parser, md_parser])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences\n",
      "[['Sheldon', ':', ' ', 'So', 'if', 'a', 'photon', 'is', 'directed', 'through', 'a', 'plane', 'with', 'two', 'slits', 'in', 'it', 'and', 'either', 'slit', 'is', 'observed', 'it', 'will', 'not', 'go', 'through', 'both', 'slits', '.', 'If', 'it', '’s', 'unobserved', 'it', 'will', ',', 'however', ',', 'if', 'it', '’s', 'observed', 'after', 'it', '’s', 'left', 'the', 'plane', 'but', 'before', 'it', 'hits', 'its', 'target', ',', 'it', 'will', 'not', 'have', 'gone', 'through', 'both', 'slits', '.', '\\n'], ['Leonard', ':', ' ', 'Agreed', ',', 'what', '’s', 'your', 'point', '?', '\\n'], ['Sheldon', ':', ' ', 'There', '’s', 'no', 'point', ',', 'I', 'just', 'think', 'it', '’s', 'a', 'good', 'idea', 'for', 'a', 'tee', 'shirt', '.', '\\n'], ['Leonard', ':', ' ', 'Excuse', 'me', '?', '\\n'], ['Receptionist', ':', ' ', 'Hang', 'on', '.', '\\n'], ['Leonard', ':', ' ', 'One', 'across', 'is', 'Aegean', ',', 'eight', 'down', 'is', 'Nabakov', ',', 'twenty', 'six', 'across', 'is', 'MCM', ',', 'fourteen', 'down', 'is', '…', 'move', 'your', 'finger', '…', 'phylum', ',', 'which', 'makes', 'fourteen', 'across', 'Port', 'au', 'Prince', '.', 'See', ',', 'Papa', 'Doc', '’s', 'capital', 'idea', ',', 'that', '’s', 'Port', 'au', 'Prince', '.', 'Haiti', '.', '\\n'], ['Receptionist', ':', ' ', 'Can', 'I', 'help', 'you', '?', '\\n'], ['Leonard', ':', ' ', 'Yes', '.', 'Um', ',', 'is', 'this', 'the', 'High', 'IQ', 'sperm', 'bank', '?', '\\n'], ['Receptionist', ':', ' ', 'If', 'you', 'have', 'to', 'ask', ',', 'maybe', 'you', 'should', 'n’t', 'be', 'here', '.', '\\n'], ['Sheldon', ':', ' ', 'I', 'think', 'this', 'is', 'the', 'place', '.', '\\n'], ['Receptionist', ':', ' ', 'Fill', 'these', 'out', '.', '\\n'], ['Leonard', ':', ' ', 'Thank', 'you', '.', 'We', '’ll', 'be', 'right', 'back', '.', '\\n'], ['Receptionist', ':', ' ', 'Oh', ',', 'take', 'your', 'time', '.', 'I', '’ll', 'just', 'finish', 'my', 'crossword', 'puzzle', '.', 'Oh', 'wait', '.', '\\n'], ['Sheldon', ':', ' ', 'Leonard', ',', 'I', 'do', 'n’t', 'think', 'I', 'can', 'do', 'this', '.', '\\n'], ['Leonard', ':', ' ', 'What', ',', 'are', 'you', 'kidding', '?', 'You', '’re', 'a', 'semi', 'pro', '.', '\\n'], ['Sheldon', ':', ' ', 'No', '.', 'We', 'are', 'committing', 'genetic', 'fraud', '.', 'There', '’s', 'no', 'guarantee', 'that', 'our', 'sperm', 'is', 'going', 'to', 'generate', 'high', 'IQ', 'offspring', ',', 'think', 'about', 'that', '.', 'I', 'have', 'a', 'sister', 'with', 'the', 'same', 'basic', 'DNA', 'mix', 'who', 'hostesses', 'at', 'Fuddruckers', '.', '\\n'], ['Leonard', ':', ' ', 'Sheldon', ',', 'this', 'was', 'your', 'idea', '.', 'A', 'little', 'extra', 'money', 'to', 'get', 'fractional', 'T1', 'bandwidth', 'in', 'the', 'apartment', '.', '\\n'], ['Sheldon', ':', ' ', 'I', 'know', ',', 'and', 'I', 'do', 'yearn', 'for', 'faster', 'downloads', ',', 'but', 'there', '’s', 'some', 'poor', 'woman', 'is', 'going', 'to', 'pin', 'her', 'hopes', 'on', 'my', 'sperm', ',', 'what', 'if', 'she', 'winds', 'up', 'with', 'a', 'toddler', 'who', 'does', 'n’t', 'know', 'if', 'he', 'should', 'use', 'an', 'integral', 'or', 'a', 'differential', 'to', 'solve', 'the', 'area', 'under', 'a', 'curve', '.', '\\n'], ['Leonard', ':', ' ', 'I', '’m', 'sure', 'she', '’ll', 'still', 'love', 'him', '.', '\\n'], ['Sheldon', ':', ' ', 'I', 'would', 'n’t', '.', '\\n'], ['Leonard', ':', ' ', 'Well', ',', 'what', 'do', 'you', 'want', 'to', 'do', '?', '\\n'], ['Sheldon', ':', ' ', 'I', 'want', 'to', 'leave', '.', '\\n'], ['Sheldon', ':', ' ', 'What', '’s', 'the', 'protocol', 'for', 'leaving', '?', '\\n'], ['Leonard', ':', ' ', 'I', 'do', 'n’t', 'know', ',', 'I', '’ve', 'never', 'reneged', 'on', 'a', 'proffer', 'of', 'sperm', 'before', '.', '\\n'], ['Sheldon', ':', ' ', 'Let', '’s', 'try', 'just', 'walking', 'out', '.', '\\n'], ['Leonard', ':', ' ', 'Okay', '.', '\\n'], ['Sheldon', ':', ' ', 'Bye', 'bye', '\\n'], ['Leonard', ':', ' ', 'See', 'you', '.', '\\n']]\n",
      "====================================================================================================\n",
      "querySpans\n",
      "[{'sentenceIndex': 0, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 0, 'startToken': 5, 'endToken': 7}, {'sentenceIndex': 0, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 0, 'startToken': 10, 'endToken': 12}, {'sentenceIndex': 0, 'startToken': 11, 'endToken': 12}, {'sentenceIndex': 0, 'startToken': 13, 'endToken': 15}, {'sentenceIndex': 0, 'startToken': 14, 'endToken': 15}, {'sentenceIndex': 0, 'startToken': 16, 'endToken': 17}, {'sentenceIndex': 0, 'startToken': 18, 'endToken': 20}, {'sentenceIndex': 0, 'startToken': 19, 'endToken': 20}, {'sentenceIndex': 0, 'startToken': 22, 'endToken': 23}, {'sentenceIndex': 0, 'startToken': 27, 'endToken': 29}, {'sentenceIndex': 0, 'startToken': 28, 'endToken': 29}, {'sentenceIndex': 0, 'startToken': 31, 'endToken': 32}, {'sentenceIndex': 0, 'startToken': 34, 'endToken': 35}, {'sentenceIndex': 0, 'startToken': 40, 'endToken': 41}, {'sentenceIndex': 0, 'startToken': 44, 'endToken': 45}, {'sentenceIndex': 0, 'startToken': 47, 'endToken': 49}, {'sentenceIndex': 0, 'startToken': 48, 'endToken': 49}, {'sentenceIndex': 0, 'startToken': 51, 'endToken': 52}, {'sentenceIndex': 0, 'startToken': 53, 'endToken': 54}, {'sentenceIndex': 0, 'startToken': 53, 'endToken': 55}, {'sentenceIndex': 0, 'startToken': 54, 'endToken': 55}, {'sentenceIndex': 0, 'startToken': 56, 'endToken': 57}, {'sentenceIndex': 0, 'startToken': 62, 'endToken': 64}, {'sentenceIndex': 0, 'startToken': 63, 'endToken': 64}, {'sentenceIndex': 1, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 1, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 1, 'startToken': 7, 'endToken': 8}, {'sentenceIndex': 1, 'startToken': 7, 'endToken': 9}, {'sentenceIndex': 1, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 2, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 2, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 2, 'startToken': 5, 'endToken': 7}, {'sentenceIndex': 2, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 2, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 2, 'startToken': 11, 'endToken': 12}, {'sentenceIndex': 2, 'startToken': 13, 'endToken': 16}, {'sentenceIndex': 2, 'startToken': 15, 'endToken': 16}, {'sentenceIndex': 2, 'startToken': 17, 'endToken': 20}, {'sentenceIndex': 2, 'startToken': 18, 'endToken': 19}, {'sentenceIndex': 2, 'startToken': 19, 'endToken': 20}, {'sentenceIndex': 3, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 3, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 4, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 5, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 5, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 5, 'startToken': 11, 'endToken': 12}, {'sentenceIndex': 5, 'startToken': 17, 'endToken': 18}, {'sentenceIndex': 5, 'startToken': 24, 'endToken': 25}, {'sentenceIndex': 5, 'startToken': 24, 'endToken': 26}, {'sentenceIndex': 5, 'startToken': 25, 'endToken': 26}, {'sentenceIndex': 5, 'startToken': 27, 'endToken': 28}, {'sentenceIndex': 5, 'startToken': 29, 'endToken': 30}, {'sentenceIndex': 5, 'startToken': 33, 'endToken': 36}, {'sentenceIndex': 5, 'startToken': 39, 'endToken': 44}, {'sentenceIndex': 5, 'startToken': 42, 'endToken': 43}, {'sentenceIndex': 5, 'startToken': 43, 'endToken': 44}, {'sentenceIndex': 5, 'startToken': 45, 'endToken': 46}, {'sentenceIndex': 5, 'startToken': 47, 'endToken': 50}, {'sentenceIndex': 5, 'startToken': 51, 'endToken': 52}, {'sentenceIndex': 6, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 6, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 6, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 7, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 7, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 7, 'startToken': 9, 'endToken': 14}, {'sentenceIndex': 7, 'startToken': 11, 'endToken': 12}, {'sentenceIndex': 7, 'startToken': 12, 'endToken': 13}, {'sentenceIndex': 7, 'startToken': 13, 'endToken': 14}, {'sentenceIndex': 8, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 8, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 8, 'startToken': 10, 'endToken': 11}, {'sentenceIndex': 9, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 9, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 9, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 9, 'startToken': 7, 'endToken': 9}, {'sentenceIndex': 9, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 10, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 10, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 11, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 11, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 11, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 12, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 12, 'startToken': 6, 'endToken': 8}, {'sentenceIndex': 12, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 12, 'startToken': 7, 'endToken': 8}, {'sentenceIndex': 12, 'startToken': 9, 'endToken': 10}, {'sentenceIndex': 12, 'startToken': 13, 'endToken': 16}, {'sentenceIndex': 12, 'startToken': 13, 'endToken': 14}, {'sentenceIndex': 12, 'startToken': 14, 'endToken': 15}, {'sentenceIndex': 12, 'startToken': 15, 'endToken': 16}, {'sentenceIndex': 13, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 13, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 13, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 13, 'startToken': 9, 'endToken': 10}, {'sentenceIndex': 13, 'startToken': 12, 'endToken': 13}, {'sentenceIndex': 14, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 14, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 14, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 14, 'startToken': 9, 'endToken': 10}, {'sentenceIndex': 14, 'startToken': 11, 'endToken': 14}, {'sentenceIndex': 14, 'startToken': 12, 'endToken': 13}, {'sentenceIndex': 14, 'startToken': 13, 'endToken': 14}, {'sentenceIndex': 15, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 15, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 15, 'startToken': 8, 'endToken': 10}, {'sentenceIndex': 15, 'startToken': 9, 'endToken': 10}, {'sentenceIndex': 15, 'startToken': 11, 'endToken': 12}, {'sentenceIndex': 15, 'startToken': 13, 'endToken': 15}, {'sentenceIndex': 15, 'startToken': 14, 'endToken': 15}, {'sentenceIndex': 15, 'startToken': 16, 'endToken': 17}, {'sentenceIndex': 15, 'startToken': 16, 'endToken': 18}, {'sentenceIndex': 15, 'startToken': 17, 'endToken': 18}, {'sentenceIndex': 15, 'startToken': 22, 'endToken': 25}, {'sentenceIndex': 15, 'startToken': 23, 'endToken': 24}, {'sentenceIndex': 15, 'startToken': 24, 'endToken': 25}, {'sentenceIndex': 15, 'startToken': 28, 'endToken': 29}, {'sentenceIndex': 15, 'startToken': 30, 'endToken': 31}, {'sentenceIndex': 15, 'startToken': 32, 'endToken': 34}, {'sentenceIndex': 15, 'startToken': 33, 'endToken': 34}, {'sentenceIndex': 15, 'startToken': 35, 'endToken': 40}, {'sentenceIndex': 15, 'startToken': 38, 'endToken': 39}, {'sentenceIndex': 15, 'startToken': 39, 'endToken': 40}, {'sentenceIndex': 15, 'startToken': 40, 'endToken': 41}, {'sentenceIndex': 15, 'startToken': 43, 'endToken': 44}, {'sentenceIndex': 16, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 16, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 16, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 16, 'startToken': 7, 'endToken': 9}, {'sentenceIndex': 16, 'startToken': 7, 'endToken': 8}, {'sentenceIndex': 16, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 16, 'startToken': 10, 'endToken': 14}, {'sentenceIndex': 16, 'startToken': 13, 'endToken': 14}, {'sentenceIndex': 16, 'startToken': 16, 'endToken': 19}, {'sentenceIndex': 16, 'startToken': 17, 'endToken': 18}, {'sentenceIndex': 16, 'startToken': 18, 'endToken': 19}, {'sentenceIndex': 16, 'startToken': 20, 'endToken': 22}, {'sentenceIndex': 16, 'startToken': 21, 'endToken': 22}, {'sentenceIndex': 17, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 17, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 17, 'startToken': 7, 'endToken': 8}, {'sentenceIndex': 17, 'startToken': 11, 'endToken': 13}, {'sentenceIndex': 17, 'startToken': 12, 'endToken': 13}, {'sentenceIndex': 17, 'startToken': 15, 'endToken': 16}, {'sentenceIndex': 17, 'startToken': 17, 'endToken': 20}, {'sentenceIndex': 17, 'startToken': 19, 'endToken': 20}, {'sentenceIndex': 17, 'startToken': 24, 'endToken': 26}, {'sentenceIndex': 17, 'startToken': 24, 'endToken': 25}, {'sentenceIndex': 17, 'startToken': 25, 'endToken': 26}, {'sentenceIndex': 17, 'startToken': 27, 'endToken': 28}, {'sentenceIndex': 17, 'startToken': 27, 'endToken': 29}, {'sentenceIndex': 17, 'startToken': 28, 'endToken': 29}, {'sentenceIndex': 17, 'startToken': 30, 'endToken': 31}, {'sentenceIndex': 17, 'startToken': 32, 'endToken': 33}, {'sentenceIndex': 17, 'startToken': 36, 'endToken': 38}, {'sentenceIndex': 17, 'startToken': 37, 'endToken': 38}, {'sentenceIndex': 17, 'startToken': 38, 'endToken': 39}, {'sentenceIndex': 17, 'startToken': 43, 'endToken': 44}, {'sentenceIndex': 17, 'startToken': 46, 'endToken': 48}, {'sentenceIndex': 17, 'startToken': 47, 'endToken': 48}, {'sentenceIndex': 17, 'startToken': 49, 'endToken': 51}, {'sentenceIndex': 17, 'startToken': 50, 'endToken': 51}, {'sentenceIndex': 17, 'startToken': 53, 'endToken': 55}, {'sentenceIndex': 17, 'startToken': 54, 'endToken': 55}, {'sentenceIndex': 17, 'startToken': 56, 'endToken': 58}, {'sentenceIndex': 17, 'startToken': 57, 'endToken': 58}, {'sentenceIndex': 18, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 18, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 18, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 18, 'startToken': 10, 'endToken': 11}, {'sentenceIndex': 19, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 19, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 20, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 20, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 20, 'startToken': 7, 'endToken': 8}, {'sentenceIndex': 21, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 21, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 22, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 22, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 22, 'startToken': 5, 'endToken': 7}, {'sentenceIndex': 22, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 23, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 23, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 23, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 23, 'startToken': 13, 'endToken': 15}, {'sentenceIndex': 23, 'startToken': 14, 'endToken': 15}, {'sentenceIndex': 23, 'startToken': 16, 'endToken': 17}, {'sentenceIndex': 24, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 24, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 25, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 26, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 26, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 27, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 27, 'startToken': 4, 'endToken': 5}]\n",
      "====================================================================================================\n",
      "candidateSpans\n",
      "[{'sentenceIndex': 0, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 0, 'startToken': 5, 'endToken': 7}, {'sentenceIndex': 0, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 0, 'startToken': 10, 'endToken': 12}, {'sentenceIndex': 0, 'startToken': 11, 'endToken': 12}, {'sentenceIndex': 0, 'startToken': 13, 'endToken': 15}, {'sentenceIndex': 0, 'startToken': 14, 'endToken': 15}, {'sentenceIndex': 0, 'startToken': 16, 'endToken': 17}, {'sentenceIndex': 0, 'startToken': 18, 'endToken': 20}, {'sentenceIndex': 0, 'startToken': 19, 'endToken': 20}, {'sentenceIndex': 0, 'startToken': 22, 'endToken': 23}, {'sentenceIndex': 0, 'startToken': 27, 'endToken': 29}, {'sentenceIndex': 0, 'startToken': 28, 'endToken': 29}, {'sentenceIndex': 0, 'startToken': 31, 'endToken': 32}, {'sentenceIndex': 0, 'startToken': 34, 'endToken': 35}, {'sentenceIndex': 0, 'startToken': 40, 'endToken': 41}, {'sentenceIndex': 0, 'startToken': 44, 'endToken': 45}, {'sentenceIndex': 0, 'startToken': 47, 'endToken': 49}, {'sentenceIndex': 0, 'startToken': 48, 'endToken': 49}, {'sentenceIndex': 0, 'startToken': 51, 'endToken': 52}, {'sentenceIndex': 0, 'startToken': 53, 'endToken': 54}, {'sentenceIndex': 0, 'startToken': 53, 'endToken': 55}, {'sentenceIndex': 0, 'startToken': 54, 'endToken': 55}, {'sentenceIndex': 0, 'startToken': 56, 'endToken': 57}, {'sentenceIndex': 0, 'startToken': 62, 'endToken': 64}, {'sentenceIndex': 0, 'startToken': 63, 'endToken': 64}, {'sentenceIndex': 1, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 1, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 1, 'startToken': 7, 'endToken': 8}, {'sentenceIndex': 1, 'startToken': 7, 'endToken': 9}, {'sentenceIndex': 1, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 2, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 2, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 2, 'startToken': 5, 'endToken': 7}, {'sentenceIndex': 2, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 2, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 2, 'startToken': 11, 'endToken': 12}, {'sentenceIndex': 2, 'startToken': 13, 'endToken': 16}, {'sentenceIndex': 2, 'startToken': 15, 'endToken': 16}, {'sentenceIndex': 2, 'startToken': 17, 'endToken': 20}, {'sentenceIndex': 2, 'startToken': 18, 'endToken': 19}, {'sentenceIndex': 2, 'startToken': 19, 'endToken': 20}, {'sentenceIndex': 3, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 3, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 4, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 5, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 5, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 5, 'startToken': 11, 'endToken': 12}, {'sentenceIndex': 5, 'startToken': 17, 'endToken': 18}, {'sentenceIndex': 5, 'startToken': 24, 'endToken': 25}, {'sentenceIndex': 5, 'startToken': 24, 'endToken': 26}, {'sentenceIndex': 5, 'startToken': 25, 'endToken': 26}, {'sentenceIndex': 5, 'startToken': 27, 'endToken': 28}, {'sentenceIndex': 5, 'startToken': 29, 'endToken': 30}, {'sentenceIndex': 5, 'startToken': 33, 'endToken': 36}, {'sentenceIndex': 5, 'startToken': 39, 'endToken': 44}, {'sentenceIndex': 5, 'startToken': 42, 'endToken': 43}, {'sentenceIndex': 5, 'startToken': 43, 'endToken': 44}, {'sentenceIndex': 5, 'startToken': 45, 'endToken': 46}, {'sentenceIndex': 5, 'startToken': 47, 'endToken': 50}, {'sentenceIndex': 5, 'startToken': 51, 'endToken': 52}, {'sentenceIndex': 6, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 6, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 6, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 7, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 7, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 7, 'startToken': 9, 'endToken': 14}, {'sentenceIndex': 7, 'startToken': 11, 'endToken': 12}, {'sentenceIndex': 7, 'startToken': 12, 'endToken': 13}, {'sentenceIndex': 7, 'startToken': 13, 'endToken': 14}, {'sentenceIndex': 8, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 8, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 8, 'startToken': 10, 'endToken': 11}, {'sentenceIndex': 9, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 9, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 9, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 9, 'startToken': 7, 'endToken': 9}, {'sentenceIndex': 9, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 10, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 10, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 11, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 11, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 11, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 12, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 12, 'startToken': 6, 'endToken': 8}, {'sentenceIndex': 12, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 12, 'startToken': 7, 'endToken': 8}, {'sentenceIndex': 12, 'startToken': 9, 'endToken': 10}, {'sentenceIndex': 12, 'startToken': 13, 'endToken': 16}, {'sentenceIndex': 12, 'startToken': 13, 'endToken': 14}, {'sentenceIndex': 12, 'startToken': 14, 'endToken': 15}, {'sentenceIndex': 12, 'startToken': 15, 'endToken': 16}, {'sentenceIndex': 13, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 13, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 13, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 13, 'startToken': 9, 'endToken': 10}, {'sentenceIndex': 13, 'startToken': 12, 'endToken': 13}, {'sentenceIndex': 14, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 14, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 14, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 14, 'startToken': 9, 'endToken': 10}, {'sentenceIndex': 14, 'startToken': 11, 'endToken': 14}, {'sentenceIndex': 14, 'startToken': 12, 'endToken': 13}, {'sentenceIndex': 14, 'startToken': 13, 'endToken': 14}, {'sentenceIndex': 15, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 15, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 15, 'startToken': 8, 'endToken': 10}, {'sentenceIndex': 15, 'startToken': 9, 'endToken': 10}, {'sentenceIndex': 15, 'startToken': 11, 'endToken': 12}, {'sentenceIndex': 15, 'startToken': 13, 'endToken': 15}, {'sentenceIndex': 15, 'startToken': 14, 'endToken': 15}, {'sentenceIndex': 15, 'startToken': 16, 'endToken': 17}, {'sentenceIndex': 15, 'startToken': 16, 'endToken': 18}, {'sentenceIndex': 15, 'startToken': 17, 'endToken': 18}, {'sentenceIndex': 15, 'startToken': 22, 'endToken': 25}, {'sentenceIndex': 15, 'startToken': 23, 'endToken': 24}, {'sentenceIndex': 15, 'startToken': 24, 'endToken': 25}, {'sentenceIndex': 15, 'startToken': 28, 'endToken': 29}, {'sentenceIndex': 15, 'startToken': 30, 'endToken': 31}, {'sentenceIndex': 15, 'startToken': 32, 'endToken': 34}, {'sentenceIndex': 15, 'startToken': 33, 'endToken': 34}, {'sentenceIndex': 15, 'startToken': 35, 'endToken': 40}, {'sentenceIndex': 15, 'startToken': 38, 'endToken': 39}, {'sentenceIndex': 15, 'startToken': 39, 'endToken': 40}, {'sentenceIndex': 15, 'startToken': 40, 'endToken': 41}, {'sentenceIndex': 15, 'startToken': 43, 'endToken': 44}, {'sentenceIndex': 16, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 16, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 16, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 16, 'startToken': 7, 'endToken': 9}, {'sentenceIndex': 16, 'startToken': 7, 'endToken': 8}, {'sentenceIndex': 16, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 16, 'startToken': 10, 'endToken': 14}, {'sentenceIndex': 16, 'startToken': 13, 'endToken': 14}, {'sentenceIndex': 16, 'startToken': 16, 'endToken': 19}, {'sentenceIndex': 16, 'startToken': 17, 'endToken': 18}, {'sentenceIndex': 16, 'startToken': 18, 'endToken': 19}, {'sentenceIndex': 16, 'startToken': 20, 'endToken': 22}, {'sentenceIndex': 16, 'startToken': 21, 'endToken': 22}, {'sentenceIndex': 17, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 17, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 17, 'startToken': 7, 'endToken': 8}, {'sentenceIndex': 17, 'startToken': 11, 'endToken': 13}, {'sentenceIndex': 17, 'startToken': 12, 'endToken': 13}, {'sentenceIndex': 17, 'startToken': 15, 'endToken': 16}, {'sentenceIndex': 17, 'startToken': 17, 'endToken': 20}, {'sentenceIndex': 17, 'startToken': 19, 'endToken': 20}, {'sentenceIndex': 17, 'startToken': 24, 'endToken': 26}, {'sentenceIndex': 17, 'startToken': 24, 'endToken': 25}, {'sentenceIndex': 17, 'startToken': 25, 'endToken': 26}, {'sentenceIndex': 17, 'startToken': 27, 'endToken': 28}, {'sentenceIndex': 17, 'startToken': 27, 'endToken': 29}, {'sentenceIndex': 17, 'startToken': 28, 'endToken': 29}, {'sentenceIndex': 17, 'startToken': 30, 'endToken': 31}, {'sentenceIndex': 17, 'startToken': 32, 'endToken': 33}, {'sentenceIndex': 17, 'startToken': 36, 'endToken': 38}, {'sentenceIndex': 17, 'startToken': 37, 'endToken': 38}, {'sentenceIndex': 17, 'startToken': 38, 'endToken': 39}, {'sentenceIndex': 17, 'startToken': 43, 'endToken': 44}, {'sentenceIndex': 17, 'startToken': 46, 'endToken': 48}, {'sentenceIndex': 17, 'startToken': 47, 'endToken': 48}, {'sentenceIndex': 17, 'startToken': 49, 'endToken': 51}, {'sentenceIndex': 17, 'startToken': 50, 'endToken': 51}, {'sentenceIndex': 17, 'startToken': 53, 'endToken': 55}, {'sentenceIndex': 17, 'startToken': 54, 'endToken': 55}, {'sentenceIndex': 17, 'startToken': 56, 'endToken': 58}, {'sentenceIndex': 17, 'startToken': 57, 'endToken': 58}, {'sentenceIndex': 18, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 18, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 18, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 18, 'startToken': 10, 'endToken': 11}, {'sentenceIndex': 19, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 19, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 20, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 20, 'startToken': 5, 'endToken': 6}, {'sentenceIndex': 20, 'startToken': 7, 'endToken': 8}, {'sentenceIndex': 21, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 21, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 22, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 22, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 22, 'startToken': 5, 'endToken': 7}, {'sentenceIndex': 22, 'startToken': 6, 'endToken': 7}, {'sentenceIndex': 23, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 23, 'startToken': 3, 'endToken': 4}, {'sentenceIndex': 23, 'startToken': 8, 'endToken': 9}, {'sentenceIndex': 23, 'startToken': 13, 'endToken': 15}, {'sentenceIndex': 23, 'startToken': 14, 'endToken': 15}, {'sentenceIndex': 23, 'startToken': 16, 'endToken': 17}, {'sentenceIndex': 24, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 24, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 25, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 26, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 26, 'startToken': 4, 'endToken': 5}, {'sentenceIndex': 27, 'startToken': 0, 'endToken': 1}, {'sentenceIndex': 27, 'startToken': 4, 'endToken': 5}]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for instance in multi_output[0]:\n",
    "    print(instance)\n",
    "    print(multi_output[0][instance])\n",
    "    print(\"==\"*50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(type(multi_output))\n",
    "print(len(multi_output))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences\n",
      "querySpans\n",
      "candidateSpans\n"
     ]
    }
   ],
   "source": [
    "for item in multi_output[0]:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "198\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(multi_output[0]['querySpans']))\n",
    "print(len(multi_output[0]['candidateSpans']))\n",
    "print(len(multi_output[0]['sentences']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "sentences = stanza_output['sentences']\n",
    "querySpans = stanza_output['querySpans']\n",
    "candidateSpans = stanza_output['candidateSpans']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\n",
      "['Sheldon', ':', ' ', 'So', 'if', 'a', 'photon', 'is', 'directed', 'through', 'a', 'plane', 'with', 'two', 'slits', 'in', 'it', 'and', 'either', 'slit', 'is', 'observed', 'it', 'will', 'not', 'go', 'through', 'both', 'slits', '.', 'If', 'it', '’s', 'unobserved', 'it', 'will', ',', 'however', ',', 'if', 'it', '’s', 'observed', 'after', 'it', '’s', 'left', 'the', 'plane', 'but', 'before', 'it', 'hits', 'its', 'target', ',', 'it', 'will', 'not', 'have', 'gone', 'through', 'both', 'slits', '.', '\\n']\n",
      "['Leonard', ':', ' ', 'Agreed', ',', 'what', '’s', 'your', 'point', '?', '\\n']\n",
      "['Sheldon', ':', ' ', 'There', '’s', 'no', 'point', ',', 'I', 'just', 'think', 'it', '’s', 'a', 'good', 'idea', 'for', 'a', 'tee', 'shirt', '.', '\\n']\n",
      "['Leonard', ':', ' ', 'Excuse', 'me', '?', '\\n']\n",
      "['Receptionist', ':', ' ', 'Hang', 'on', '.', '\\n']\n",
      "['Leonard', ':', ' ', 'One', 'across', 'is', 'Aegean', ',', 'eight', 'down', 'is', 'Nabakov', ',', 'twenty', 'six', 'across', 'is', 'MCM', ',', 'fourteen', 'down', 'is', '…', 'move', 'your', 'finger', '…', 'phylum', ',', 'which', 'makes', 'fourteen', 'across', 'Port', 'au', 'Prince', '.', 'See', ',', 'Papa', 'Doc', '’s', 'capital', 'idea', ',', 'that', '’s', 'Port', 'au', 'Prince', '.', 'Haiti', '.', '\\n']\n",
      "['Receptionist', ':', ' ', 'Can', 'I', 'help', 'you', '?', '\\n']\n",
      "['Leonard', ':', ' ', 'Yes', '.', 'Um', ',', 'is', 'this', 'the', 'High', 'IQ', 'sperm', 'bank', '?', '\\n']\n",
      "['Receptionist', ':', ' ', 'If', 'you', 'have', 'to', 'ask', ',', 'maybe', 'you', 'should', 'n’t', 'be', 'here', '.', '\\n']\n",
      "['Sheldon', ':', ' ', 'I', 'think', 'this', 'is', 'the', 'place', '.', '\\n']\n",
      "['Receptionist', ':', ' ', 'Fill', 'these', 'out', '.', '\\n']\n",
      "['Leonard', ':', ' ', 'Thank', 'you', '.', 'We', '’ll', 'be', 'right', 'back', '.', '\\n']\n",
      "['Receptionist', ':', ' ', 'Oh', ',', 'take', 'your', 'time', '.', 'I', '’ll', 'just', 'finish', 'my', 'crossword', 'puzzle', '.', 'Oh', 'wait', '.', '\\n']\n",
      "['Sheldon', ':', ' ', 'Leonard', ',', 'I', 'do', 'n’t', 'think', 'I', 'can', 'do', 'this', '.', '\\n']\n",
      "['Leonard', ':', ' ', 'What', ',', 'are', 'you', 'kidding', '?', 'You', '’re', 'a', 'semi', 'pro', '.', '\\n']\n",
      "['Sheldon', ':', ' ', 'No', '.', 'We', 'are', 'committing', 'genetic', 'fraud', '.', 'There', '’s', 'no', 'guarantee', 'that', 'our', 'sperm', 'is', 'going', 'to', 'generate', 'high', 'IQ', 'offspring', ',', 'think', 'about', 'that', '.', 'I', 'have', 'a', 'sister', 'with', 'the', 'same', 'basic', 'DNA', 'mix', 'who', 'hostesses', 'at', 'Fuddruckers', '.', '\\n']\n",
      "['Leonard', ':', ' ', 'Sheldon', ',', 'this', 'was', 'your', 'idea', '.', 'A', 'little', 'extra', 'money', 'to', 'get', 'fractional', 'T1', 'bandwidth', 'in', 'the', 'apartment', '.', '\\n']\n",
      "['Sheldon', ':', ' ', 'I', 'know', ',', 'and', 'I', 'do', 'yearn', 'for', 'faster', 'downloads', ',', 'but', 'there', '’s', 'some', 'poor', 'woman', 'is', 'going', 'to', 'pin', 'her', 'hopes', 'on', 'my', 'sperm', ',', 'what', 'if', 'she', 'winds', 'up', 'with', 'a', 'toddler', 'who', 'does', 'n’t', 'know', 'if', 'he', 'should', 'use', 'an', 'integral', 'or', 'a', 'differential', 'to', 'solve', 'the', 'area', 'under', 'a', 'curve', '.', '\\n']\n",
      "['Leonard', ':', ' ', 'I', '’m', 'sure', 'she', '’ll', 'still', 'love', 'him', '.', '\\n']\n",
      "['Sheldon', ':', ' ', 'I', 'would', 'n’t', '.', '\\n']\n",
      "['Leonard', ':', ' ', 'Well', ',', 'what', 'do', 'you', 'want', 'to', 'do', '?', '\\n']\n",
      "['Sheldon', ':', ' ', 'I', 'want', 'to', 'leave', '.', '\\n']\n",
      "['Sheldon', ':', ' ', 'What', '’s', 'the', 'protocol', 'for', 'leaving', '?', '\\n']\n",
      "['Leonard', ':', ' ', 'I', 'do', 'n’t', 'know', ',', 'I', '’ve', 'never', 'reneged', 'on', 'a', 'proffer', 'of', 'sperm', 'before', '.', '\\n']\n",
      "['Sheldon', ':', ' ', 'Let', '’s', 'try', 'just', 'walking', 'out', '.', '\\n']\n",
      "['Leonard', ':', ' ', 'Okay', '.', '\\n']\n",
      "['Sheldon', ':', ' ', 'Bye', 'bye', '\\n']\n",
      "['Leonard', ':', ' ', 'See', 'you', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence:\")\n",
    "for item in sentences:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Spans:\n",
      "{'sentenceIndex': 0, 'startToken': 54, 'endToken': 55} ['target']\n",
      "{'sentenceIndex': 0, 'startToken': 22, 'endToken': 23} ['it']\n",
      "{'sentenceIndex': 0, 'startToken': 48, 'endToken': 49} ['plane']\n",
      "{'sentenceIndex': 0, 'startToken': 34, 'endToken': 35} ['it']\n",
      "{'sentenceIndex': 0, 'startToken': 44, 'endToken': 45} ['it']\n",
      "{'sentenceIndex': 0, 'startToken': 53, 'endToken': 54} ['its']\n",
      "{'sentenceIndex': 0, 'startToken': 16, 'endToken': 17} ['it']\n",
      "{'sentenceIndex': 0, 'startToken': 56, 'endToken': 57} ['it']\n",
      "{'sentenceIndex': 0, 'startToken': 51, 'endToken': 52} ['it']\n",
      "{'sentenceIndex': 0, 'startToken': 28, 'endToken': 29} ['slits']\n",
      "{'sentenceIndex': 0, 'startToken': 19, 'endToken': 20} ['slit']\n",
      "{'sentenceIndex': 0, 'startToken': 14, 'endToken': 15} ['slits']\n",
      "{'sentenceIndex': 0, 'startToken': 6, 'endToken': 7} ['photon']\n",
      "{'sentenceIndex': 0, 'startToken': 63, 'endToken': 64} ['slits']\n",
      "{'sentenceIndex': 0, 'startToken': 31, 'endToken': 32} ['it']\n",
      "{'sentenceIndex': 0, 'startToken': 40, 'endToken': 41} ['it']\n",
      "{'sentenceIndex': 0, 'startToken': 11, 'endToken': 12} ['plane']\n",
      "{'sentenceIndex': 1, 'startToken': 5, 'endToken': 6} ['what']\n",
      "{'sentenceIndex': 1, 'startToken': 7, 'endToken': 9} ['your', 'point']\n",
      "{'sentenceIndex': 1, 'startToken': 7, 'endToken': 8} ['your']\n",
      "{'sentenceIndex': 1, 'startToken': 8, 'endToken': 9} ['point']\n",
      "{'sentenceIndex': 2, 'startToken': 11, 'endToken': 12} ['it']\n",
      "{'sentenceIndex': 2, 'startToken': 6, 'endToken': 7} ['point']\n",
      "{'sentenceIndex': 2, 'startToken': 15, 'endToken': 16} ['idea']\n",
      "{'sentenceIndex': 2, 'startToken': 8, 'endToken': 9} ['I']\n",
      "{'sentenceIndex': 2, 'startToken': 5, 'endToken': 7} ['no', 'point']\n",
      "{'sentenceIndex': 2, 'startToken': 3, 'endToken': 4} ['There']\n",
      "{'sentenceIndex': 2, 'startToken': 19, 'endToken': 20} ['shirt']\n",
      "{'sentenceIndex': 2, 'startToken': 18, 'endToken': 19} ['tee']\n",
      "{'sentenceIndex': 3, 'startToken': 4, 'endToken': 5} ['me']\n",
      "{'sentenceIndex': 4, 'startToken': 0, 'endToken': 1} ['Receptionist']\n",
      "{'sentenceIndex': 5, 'startToken': 26, 'endToken': 28} ['…', 'phylum']\n",
      "{'sentenceIndex': 5, 'startToken': 43, 'endToken': 44} ['idea']\n",
      "{'sentenceIndex': 5, 'startToken': 24, 'endToken': 25} ['your']\n",
      "{'sentenceIndex': 5, 'startToken': 27, 'endToken': 28} ['phylum']\n",
      "{'sentenceIndex': 5, 'startToken': 25, 'endToken': 26} ['finger']\n",
      "{'sentenceIndex': 5, 'startToken': 45, 'endToken': 46} ['that']\n",
      "{'sentenceIndex': 5, 'startToken': 29, 'endToken': 30} ['which']\n",
      "{'sentenceIndex': 6, 'startToken': 0, 'endToken': 1} ['Receptionist']\n",
      "{'sentenceIndex': 6, 'startToken': 4, 'endToken': 5} ['I']\n",
      "{'sentenceIndex': 6, 'startToken': 6, 'endToken': 7} ['you']\n",
      "{'sentenceIndex': 7, 'startToken': 12, 'endToken': 13} ['sperm']\n",
      "{'sentenceIndex': 7, 'startToken': 11, 'endToken': 12} ['IQ']\n",
      "{'sentenceIndex': 7, 'startToken': 13, 'endToken': 14} ['bank']\n",
      "{'sentenceIndex': 7, 'startToken': 8, 'endToken': 9} ['this']\n",
      "{'sentenceIndex': 8, 'startToken': 0, 'endToken': 1} ['Receptionist']\n",
      "{'sentenceIndex': 8, 'startToken': 4, 'endToken': 5} ['you']\n",
      "{'sentenceIndex': 8, 'startToken': 10, 'endToken': 11} ['you']\n",
      "{'sentenceIndex': 9, 'startToken': 8, 'endToken': 9} ['place']\n",
      "{'sentenceIndex': 9, 'startToken': 3, 'endToken': 4} ['I']\n",
      "{'sentenceIndex': 9, 'startToken': 5, 'endToken': 6} ['this']\n",
      "{'sentenceIndex': 10, 'startToken': 0, 'endToken': 1} ['Receptionist']\n",
      "{'sentenceIndex': 10, 'startToken': 4, 'endToken': 5} ['these']\n",
      "{'sentenceIndex': 11, 'startToken': 4, 'endToken': 5} ['you']\n",
      "{'sentenceIndex': 11, 'startToken': 6, 'endToken': 7} ['We']\n",
      "{'sentenceIndex': 12, 'startToken': 7, 'endToken': 8} ['time']\n",
      "{'sentenceIndex': 12, 'startToken': 15, 'endToken': 16} ['puzzle']\n",
      "{'sentenceIndex': 12, 'startToken': 0, 'endToken': 1} ['Receptionist']\n",
      "{'sentenceIndex': 12, 'startToken': 14, 'endToken': 15} ['crossword']\n",
      "{'sentenceIndex': 12, 'startToken': 9, 'endToken': 10} ['I']\n",
      "{'sentenceIndex': 12, 'startToken': 13, 'endToken': 14} ['my']\n",
      "{'sentenceIndex': 12, 'startToken': 6, 'endToken': 7} ['your']\n",
      "{'sentenceIndex': 13, 'startToken': 5, 'endToken': 6} ['I']\n",
      "{'sentenceIndex': 13, 'startToken': 3, 'endToken': 4} ['Leonard']\n",
      "{'sentenceIndex': 13, 'startToken': 12, 'endToken': 13} ['this']\n",
      "{'sentenceIndex': 13, 'startToken': 9, 'endToken': 10} ['I']\n",
      "{'sentenceIndex': 14, 'startToken': 9, 'endToken': 10} ['You']\n",
      "{'sentenceIndex': 14, 'startToken': 13, 'endToken': 14} ['pro']\n",
      "{'sentenceIndex': 14, 'startToken': 6, 'endToken': 7} ['you']\n",
      "{'sentenceIndex': 14, 'startToken': 12, 'endToken': 13} ['semi']\n",
      "{'sentenceIndex': 14, 'startToken': 3, 'endToken': 4} ['What']\n",
      "{'sentenceIndex': 15, 'startToken': 11, 'endToken': 12} ['There']\n",
      "{'sentenceIndex': 15, 'startToken': 13, 'endToken': 15} ['no', 'guarantee']\n",
      "{'sentenceIndex': 15, 'startToken': 30, 'endToken': 31} ['I']\n",
      "{'sentenceIndex': 15, 'startToken': 5, 'endToken': 6} ['We']\n",
      "{'sentenceIndex': 15, 'startToken': 14, 'endToken': 15} ['guarantee']\n",
      "{'sentenceIndex': 15, 'startToken': 23, 'endToken': 24} ['IQ']\n",
      "{'sentenceIndex': 15, 'startToken': 16, 'endToken': 17} ['our']\n",
      "{'sentenceIndex': 15, 'startToken': 39, 'endToken': 40} ['mix']\n",
      "{'sentenceIndex': 15, 'startToken': 17, 'endToken': 18} ['sperm']\n",
      "{'sentenceIndex': 15, 'startToken': 28, 'endToken': 29} ['that']\n",
      "{'sentenceIndex': 15, 'startToken': 38, 'endToken': 39} ['DNA']\n",
      "{'sentenceIndex': 15, 'startToken': 40, 'endToken': 41} ['who']\n",
      "{'sentenceIndex': 15, 'startToken': 16, 'endToken': 18} ['our', 'sperm']\n",
      "{'sentenceIndex': 15, 'startToken': 24, 'endToken': 25} ['offspring']\n",
      "{'sentenceIndex': 15, 'startToken': 33, 'endToken': 34} ['sister']\n",
      "{'sentenceIndex': 15, 'startToken': 9, 'endToken': 10} ['fraud']\n",
      "{'sentenceIndex': 15, 'startToken': 41, 'endToken': 42} ['hostesses']\n",
      "{'sentenceIndex': 16, 'startToken': 18, 'endToken': 19} ['bandwidth']\n",
      "{'sentenceIndex': 16, 'startToken': 7, 'endToken': 8} ['your']\n",
      "{'sentenceIndex': 16, 'startToken': 13, 'endToken': 14} ['money']\n",
      "{'sentenceIndex': 16, 'startToken': 17, 'endToken': 18} ['T1']\n",
      "{'sentenceIndex': 16, 'startToken': 21, 'endToken': 22} ['apartment']\n",
      "{'sentenceIndex': 16, 'startToken': 8, 'endToken': 9} ['idea']\n",
      "{'sentenceIndex': 16, 'startToken': 5, 'endToken': 6} ['this']\n",
      "{'sentenceIndex': 17, 'startToken': 27, 'endToken': 28} ['my']\n",
      "{'sentenceIndex': 17, 'startToken': 43, 'endToken': 44} ['he']\n",
      "{'sentenceIndex': 17, 'startToken': 54, 'endToken': 55} ['area']\n",
      "{'sentenceIndex': 17, 'startToken': 32, 'endToken': 33} ['she']\n",
      "{'sentenceIndex': 17, 'startToken': 25, 'endToken': 26} ['hopes']\n",
      "{'sentenceIndex': 17, 'startToken': 57, 'endToken': 58} ['curve']\n",
      "{'sentenceIndex': 17, 'startToken': 50, 'endToken': 51} ['differential']\n",
      "{'sentenceIndex': 17, 'startToken': 30, 'endToken': 31} ['what']\n",
      "{'sentenceIndex': 17, 'startToken': 38, 'endToken': 39} ['who']\n",
      "{'sentenceIndex': 17, 'startToken': 7, 'endToken': 8} ['I']\n",
      "{'sentenceIndex': 17, 'startToken': 17, 'endToken': 20} ['some', 'poor', 'woman']\n",
      "{'sentenceIndex': 17, 'startToken': 28, 'endToken': 29} ['sperm']\n",
      "{'sentenceIndex': 17, 'startToken': 37, 'endToken': 38} ['toddler']\n",
      "{'sentenceIndex': 17, 'startToken': 3, 'endToken': 4} ['I']\n",
      "{'sentenceIndex': 17, 'startToken': 12, 'endToken': 13} ['downloads']\n",
      "{'sentenceIndex': 17, 'startToken': 19, 'endToken': 20} ['woman']\n",
      "{'sentenceIndex': 17, 'startToken': 24, 'endToken': 25} ['her']\n",
      "{'sentenceIndex': 17, 'startToken': 47, 'endToken': 48} ['integral']\n",
      "{'sentenceIndex': 17, 'startToken': 15, 'endToken': 16} ['there']\n",
      "{'sentenceIndex': 18, 'startToken': 3, 'endToken': 4} ['I']\n",
      "{'sentenceIndex': 18, 'startToken': 10, 'endToken': 11} ['him']\n",
      "{'sentenceIndex': 18, 'startToken': 6, 'endToken': 7} ['she']\n",
      "{'sentenceIndex': 19, 'startToken': 3, 'endToken': 4} ['I']\n",
      "{'sentenceIndex': 20, 'startToken': 7, 'endToken': 8} ['you']\n",
      "{'sentenceIndex': 20, 'startToken': 5, 'endToken': 6} ['what']\n",
      "{'sentenceIndex': 21, 'startToken': 3, 'endToken': 4} ['I']\n",
      "{'sentenceIndex': 22, 'startToken': 6, 'endToken': 7} ['protocol']\n",
      "{'sentenceIndex': 22, 'startToken': 3, 'endToken': 4} ['What']\n",
      "{'sentenceIndex': 22, 'startToken': 5, 'endToken': 7} ['the', 'protocol']\n",
      "{'sentenceIndex': 23, 'startToken': 16, 'endToken': 17} ['sperm']\n",
      "{'sentenceIndex': 23, 'startToken': 14, 'endToken': 15} ['proffer']\n",
      "{'sentenceIndex': 23, 'startToken': 8, 'endToken': 9} ['I']\n",
      "{'sentenceIndex': 23, 'startToken': 3, 'endToken': 4} ['I']\n",
      "{'sentenceIndex': 24, 'startToken': 4, 'endToken': 5} ['’s']\n",
      "{'sentenceIndex': 26, 'startToken': 3, 'endToken': 5} ['Bye', 'bye']\n",
      "{'sentenceIndex': 27, 'startToken': 4, 'endToken': 5} ['you']\n"
     ]
    }
   ],
   "source": [
    "print(\"Query Spans:\")\n",
    "for item in querySpans:\n",
    "    sent_idx = item['sentenceIndex']\n",
    "    start = item['startToken']\n",
    "    end = item['endToken']\n",
    "    print(item, sentences[sent_idx][start: end])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0, 1), (0, 6, 7), (0, 11, 12), (0, 14, 15), (0, 16, 17), (0, 19, 20), (0, 22, 23), (0, 28, 29), (0, 31, 32), (0, 34, 35), (0, 40, 41), (0, 44, 45), (0, 48, 49), (0, 51, 52), (0, 53, 54), (0, 54, 55), (0, 56, 57), (0, 63, 64)], [(1, 0, 1), (1, 5, 6), (1, 7, 8), (1, 7, 9), (1, 8, 9)], [(2, 0, 1), (2, 3, 4), (2, 5, 7), (2, 6, 7), (2, 8, 9), (2, 11, 12), (2, 15, 16), (2, 18, 19), (2, 19, 20)], [(3, 0, 1), (3, 4, 5)], [(4, 0, 1)], [(5, 0, 1), (5, 24, 25), (5, 25, 26), (5, 26, 28), (5, 27, 28), (5, 29, 30), (5, 43, 44), (5, 45, 46)], [(6, 0, 1), (6, 4, 5), (6, 6, 7)], [(7, 0, 1), (7, 8, 9), (7, 11, 12), (7, 12, 13), (7, 13, 14)], [(8, 0, 1), (8, 4, 5), (8, 10, 11)], [(9, 0, 1), (9, 3, 4), (9, 5, 6), (9, 8, 9)], [(10, 0, 1), (10, 4, 5)], [(11, 0, 1), (11, 4, 5), (11, 6, 7)], [(12, 0, 1), (12, 6, 7), (12, 7, 8), (12, 9, 10), (12, 13, 14), (12, 14, 15), (12, 15, 16)], [(13, 0, 1), (13, 3, 4), (13, 5, 6), (13, 9, 10), (13, 12, 13)], [(14, 0, 1), (14, 3, 4), (14, 6, 7), (14, 9, 10), (14, 12, 13), (14, 13, 14)], [(15, 0, 1), (15, 5, 6), (15, 9, 10), (15, 11, 12), (15, 13, 15), (15, 14, 15), (15, 16, 17), (15, 16, 18), (15, 17, 18), (15, 23, 24), (15, 24, 25), (15, 28, 29), (15, 30, 31), (15, 33, 34), (15, 38, 39), (15, 39, 40), (15, 40, 41), (15, 41, 42)], [(16, 0, 1), (16, 5, 6), (16, 7, 8), (16, 8, 9), (16, 13, 14), (16, 17, 18), (16, 18, 19), (16, 21, 22)], [(17, 0, 1), (17, 3, 4), (17, 7, 8), (17, 12, 13), (17, 15, 16), (17, 17, 20), (17, 19, 20), (17, 24, 25), (17, 25, 26), (17, 27, 28), (17, 28, 29), (17, 30, 31), (17, 32, 33), (17, 37, 38), (17, 38, 39), (17, 43, 44), (17, 47, 48), (17, 50, 51), (17, 54, 55), (17, 57, 58)], [(18, 0, 1), (18, 3, 4), (18, 6, 7), (18, 10, 11)], [(19, 0, 1), (19, 3, 4)], [(20, 0, 1), (20, 5, 6), (20, 7, 8)], [(21, 0, 1), (21, 3, 4)], [(22, 0, 1), (22, 3, 4), (22, 5, 7), (22, 6, 7)], [(23, 0, 1), (23, 3, 4), (23, 8, 9), (23, 14, 15), (23, 16, 17)], [(24, 0, 1), (24, 4, 5)], [(25, 0, 1)], [(26, 0, 1), (26, 3, 5)], [(27, 0, 1), (27, 4, 5)]]\n"
     ]
    }
   ],
   "source": [
    "stanza_output = get_spans(scene, stanza_parser)\n",
    "with open('test_stanza.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in stanza_output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def write_annotation_file(output, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9 5\n",
      "9 9 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuanzheng/.conda/envs/multi_coref/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9 17\n",
      "9 9 17\n",
      "====================================================================================================\n",
      "2 2 1\n",
      "2 2 2\n",
      "2 2 2\n",
      "2 2 2\n",
      "====================================================================================================\n",
      "5 3 3\n",
      "5 3 5\n",
      "5 3 5\n",
      "5 3 5\n",
      "====================================================================================================\n",
      "1 1 0\n",
      "1 1 2\n",
      "1 1 2\n",
      "1 1 2\n",
      "====================================================================================================\n",
      "1 0 0\n",
      "1 0 1\n",
      "1 0 1\n",
      "1 0 1\n",
      "====================================================================================================\n",
      "4 3 3\n",
      "5 3 11\n",
      "5 3 12\n",
      "5 3 12\n",
      "====================================================================================================\n",
      "1 2 1\n",
      "1 2 3\n",
      "1 2 3\n",
      "1 2 3\n",
      "====================================================================================================\n",
      "4 1 1\n",
      "4 1 3\n",
      "4 1 3\n",
      "4 1 3\n",
      "====================================================================================================\n",
      "1 2 2\n",
      "1 2 3\n",
      "1 2 3\n",
      "1 2 3\n",
      "====================================================================================================\n",
      "2 2 2\n",
      "2 2 3\n",
      "2 2 3\n",
      "2 2 3\n",
      "====================================================================================================\n",
      "1 1 0\n",
      "1 1 2\n",
      "1 1 2\n",
      "1 1 2\n",
      "====================================================================================================\n",
      "1 2 1\n",
      "1 2 3\n",
      "1 2 3\n",
      "1 2 3\n",
      "====================================================================================================\n",
      "4 3 1\n",
      "4 3 4\n",
      "4 3 4\n",
      "4 3 4\n",
      "====================================================================================================\n",
      "1 3 3\n",
      "1 3 4\n",
      "1 3 4\n",
      "1 3 4\n",
      "====================================================================================================\n",
      "3 3 2\n",
      "3 3 4\n",
      "3 3 4\n",
      "3 3 4\n",
      "====================================================================================================\n",
      "10 6 5\n",
      "10 6 12\n",
      "10 6 12\n",
      "10 6 12\n",
      "====================================================================================================\n",
      "6 2 1\n",
      "6 2 6\n",
      "6 2 6\n",
      "6 2 6\n",
      "====================================================================================================\n",
      "10 9 6\n",
      "10 9 14\n",
      "10 9 15\n",
      "10 9 15\n",
      "====================================================================================================\n",
      "1 3 2\n",
      "1 3 4\n",
      "1 3 4\n",
      "1 3 4\n",
      "====================================================================================================\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "====================================================================================================\n",
      "1 2 1\n",
      "1 2 2\n",
      "1 2 2\n",
      "1 2 2\n",
      "====================================================================================================\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "====================================================================================================\n",
      "2 1 1\n",
      "2 1 2\n",
      "2 1 2\n",
      "2 1 2\n",
      "====================================================================================================\n",
      "3 2 2\n",
      "3 2 4\n",
      "3 2 4\n",
      "3 2 4\n",
      "====================================================================================================\n",
      "1 1 0\n",
      "1 1 2\n",
      "1 1 2\n",
      "1 1 2\n",
      "====================================================================================================\n",
      "1 0 0\n",
      "1 0 1\n",
      "1 0 1\n",
      "1 0 1\n",
      "====================================================================================================\n",
      "1 0 1\n",
      "2 0 2\n",
      "2 0 2\n",
      "2 0 2\n",
      "====================================================================================================\n",
      "1 1 0\n",
      "1 1 2\n",
      "1 1 2\n",
      "1 1 2\n",
      "====================================================================================================\n",
      "28\n",
      "[[(0, 0, 1), (0, 5, 7), (0, 6, 7), (0, 10, 12), (0, 11, 12), (0, 13, 15), (0, 14, 15), (0, 16, 17), (0, 18, 20), (0, 19, 20), (0, 22, 23), (0, 27, 29), (0, 28, 29), (0, 31, 32), (0, 34, 35), (0, 40, 41), (0, 44, 45), (0, 47, 49), (0, 48, 49), (0, 51, 52), (0, 53, 54), (0, 53, 55), (0, 54, 55), (0, 56, 57), (0, 62, 64), (0, 63, 64)], [(1, 0, 1), (1, 5, 6), (1, 7, 8), (1, 7, 9), (1, 8, 9)], [(2, 0, 1), (2, 3, 4), (2, 5, 7), (2, 6, 7), (2, 8, 9), (2, 11, 12), (2, 13, 16), (2, 15, 16), (2, 17, 20), (2, 18, 19), (2, 19, 20)], [(3, 0, 1), (3, 4, 5)], [(4, 0, 1)], [(5, 0, 1), (5, 6, 7), (5, 11, 12), (5, 17, 18), (5, 24, 25), (5, 24, 26), (5, 25, 26), (5, 26, 28), (5, 27, 28), (5, 29, 30), (5, 33, 36), (5, 39, 44), (5, 42, 43), (5, 43, 44), (5, 45, 46), (5, 47, 50), (5, 51, 52)], [(6, 0, 1), (6, 4, 5), (6, 6, 7)], [(7, 0, 1), (7, 8, 9), (7, 9, 14), (7, 11, 12), (7, 12, 13), (7, 13, 14)], [(8, 0, 1), (8, 4, 5), (8, 10, 11)], [(9, 0, 1), (9, 3, 4), (9, 5, 6), (9, 7, 9), (9, 8, 9)], [(10, 0, 1), (10, 4, 5)], [(11, 0, 1), (11, 4, 5), (11, 6, 7)], [(12, 0, 1), (12, 6, 8), (12, 6, 7), (12, 7, 8), (12, 9, 10), (12, 13, 16), (12, 13, 14), (12, 14, 15), (12, 15, 16)], [(13, 0, 1), (13, 3, 4), (13, 5, 6), (13, 9, 10), (13, 12, 13)], [(14, 0, 1), (14, 3, 4), (14, 6, 7), (14, 9, 10), (14, 11, 14), (14, 12, 13), (14, 13, 14)], [(15, 0, 1), (15, 5, 6), (15, 8, 10), (15, 9, 10), (15, 11, 12), (15, 13, 15), (15, 14, 15), (15, 16, 17), (15, 16, 18), (15, 17, 18), (15, 22, 25), (15, 23, 24), (15, 24, 25), (15, 28, 29), (15, 30, 31), (15, 32, 34), (15, 33, 34), (15, 35, 40), (15, 38, 39), (15, 39, 40), (15, 40, 41), (15, 41, 42), (15, 43, 44)], [(16, 0, 1), (16, 3, 4), (16, 5, 6), (16, 7, 9), (16, 7, 8), (16, 8, 9), (16, 10, 14), (16, 13, 14), (16, 16, 19), (16, 17, 18), (16, 18, 19), (16, 20, 22), (16, 21, 22)], [(17, 0, 1), (17, 3, 4), (17, 7, 8), (17, 11, 13), (17, 12, 13), (17, 15, 16), (17, 17, 20), (17, 19, 20), (17, 24, 26), (17, 24, 25), (17, 25, 26), (17, 27, 28), (17, 27, 29), (17, 28, 29), (17, 30, 31), (17, 32, 33), (17, 36, 38), (17, 37, 38), (17, 38, 39), (17, 43, 44), (17, 46, 48), (17, 47, 48), (17, 49, 51), (17, 50, 51), (17, 53, 55), (17, 54, 55), (17, 56, 58), (17, 57, 58)], [(18, 0, 1), (18, 3, 4), (18, 6, 7), (18, 10, 11)], [(19, 0, 1), (19, 3, 4)], [(20, 0, 1), (20, 5, 6), (20, 7, 8)], [(21, 0, 1), (21, 3, 4)], [(22, 0, 1), (22, 3, 4), (22, 5, 7), (22, 6, 7)], [(23, 0, 1), (23, 3, 4), (23, 8, 9), (23, 13, 15), (23, 14, 15), (23, 16, 17)], [(24, 0, 1), (24, 4, 5)], [(25, 0, 1)], [(26, 0, 1), (26, 3, 5), (26, 4, 5)], [(27, 0, 1), (27, 4, 5)]]\n"
     ]
    }
   ],
   "source": [
    "multi_output = get_spans_multi_parsers(scene, [stanza_parser, berkeley_parser, trf_parser, md_parser])\n",
    "write_annotation_file(stanza_output, \"trial_path.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0, 1), (0, 5, 7), (0, 6, 7), (0, 10, 12), (0, 11, 12), (0, 13, 15), (0, 14, 15), (0, 16, 17), (0, 18, 20), (0, 19, 20), (0, 22, 23), (0, 27, 29), (0, 28, 29), (0, 31, 32), (0, 34, 35), (0, 40, 41), (0, 44, 45), (0, 47, 49), (0, 48, 49), (0, 51, 52), (0, 53, 54), (0, 53, 55), (0, 54, 55), (0, 56, 57), (0, 62, 64), (0, 63, 64)], [(1, 0, 1), (1, 5, 6), (1, 7, 8), (1, 7, 9), (1, 8, 9)], [(2, 0, 1), (2, 3, 4), (2, 5, 7), (2, 6, 7), (2, 8, 9), (2, 11, 12), (2, 13, 16), (2, 15, 16), (2, 17, 20), (2, 18, 19), (2, 19, 20)], [(3, 0, 1), (3, 4, 5)], [(4, 0, 1)], [(5, 0, 1), (5, 6, 7), (5, 11, 12), (5, 17, 18), (5, 24, 25), (5, 24, 26), (5, 25, 26), (5, 27, 28), (5, 29, 30), (5, 33, 36), (5, 39, 44), (5, 42, 43), (5, 43, 44), (5, 45, 46), (5, 47, 50), (5, 51, 52)], [(6, 0, 1), (6, 4, 5), (6, 6, 7)], [(7, 0, 1), (7, 8, 9), (7, 9, 14), (7, 11, 12), (7, 12, 13), (7, 13, 14)], [(8, 0, 1), (8, 4, 5), (8, 10, 11)], [(9, 0, 1), (9, 3, 4), (9, 5, 6), (9, 7, 9), (9, 8, 9)], [(10, 0, 1), (10, 4, 5)], [(11, 0, 1), (11, 4, 5), (11, 6, 7)], [(12, 0, 1), (12, 6, 8), (12, 6, 7), (12, 7, 8), (12, 9, 10), (12, 13, 16), (12, 13, 14), (12, 15, 16)], [(13, 0, 1), (13, 3, 4), (13, 5, 6), (13, 9, 10), (13, 12, 13)], [(14, 0, 1), (14, 3, 4), (14, 6, 7), (14, 9, 10), (14, 11, 14), (14, 13, 14)], [(15, 0, 1), (15, 5, 6), (15, 8, 10), (15, 9, 10), (15, 11, 12), (15, 13, 15), (15, 14, 15), (15, 16, 17), (15, 16, 18), (15, 17, 18), (15, 22, 25), (15, 24, 25), (15, 28, 29), (15, 30, 31), (15, 32, 34), (15, 33, 34), (15, 35, 40), (15, 38, 39), (15, 39, 40), (15, 40, 41), (15, 43, 44)], [(16, 0, 1), (16, 3, 4), (16, 5, 6), (16, 7, 9), (16, 7, 8), (16, 8, 9), (16, 10, 14), (16, 13, 14), (16, 16, 19), (16, 17, 18), (16, 18, 19), (16, 20, 22), (16, 21, 22)], [(17, 0, 1), (17, 3, 4), (17, 7, 8), (17, 11, 13), (17, 12, 13), (17, 15, 16), (17, 17, 20), (17, 19, 20), (17, 24, 26), (17, 24, 25), (17, 25, 26), (17, 27, 28), (17, 27, 29), (17, 28, 29), (17, 30, 31), (17, 32, 33), (17, 36, 38), (17, 37, 38), (17, 38, 39), (17, 43, 44), (17, 49, 51), (17, 50, 51), (17, 53, 55), (17, 54, 55), (17, 56, 58), (17, 57, 58)], [(18, 0, 1), (18, 3, 4), (18, 6, 7), (18, 10, 11)], [(19, 0, 1), (19, 3, 4)], [(20, 0, 1), (20, 5, 6), (20, 7, 8)], [(21, 0, 1), (21, 3, 4)], [(22, 0, 1), (22, 3, 4), (22, 5, 7), (22, 6, 7)], [(23, 0, 1), (23, 3, 4), (23, 8, 9), (23, 13, 15), (23, 14, 15), (23, 16, 17)], [(24, 0, 1), (24, 4, 5)], [(25, 0, 1)], [(26, 0, 1), (26, 4, 5)], [(27, 0, 1), (27, 4, 5)]]\n"
     ]
    }
   ],
   "source": [
    "berkeley_output = get_spans(scene, berkeley_parser)\n",
    "with open('test_berkeley.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in stanza_output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0, 1), (0, 5, 7), (0, 6, 7), (0, 10, 12), (0, 11, 12), (0, 13, 15), (0, 14, 15), (0, 16, 17), (0, 18, 20), (0, 19, 20), (0, 22, 23), (0, 27, 29), (0, 28, 29), (0, 31, 32), (0, 34, 35), (0, 40, 41), (0, 44, 45), (0, 47, 49), (0, 48, 49), (0, 51, 52), (0, 53, 54), (0, 53, 55), (0, 54, 55), (0, 56, 57), (0, 62, 64), (0, 63, 64)], [(1, 0, 1), (1, 5, 6), (1, 7, 8), (1, 7, 9), (1, 8, 9)], [(2, 0, 1), (2, 3, 4), (2, 5, 7), (2, 6, 7), (2, 8, 9), (2, 11, 12), (2, 13, 16), (2, 15, 16), (2, 17, 20), (2, 18, 19), (2, 19, 20)], [(3, 0, 1), (3, 4, 5)], [(4, 0, 1)], [(5, 0, 1), (5, 6, 7), (5, 11, 12), (5, 17, 18), (5, 24, 25), (5, 24, 26), (5, 25, 26), (5, 27, 28), (5, 29, 30), (5, 33, 36), (5, 39, 44), (5, 42, 43), (5, 43, 44), (5, 45, 46), (5, 47, 50), (5, 51, 52)], [(6, 0, 1), (6, 4, 5), (6, 6, 7)], [(7, 0, 1), (7, 8, 9), (7, 9, 14), (7, 11, 12), (7, 12, 13), (7, 13, 14)], [(8, 0, 1), (8, 4, 5), (8, 10, 11)], [(9, 0, 1), (9, 3, 4), (9, 5, 6), (9, 7, 9), (9, 8, 9)], [(10, 0, 1), (10, 4, 5)], [(11, 0, 1), (11, 4, 5), (11, 6, 7)], [(12, 0, 1), (12, 6, 8), (12, 6, 7), (12, 7, 8), (12, 9, 10), (12, 13, 16), (12, 13, 14), (12, 15, 16)], [(13, 0, 1), (13, 3, 4), (13, 5, 6), (13, 9, 10), (13, 12, 13)], [(14, 0, 1), (14, 3, 4), (14, 6, 7), (14, 9, 10), (14, 11, 14), (14, 13, 14)], [(15, 0, 1), (15, 5, 6), (15, 8, 10), (15, 9, 10), (15, 11, 12), (15, 13, 15), (15, 14, 15), (15, 16, 17), (15, 16, 18), (15, 17, 18), (15, 22, 25), (15, 24, 25), (15, 28, 29), (15, 30, 31), (15, 32, 34), (15, 33, 34), (15, 35, 40), (15, 38, 39), (15, 39, 40), (15, 40, 41), (15, 43, 44)], [(16, 0, 1), (16, 3, 4), (16, 5, 6), (16, 7, 9), (16, 7, 8), (16, 8, 9), (16, 10, 14), (16, 13, 14), (16, 16, 19), (16, 17, 18), (16, 18, 19), (16, 20, 22), (16, 21, 22)], [(17, 0, 1), (17, 3, 4), (17, 7, 8), (17, 11, 13), (17, 12, 13), (17, 15, 16), (17, 17, 20), (17, 19, 20), (17, 24, 26), (17, 24, 25), (17, 25, 26), (17, 27, 28), (17, 27, 29), (17, 28, 29), (17, 30, 31), (17, 32, 33), (17, 36, 38), (17, 37, 38), (17, 38, 39), (17, 43, 44), (17, 49, 51), (17, 50, 51), (17, 53, 55), (17, 54, 55), (17, 56, 58), (17, 57, 58)], [(18, 0, 1), (18, 3, 4), (18, 6, 7), (18, 10, 11)], [(19, 0, 1), (19, 3, 4)], [(20, 0, 1), (20, 5, 6), (20, 7, 8)], [(21, 0, 1), (21, 3, 4)], [(22, 0, 1), (22, 3, 4), (22, 5, 7), (22, 6, 7)], [(23, 0, 1), (23, 3, 4), (23, 8, 9), (23, 13, 15), (23, 14, 15), (23, 16, 17)], [(24, 0, 1), (24, 4, 5)], [(25, 0, 1)], [(26, 0, 1), (26, 4, 5)], [(27, 0, 1), (27, 4, 5)]]\n"
     ]
    }
   ],
   "source": [
    "md_output = get_spans(scene, md_parser)\n",
    "with open('test_md.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in stanza_output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuanzheng/.conda/envs/multi_coref/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0, 1), (0, 5, 7), (0, 6, 7), (0, 10, 12), (0, 11, 12), (0, 13, 15), (0, 14, 15), (0, 16, 17), (0, 18, 20), (0, 19, 20), (0, 22, 23), (0, 27, 29), (0, 28, 29), (0, 31, 32), (0, 34, 35), (0, 40, 41), (0, 44, 45), (0, 47, 49), (0, 48, 49), (0, 51, 52), (0, 53, 54), (0, 53, 55), (0, 54, 55), (0, 56, 57), (0, 62, 64), (0, 63, 64)], [(1, 0, 1), (1, 5, 6), (1, 7, 8), (1, 7, 9), (1, 8, 9)], [(2, 0, 1), (2, 3, 4), (2, 5, 7), (2, 6, 7), (2, 8, 9), (2, 11, 12), (2, 13, 16), (2, 15, 16), (2, 17, 20), (2, 18, 19), (2, 19, 20)], [(3, 0, 1), (3, 4, 5)], [(4, 0, 1)], [(5, 0, 1), (5, 6, 7), (5, 11, 12), (5, 17, 18), (5, 24, 25), (5, 24, 26), (5, 25, 26), (5, 27, 28), (5, 29, 30), (5, 33, 36), (5, 42, 43), (5, 43, 44), (5, 45, 46), (5, 47, 50), (5, 51, 52)], [(6, 0, 1), (6, 4, 5), (6, 6, 7)], [(7, 0, 1), (7, 8, 9), (7, 9, 14), (7, 12, 13), (7, 13, 14)], [(8, 0, 1), (8, 4, 5), (8, 10, 11)], [(9, 0, 1), (9, 3, 4), (9, 5, 6), (9, 7, 9), (9, 8, 9)], [(10, 0, 1), (10, 4, 5)], [(11, 0, 1), (11, 4, 5), (11, 6, 7)], [(12, 0, 1), (12, 6, 8), (12, 6, 7), (12, 7, 8), (12, 9, 10), (12, 13, 16), (12, 13, 14), (12, 14, 15), (12, 15, 16)], [(13, 0, 1), (13, 5, 6), (13, 9, 10), (13, 12, 13)], [(14, 0, 1), (14, 3, 4), (14, 6, 7), (14, 9, 10), (14, 11, 14), (14, 12, 13), (14, 13, 14)], [(15, 0, 1), (15, 5, 6), (15, 8, 10), (15, 9, 10), (15, 11, 12), (15, 13, 15), (15, 14, 15), (15, 16, 17), (15, 16, 18), (15, 17, 18), (15, 22, 25), (15, 23, 24), (15, 24, 25), (15, 28, 29), (15, 30, 31), (15, 32, 34), (15, 33, 34), (15, 35, 40), (15, 38, 39), (15, 39, 40), (15, 40, 41), (15, 43, 44)], [(16, 0, 1), (16, 5, 6), (16, 7, 9), (16, 7, 8), (16, 8, 9), (16, 10, 14), (16, 13, 14), (16, 16, 19), (16, 17, 18), (16, 18, 19), (16, 20, 22), (16, 21, 22)], [(17, 0, 1), (17, 3, 4), (17, 7, 8), (17, 11, 13), (17, 12, 13), (17, 15, 16), (17, 17, 20), (17, 19, 20), (17, 24, 26), (17, 24, 25), (17, 25, 26), (17, 27, 28), (17, 27, 29), (17, 28, 29), (17, 30, 31), (17, 32, 33), (17, 36, 38), (17, 37, 38), (17, 38, 39), (17, 43, 44), (17, 46, 48), (17, 47, 48), (17, 49, 51), (17, 50, 51), (17, 53, 55), (17, 54, 55), (17, 56, 58), (17, 57, 58)], [(18, 0, 1), (18, 3, 4), (18, 6, 7), (18, 10, 11)], [(19, 0, 1), (19, 3, 4)], [(20, 0, 1), (20, 5, 6), (20, 7, 8)], [(21, 0, 1), (21, 3, 4)], [(22, 0, 1), (22, 3, 4), (22, 5, 7), (22, 6, 7)], [(23, 0, 1), (23, 3, 4), (23, 8, 9), (23, 13, 15), (23, 14, 15), (23, 16, 17)], [(24, 0, 1), (24, 4, 5)], [(25, 0, 1)], [(26, 0, 1)], [(27, 0, 1), (27, 4, 5)]]\n"
     ]
    }
   ],
   "source": [
    "trf_output = get_spans(scene, trf_parser)\n",
    "with open('test_trf.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in stanza_output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "stanza = [[(0, 0, 1), (0, 6, 7), (0, 11, 12), (0, 14, 15), (0, 16, 17), (0, 19, 20), (0, 22, 23), (0, 28, 29), (0, 31, 32), (0, 34, 35), (0, 40, 41), (0, 44, 45), (0, 48, 49), (0, 51, 52), (0, 53, 54), (0, 54, 55), (0, 56, 57), (0, 63, 64)], [(1, 0, 1), (1, 5, 6), (1, 7, 8), (1, 7, 9), (1, 8, 9)], [(2, 0, 1), (2, 3, 4), (2, 5, 7), (2, 6, 7), (2, 8, 9), (2, 11, 12), (2, 15, 16), (2, 18, 19), (2, 19, 20)], [(3, 0, 1), (3, 4, 5)], [(4, 0, 1)], [(5, 0, 1), (5, 24, 25), (5, 25, 26), (5, 26, 28), (5, 27, 28), (5, 29, 30), (5, 43, 44), (5, 45, 46)], [(6, 0, 1), (6, 4, 5), (6, 6, 7)], [(7, 0, 1), (7, 8, 9), (7, 11, 12), (7, 12, 13), (7, 13, 14)], [(8, 0, 1), (8, 4, 5), (8, 10, 11)], [(9, 0, 1), (9, 3, 4), (9, 5, 6), (9, 8, 9)], [(10, 0, 1), (10, 4, 5)], [(11, 0, 1), (11, 4, 5), (11, 6, 7)], [(12, 0, 1), (12, 6, 7), (12, 7, 8), (12, 9, 10), (12, 13, 14), (12, 14, 15), (12, 15, 16)], [(13, 0, 1), (13, 3, 4), (13, 5, 6), (13, 9, 10), (13, 12, 13)], [(14, 0, 1), (14, 3, 4), (14, 6, 7), (14, 9, 10), (14, 12, 13), (14, 13, 14)], [(15, 0, 1), (15, 5, 6), (15, 9, 10), (15, 11, 12), (15, 13, 15), (15, 14, 15), (15, 16, 17), (15, 16, 18), (15, 17, 18), (15, 23, 24), (15, 24, 25), (15, 28, 29), (15, 30, 31), (15, 33, 34), (15, 38, 39), (15, 39, 40), (15, 40, 41), (15, 41, 42)], [(16, 0, 1), (16, 5, 6), (16, 7, 8), (16, 8, 9), (16, 13, 14), (16, 17, 18), (16, 18, 19), (16, 21, 22)], [(17, 0, 1), (17, 3, 4), (17, 7, 8), (17, 12, 13), (17, 15, 16), (17, 17, 20), (17, 19, 20), (17, 24, 25), (17, 25, 26), (17, 27, 28), (17, 28, 29), (17, 30, 31), (17, 32, 33), (17, 37, 38), (17, 38, 39), (17, 43, 44), (17, 47, 48), (17, 50, 51), (17, 54, 55), (17, 57, 58)], [(18, 0, 1), (18, 3, 4), (18, 6, 7), (18, 10, 11)], [(19, 0, 1), (19, 3, 4)], [(20, 0, 1), (20, 5, 6), (20, 7, 8)], [(21, 0, 1), (21, 3, 4)], [(22, 0, 1), (22, 3, 4), (22, 5, 7), (22, 6, 7)], [(23, 0, 1), (23, 3, 4), (23, 8, 9), (23, 14, 15), (23, 16, 17)], [(24, 0, 1), (24, 4, 5)], [(25, 0, 1)], [(26, 0, 1), (26, 3, 5)], [(27, 0, 1), (27, 4, 5)]]\n",
    "\n",
    "berkeley = [[(0, 0, 1), (0, 5, 7), (0, 6, 7), (0, 10, 12), (0, 11, 12), (0, 13, 15), (0, 14, 15), (0, 16, 17), (0, 18, 20), (0, 19, 20), (0, 22, 23), (0, 27, 29), (0, 28, 29), (0, 31, 32), (0, 34, 35), (0, 40, 41), (0, 44, 45), (0, 47, 49), (0, 48, 49), (0, 51, 52), (0, 53, 54), (0, 53, 55), (0, 54, 55), (0, 56, 57), (0, 62, 64), (0, 63, 64)], [(1, 0, 1), (1, 5, 6), (1, 7, 8), (1, 7, 9), (1, 8, 9)], [(2, 0, 1), (2, 3, 4), (2, 5, 7), (2, 6, 7), (2, 8, 9), (2, 11, 12), (2, 13, 16), (2, 15, 16), (2, 17, 20), (2, 18, 19), (2, 19, 20)], [(3, 0, 1), (3, 4, 5)], [(4, 0, 1)], [(5, 0, 1), (5, 6, 7), (5, 11, 12), (5, 17, 18), (5, 24, 25), (5, 24, 26), (5, 25, 26), (5, 27, 28), (5, 29, 30), (5, 33, 36), (5, 39, 44), (5, 42, 43), (5, 43, 44), (5, 45, 46), (5, 47, 50), (5, 51, 52)], [(6, 0, 1), (6, 4, 5), (6, 6, 7)], [(7, 0, 1), (7, 8, 9), (7, 9, 14), (7, 11, 12), (7, 12, 13), (7, 13, 14)], [(8, 0, 1), (8, 4, 5), (8, 10, 11)], [(9, 0, 1), (9, 3, 4), (9, 5, 6), (9, 7, 9), (9, 8, 9)], [(10, 0, 1), (10, 4, 5)], [(11, 0, 1), (11, 4, 5), (11, 6, 7)], [(12, 0, 1), (12, 6, 8), (12, 6, 7), (12, 7, 8), (12, 9, 10), (12, 13, 16), (12, 13, 14), (12, 15, 16)], [(13, 0, 1), (13, 3, 4), (13, 5, 6), (13, 9, 10), (13, 12, 13)], [(14, 0, 1), (14, 3, 4), (14, 6, 7), (14, 9, 10), (14, 11, 14), (14, 13, 14)], [(15, 0, 1), (15, 5, 6), (15, 8, 10), (15, 9, 10), (15, 11, 12), (15, 13, 15), (15, 14, 15), (15, 16, 17), (15, 16, 18), (15, 17, 18), (15, 22, 25), (15, 24, 25), (15, 28, 29), (15, 30, 31), (15, 32, 34), (15, 33, 34), (15, 35, 40), (15, 38, 39), (15, 39, 40), (15, 40, 41), (15, 43, 44)], [(16, 0, 1), (16, 3, 4), (16, 5, 6), (16, 7, 9), (16, 7, 8), (16, 8, 9), (16, 10, 14), (16, 13, 14), (16, 16, 19), (16, 17, 18), (16, 18, 19), (16, 20, 22), (16, 21, 22)], [(17, 0, 1), (17, 3, 4), (17, 7, 8), (17, 11, 13), (17, 12, 13), (17, 15, 16), (17, 17, 20), (17, 19, 20), (17, 24, 26), (17, 24, 25), (17, 25, 26), (17, 27, 28), (17, 27, 29), (17, 28, 29), (17, 30, 31), (17, 32, 33), (17, 36, 38), (17, 37, 38), (17, 38, 39), (17, 43, 44), (17, 49, 51), (17, 50, 51), (17, 53, 55), (17, 54, 55), (17, 56, 58), (17, 57, 58)], [(18, 0, 1), (18, 3, 4), (18, 6, 7), (18, 10, 11)], [(19, 0, 1), (19, 3, 4)], [(20, 0, 1), (20, 5, 6), (20, 7, 8)], [(21, 0, 1), (21, 3, 4)], [(22, 0, 1), (22, 3, 4), (22, 5, 7), (22, 6, 7)], [(23, 0, 1), (23, 3, 4), (23, 8, 9), (23, 13, 15), (23, 14, 15), (23, 16, 17)], [(24, 0, 1), (24, 4, 5)], [(25, 0, 1)], [(26, 0, 1), (26, 4, 5)], [(27, 0, 1), (27, 4, 5)]]\n",
    "\n",
    "md = [[(0, 0, 1), (0, 5, 7), (0, 6, 7), (0, 10, 12), (0, 11, 12), (0, 13, 15), (0, 14, 15), (0, 16, 17), (0, 18, 20), (0, 19, 20), (0, 22, 23), (0, 27, 29), (0, 28, 29), (0, 31, 32), (0, 34, 35), (0, 40, 41), (0, 44, 45), (0, 47, 49), (0, 48, 49), (0, 51, 52), (0, 53, 54), (0, 53, 55), (0, 54, 55), (0, 56, 57), (0, 62, 64), (0, 63, 64)], [(1, 0, 1), (1, 5, 6), (1, 7, 8), (1, 7, 9), (1, 8, 9)], [(2, 0, 1), (2, 3, 4), (2, 5, 7), (2, 6, 7), (2, 8, 9), (2, 11, 12), (2, 13, 16), (2, 15, 16), (2, 17, 20), (2, 18, 19), (2, 19, 20)], [(3, 0, 1), (3, 4, 5)], [(4, 0, 1)], [(5, 0, 1), (5, 6, 7), (5, 11, 12), (5, 17, 18), (5, 24, 25), (5, 24, 26), (5, 25, 26), (5, 27, 28), (5, 29, 30), (5, 33, 36), (5, 39, 44), (5, 42, 43), (5, 43, 44), (5, 45, 46), (5, 47, 50), (5, 51, 52)], [(6, 0, 1), (6, 4, 5), (6, 6, 7)], [(7, 0, 1), (7, 8, 9), (7, 9, 14), (7, 11, 12), (7, 12, 13), (7, 13, 14)], [(8, 0, 1), (8, 4, 5), (8, 10, 11)], [(9, 0, 1), (9, 3, 4), (9, 5, 6), (9, 7, 9), (9, 8, 9)], [(10, 0, 1), (10, 4, 5)], [(11, 0, 1), (11, 4, 5), (11, 6, 7)], [(12, 0, 1), (12, 6, 8), (12, 6, 7), (12, 7, 8), (12, 9, 10), (12, 13, 16), (12, 13, 14), (12, 15, 16)], [(13, 0, 1), (13, 3, 4), (13, 5, 6), (13, 9, 10), (13, 12, 13)], [(14, 0, 1), (14, 3, 4), (14, 6, 7), (14, 9, 10), (14, 11, 14), (14, 13, 14)], [(15, 0, 1), (15, 5, 6), (15, 8, 10), (15, 9, 10), (15, 11, 12), (15, 13, 15), (15, 14, 15), (15, 16, 17), (15, 16, 18), (15, 17, 18), (15, 22, 25), (15, 24, 25), (15, 28, 29), (15, 30, 31), (15, 32, 34), (15, 33, 34), (15, 35, 40), (15, 38, 39), (15, 39, 40), (15, 40, 41), (15, 43, 44)], [(16, 0, 1), (16, 3, 4), (16, 5, 6), (16, 7, 9), (16, 7, 8), (16, 8, 9), (16, 10, 14), (16, 13, 14), (16, 16, 19), (16, 17, 18), (16, 18, 19), (16, 20, 22), (16, 21, 22)], [(17, 0, 1), (17, 3, 4), (17, 7, 8), (17, 11, 13), (17, 12, 13), (17, 15, 16), (17, 17, 20), (17, 19, 20), (17, 24, 26), (17, 24, 25), (17, 25, 26), (17, 27, 28), (17, 27, 29), (17, 28, 29), (17, 30, 31), (17, 32, 33), (17, 36, 38), (17, 37, 38), (17, 38, 39), (17, 43, 44), (17, 49, 51), (17, 50, 51), (17, 53, 55), (17, 54, 55), (17, 56, 58), (17, 57, 58)], [(18, 0, 1), (18, 3, 4), (18, 6, 7), (18, 10, 11)], [(19, 0, 1), (19, 3, 4)], [(20, 0, 1), (20, 5, 6), (20, 7, 8)], [(21, 0, 1), (21, 3, 4)], [(22, 0, 1), (22, 3, 4), (22, 5, 7), (22, 6, 7)], [(23, 0, 1), (23, 3, 4), (23, 8, 9), (23, 13, 15), (23, 14, 15), (23, 16, 17)], [(24, 0, 1), (24, 4, 5)], [(25, 0, 1)], [(26, 0, 1), (26, 4, 5)], [(27, 0, 1), (27, 4, 5)]]\n",
    "\n",
    "trf = [[(0, 0, 1), (0, 5, 7), (0, 6, 7), (0, 10, 12), (0, 11, 12), (0, 13, 15), (0, 14, 15), (0, 16, 17), (0, 18, 20), (0, 19, 20), (0, 22, 23), (0, 27, 29), (0, 28, 29), (0, 31, 32), (0, 34, 35), (0, 40, 41), (0, 44, 45), (0, 47, 49), (0, 48, 49), (0, 51, 52), (0, 53, 54), (0, 53, 55), (0, 54, 55), (0, 56, 57), (0, 62, 64), (0, 63, 64)], [(1, 0, 1), (1, 5, 6), (1, 7, 8), (1, 7, 9), (1, 8, 9)], [(2, 0, 1), (2, 3, 4), (2, 5, 7), (2, 6, 7), (2, 8, 9), (2, 11, 12), (2, 13, 16), (2, 15, 16), (2, 17, 20), (2, 18, 19), (2, 19, 20)], [(3, 0, 1), (3, 4, 5)], [(4, 0, 1)], [(5, 0, 1), (5, 6, 7), (5, 11, 12), (5, 17, 18), (5, 24, 25), (5, 24, 26), (5, 25, 26), (5, 27, 28), (5, 29, 30), (5, 33, 36), (5, 42, 43), (5, 43, 44), (5, 45, 46), (5, 47, 50), (5, 51, 52)], [(6, 0, 1), (6, 4, 5), (6, 6, 7)], [(7, 0, 1), (7, 8, 9), (7, 9, 14), (7, 12, 13), (7, 13, 14)], [(8, 0, 1), (8, 4, 5), (8, 10, 11)], [(9, 0, 1), (9, 3, 4), (9, 5, 6), (9, 7, 9), (9, 8, 9)], [(10, 0, 1), (10, 4, 5)], [(11, 0, 1), (11, 4, 5), (11, 6, 7)], [(12, 0, 1), (12, 6, 8), (12, 6, 7), (12, 7, 8), (12, 9, 10), (12, 13, 16), (12, 13, 14), (12, 14, 15), (12, 15, 16)], [(13, 0, 1), (13, 5, 6), (13, 9, 10), (13, 12, 13)], [(14, 0, 1), (14, 3, 4), (14, 6, 7), (14, 9, 10), (14, 11, 14), (14, 12, 13), (14, 13, 14)], [(15, 0, 1), (15, 5, 6), (15, 8, 10), (15, 9, 10), (15, 11, 12), (15, 13, 15), (15, 14, 15), (15, 16, 17), (15, 16, 18), (15, 17, 18), (15, 22, 25), (15, 23, 24), (15, 24, 25), (15, 28, 29), (15, 30, 31), (15, 32, 34), (15, 33, 34), (15, 35, 40), (15, 38, 39), (15, 39, 40), (15, 40, 41), (15, 43, 44)], [(16, 0, 1), (16, 5, 6), (16, 7, 9), (16, 7, 8), (16, 8, 9), (16, 10, 14), (16, 13, 14), (16, 16, 19), (16, 17, 18), (16, 18, 19), (16, 20, 22), (16, 21, 22)], [(17, 0, 1), (17, 3, 4), (17, 7, 8), (17, 11, 13), (17, 12, 13), (17, 15, 16), (17, 17, 20), (17, 19, 20), (17, 24, 26), (17, 24, 25), (17, 25, 26), (17, 27, 28), (17, 27, 29), (17, 28, 29), (17, 30, 31), (17, 32, 33), (17, 36, 38), (17, 37, 38), (17, 38, 39), (17, 43, 44), (17, 46, 48), (17, 47, 48), (17, 49, 51), (17, 50, 51), (17, 53, 55), (17, 54, 55), (17, 56, 58), (17, 57, 58)], [(18, 0, 1), (18, 3, 4), (18, 6, 7), (18, 10, 11)], [(19, 0, 1), (19, 3, 4)], [(20, 0, 1), (20, 5, 6), (20, 7, 8)], [(21, 0, 1), (21, 3, 4)], [(22, 0, 1), (22, 3, 4), (22, 5, 7), (22, 6, 7)], [(23, 0, 1), (23, 3, 4), (23, 8, 9), (23, 13, 15), (23, 14, 15), (23, 16, 17)], [(24, 0, 1), (24, 4, 5)], [(25, 0, 1)], [(26, 0, 1)], [(27, 0, 1), (27, 4, 5)]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "spans = []\n",
    "for instance in (stanza, berkeley, md, trf):\n",
    "    temp = set()\n",
    "    for item in instance:\n",
    "        for token in item:\n",
    "            temp.add(token)\n",
    "    spans.append(temp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "190\n",
      "190\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "for item in spans:\n",
    "    print(len(item))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 154\n",
      "0 1 147\n",
      "0 2 147\n",
      "0 3 149\n",
      "1 0 147\n",
      "1 1 190\n",
      "1 2 190\n",
      "1 3 185\n",
      "2 0 147\n",
      "2 1 190\n",
      "2 2 190\n",
      "2 3 185\n",
      "3 0 149\n",
      "3 1 185\n",
      "3 2 185\n",
      "3 3 190\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(spans)):\n",
    "    for j in range(len(spans)):\n",
    "        print(i, j, len(spans[i] & spans[j]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "union = spans[0] | spans[1] | spans[2] | spans[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "print(len(union))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "all_speakers = []\n",
    "all_query_spans = []\n",
    "all_candidate_spans = []\n",
    "\n",
    "for i, utt in enumerate(scene):\n",
    "    speaker = utt['speaker']\n",
    "    transcript = speaker + \" : \" + utt['utterance']\n",
    "    doc = stanza_parser(transcript)\n",
    "\n",
    "    # Load tokens and build instance\n",
    "    sent_tokens = []\n",
    "    for j, item in enumerate(doc):\n",
    "        sent_tokens.append(str(item))\n",
    "    all_sentences.append(sent_tokens)\n",
    "    all_speakers.append(speaker)\n",
    "\n",
    "    # Fetch Noun\n",
    "    noun = set()\n",
    "    for j, item in enumerate(doc):\n",
    "        pos = item.pos_\n",
    "        if pos==\"NOUN\":\n",
    "            noun.add((i, j, j+1))\n",
    "            # print(j, str(item), pos, sent_tokens[j: j+1], noun_pron)\n",
    "    noun.add((j, 0, 1))\n",
    "\n",
    "    # Fetch Prons\n",
    "    pron = set()\n",
    "    for j, item in enumerate(doc):\n",
    "        pos = item.pos_\n",
    "        if pos==\"PRON\":\n",
    "            pron.add((i, j, j+1))\n",
    "            # print(j, str(item), pos, sent_tokens[j: j+1], noun_pron)\n",
    "\n",
    "    # Check Noun Phrase\n",
    "    noun_phrases = set()\n",
    "    for item in doc.noun_chunks:\n",
    "        noun_phrases.add((i, item.start, item.end))\n",
    "        # print(item, item.start, item.end, sent_tokens[item.start: item.end], noun_phrases)\n",
    "\n",
    "    # Add into Query Spans (Noun Phrases + Prons)\n",
    "    query_source = noun_phrases | pron\n",
    "    for (sentenceIndex, startToken, endToken) in query_source:\n",
    "        span = {\n",
    "            \"sentenceIndex\": sentenceIndex,\n",
    "            \"startToken\": startToken,\n",
    "            \"endToken\": endToken\n",
    "        }\n",
    "        all_query_spans.append(span)\n",
    "\n",
    "    # Add into Candidate Spans (Noun Phrases + Prons + Nouns)\n",
    "    candidate_source = query_source | noun\n",
    "    for (sentenceIndex, startToken, endToken) in query_source:\n",
    "        span = {\n",
    "            \"sentenceIndex\": sentenceIndex,\n",
    "            \"startToken\": startToken,\n",
    "            \"endToken\": endToken\n",
    "        }\n",
    "        all_candidate_spans.append(span)\n",
    "\n",
    "output = []\n",
    "for sent in all_sentences:\n",
    "    sent.append(\"\\n\")\n",
    "temp = {\n",
    "    \"sentences\": all_sentences,\n",
    "    \"querySpans\": all_query_spans,\n",
    "    \"candidateSpans\": all_candidate_spans\n",
    "}\n",
    "output.append(temp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "trf = deepcopy(all_candidate_spans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "lg = deepcopy(all_candidate_spans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "md = deepcopy(all_candidate_spans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "sm = deepcopy(all_candidate_spans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "129\n",
      "131\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "spans = [trf, lg, md, sm]\n",
    "for x in spans:\n",
    "    print(len(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a photon', 'it', 'it', 'both slits', 'it', 'its', 'two slits', 'it', 'the plane', 'it', 'it', 'either slit', 'its target', 'both slits', 'it', 'it', 'a plane']\n",
      "['a photon', 'it', 'it', 'both slits', 'it', 'its', 'two slits', 'it', 'the plane', 'it', 'it', 'either slit', 'its target', 'both slits', 'it', 'it', 'a plane']\n",
      "['a photon', 'it', 'it', 'both slits', 'it', 'its', 'two slits', 'it', 'the plane', 'Sheldon', 'it', 'it', 'either slit', 'its target', 'both slits', 'it', 'it', 'a plane']\n",
      "['a photon', 'it', 'it', 'both slits', 'it', 'its', 'two slits', 'it', 'the plane', 'Sheldon', 'it', 'it', 'either slit', 'its target', 'both slits', 'it', 'it', 'a plane']\n",
      "Sheldon :   So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits . If it ’s unobserved it will , however , if it ’s observed after it ’s left the plane but before it hits its target , it will not have gone through both slits . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in spans:\n",
    "    temp = {}\n",
    "    for item in x:\n",
    "        sentenceIndex = item['sentenceIndex']\n",
    "        startToken = item['startToken']\n",
    "        endToken = item['endToken']\n",
    "        if sentenceIndex not in temp:\n",
    "            temp[sentenceIndex] = [\" \".join(all_sentences[sentenceIndex][startToken: endToken])]\n",
    "        else:\n",
    "            temp[sentenceIndex].append(\" \".join(all_sentences[sentenceIndex][startToken: endToken]))\n",
    "    print(temp[0])\n",
    "print(\" \".join(all_sentences[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'your point', 'your']\n",
      "['what', 'your point', 'your']\n",
      "['what', 'your point', 'your']\n",
      "['what', 'your', 'Leonard', 'your point', 'Agreed']\n",
      "Leonard :   Agreed , what ’s your point ? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in spans:\n",
    "    temp = {}\n",
    "    for item in x:\n",
    "        sentenceIndex = item['sentenceIndex']\n",
    "        startToken = item['startToken']\n",
    "        endToken = item['endToken']\n",
    "        if sentenceIndex not in temp:\n",
    "            temp[sentenceIndex] = [\" \".join(all_sentences[sentenceIndex][startToken: endToken])]\n",
    "        else:\n",
    "            temp[sentenceIndex].append(\" \".join(all_sentences[sentenceIndex][startToken: endToken]))\n",
    "    print(temp[1])\n",
    "print(\" \".join(all_sentences[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "with open('test_stanza.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Berkeley Neural Parser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import benepar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /Users/boyuanzheng/nltk_data...\n",
      "[nltk_data]   Unzipping models/benepar_en3.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benepar.download('benepar_en3')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "import benepar, spacy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "<benepar.integrations.spacy_plugin.BeneparComponent at 0x7f9c0cddd4f0>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuanzheng/.conda/envs/multi_coref/lib/python3.8/site-packages/torch/distributions/distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The time for action is now. It's never too late to do something.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for action is now.\n",
      "(S (NP (NP (DT The) (NN time)) (PP (IN for) (NP (NN action)))) (VP (VBZ is) (ADVP (RB now))) (. .))\n",
      "('S',)\n",
      "The time for action\n"
     ]
    }
   ],
   "source": [
    "sent = list(doc.sents)[0]\n",
    "print(sent)\n",
    "print(sent._.parse_string)\n",
    "print(sent._.labels)\n",
    "print(list(sent._.children)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for action is now. ('S',)\n",
      "The time for action ('NP',)\n",
      "The time ('NP',)\n",
      "The ()\n",
      "time ()\n",
      "for action ('PP',)\n",
      "for ()\n",
      "action ('NP',)\n",
      "is now ('VP',)\n",
      "is ()\n",
      "now ('ADVP',)\n",
      ". ()\n"
     ]
    }
   ],
   "source": [
    "for item in sent._.constituents:\n",
    "    print(item, item._.labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n"
     ]
    }
   ],
   "source": [
    "for item in sent._.labels:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for action\n",
      "is now\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for item in sent._.children:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 10:59:02 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2022-02-28 10:59:02 INFO: Use device: cpu\n",
      "2022-02-28 10:59:02 INFO: Loading: tokenize\n",
      "2022-02-28 10:59:02 INFO: Loading: pos\n",
      "2022-02-28 10:59:02 INFO: Loading: constituency\n",
      "2022-02-28 10:59:02 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, pos, constituency')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}