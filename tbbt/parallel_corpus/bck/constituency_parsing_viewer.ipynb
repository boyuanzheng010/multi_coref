{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install ipydagred3\n",
    "# install benepar and download benepar_fr\n",
    "\n",
    "import ipydagred3\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy import displacy\n",
    "from benepar.spacy_plugin import BeneparComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mbenepar_en\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> benepar.download('benepar_en')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mmodels/benepar_en\u001B[0m\n\n  Searched in:\n    - '/Users/boyuanzheng/nltk_data'\n    - '/Users/boyuanzheng/.conda/envs/multi_coref/nltk_data'\n    - '/Users/boyuanzheng/.conda/envs/multi_coref/share/nltk_data'\n    - '/Users/boyuanzheng/.conda/envs/multi_coref/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/w9/673mfrb56v7dvx7hhvw7s6240000gn/T/ipykernel_33717/2384016660.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mnlp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspacy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"en_core_web_sm\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mnlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_pipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mBeneparComponent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'benepar_en'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.conda/envs/multi_coref/lib/python3.8/site-packages/benepar/integrations/spacy_plugin.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, name, subbatch_max_tokens, disable_tagger, batch_size)\u001B[0m\n\u001B[1;32m    114\u001B[0m             \u001B[0mbatch_size\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mdeprecated\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mignored\u001B[0m\u001B[0;34m;\u001B[0m \u001B[0muse\u001B[0m \u001B[0msubbatch_max_tokens\u001B[0m \u001B[0minstead\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m         \"\"\"\n\u001B[0;32m--> 116\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_parser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_trained_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    117\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_parser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/multi_coref/lib/python3.8/site-packages/benepar/integrations/downloader.py\u001B[0m in \u001B[0;36mload_trained_model\u001B[0;34m(model_name_or_path)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mload_trained_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m     \u001B[0mmodel_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlocate_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_name_or_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m     \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparse_chart\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mChartParser\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mChartParser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_trained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/multi_coref/lib/python3.8/site-packages/benepar/integrations/downloader.py\u001B[0m in \u001B[0;36mlocate_model\u001B[0;34m(name)\u001B[0m\n\u001B[1;32m     25\u001B[0m             \u001B[0marg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"nltk.download\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"benepar.download\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m     \u001B[0;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Can't find {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mbenepar_en\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> benepar.download('benepar_en')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mmodels/benepar_en\u001B[0m\n\n  Searched in:\n    - '/Users/boyuanzheng/nltk_data'\n    - '/Users/boyuanzheng/.conda/envs/multi_coref/nltk_data'\n    - '/Users/boyuanzheng/.conda/envs/multi_coref/share/nltk_data'\n    - '/Users/boyuanzheng/.conda/envs/multi_coref/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(BeneparComponent('benepar_en'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<benepar.integrations.spacy_plugin.BeneparComponent at 0x7fa6354f6130>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlp = spacy.load('en_core_web_md')\n",
    "# nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to complete this dict and PR\n",
    "\n",
    "tooltips = {\n",
    "    \"NP-SUJ\": \"Nom propre sujet\",\n",
    "    \"PONCT\": \"Ponctuation\",\n",
    "    \"VN\": \"Noyau verbal\",\n",
    "    \"ADV\": \"Adverbe\",\n",
    "    \"AP-ATS\": \"attribut du sujet\",\n",
    "    \"NC\": \"Nom commun\",\n",
    "    \"DET\": \"Déterminant\",\n",
    "    \"V\": \"Verb\",\n",
    "    \"SENT\": \"Phrase racine\",\n",
    "    \"NP-OBJ\": \"Nom propre, objet\",\n",
    "    \"Srel\": \"Subordonnée relative\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <__main__.ConstituencyParser object at 0x7fa6352b55e0> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/w9/673mfrb56v7dvx7hhvw7s6240000gn/T/ipykernel_33717/2660925383.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;31m# add constituencyParser to spacy pipeline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[0mconstituencyParser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mConstituencyParser\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 131\u001B[0;31m \u001B[0mnlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_pipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconstituencyParser\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/multi_coref/lib/python3.8/site-packages/spacy/language.py\u001B[0m in \u001B[0;36madd_pipe\u001B[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001B[0m\n\u001B[1;32m    771\u001B[0m             \u001B[0mbad_val\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrepr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfactory_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    772\u001B[0m             \u001B[0merr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mErrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mE966\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcomponent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbad_val\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 773\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    774\u001B[0m         \u001B[0mname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mfactory_name\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    775\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcomponent_names\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <__main__.ConstituencyParser object at 0x7fa6352b55e0> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "try:\n",
    "    nlp.remove_pipe(\"constituency_parser\")\n",
    "    Span.remove_extension(\"constituency\")\n",
    "    Span.remove_extension(\"show_constituency\")\n",
    "    Span.remove_extension(\"search_constituency\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "def _flatten(l):\n",
    "    for el in l:\n",
    "        if isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes)):\n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el\n",
    "\n",
    "flatten = lambda l: list(_flatten(l))\n",
    "\n",
    "class ConstituencyParser():\n",
    "    name = \"constituency_parser\"\n",
    "\n",
    "    def __init__(self):\n",
    "        Span.set_extension(\"constituency\", default=None)\n",
    "        Span.set_extension(\"show_constituency\", default=None)\n",
    "        Span.set_extension(\"search_constituency\", default=None)\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        for sent in doc.sents:\n",
    "            nodes, parsed  = self.processConstituency(sent._.parse_string)\n",
    "            sent._.set(\"show_constituency\", lambda: self.showDependencyGraph(nodes, parsed))\n",
    "            sent._.set(\"search_constituency\", lambda _type: self.find(_type, parsed, sent))\n",
    "        return doc\n",
    "\n",
    "    def processConstituency(self, pStr):\n",
    "        nodes = []\n",
    "        cur = \"\";\n",
    "        stack = [];\n",
    "        nid = 0;\n",
    "        wordIndex = 0\n",
    "        for i in range(len(pStr)):\n",
    "            if(pStr[i] == ' ' or pStr[i] == '\\n'):\n",
    "                if (len(cur) > 0): \n",
    "                    newNode = {\n",
    "                        \"nodeID\": nid,\n",
    "                        \"nodeType\": \"Internal\",\n",
    "                        \"name\": cur,\n",
    "                        \"children\": []\n",
    "                    }\n",
    "                    cur = \"\";\n",
    "                    nid += 1;\n",
    "                    if (len(stack) > 0):\n",
    "                        stack[len(stack) - 1][\"children\"].append(newNode);\n",
    "                    stack.append(newNode);\n",
    "                    nodes.append(newNode)\n",
    "            elif pStr[i] == ')':\n",
    "                if (len(cur) > 0):\n",
    "                    newNode = {\n",
    "                        \"nodeID\": nid,\n",
    "                        \"nodeType\": \"Leaf\",\n",
    "                        \"name\": cur,\n",
    "                        \"wordIndex\": wordIndex,\n",
    "                        \"children\": []\n",
    "                    }\n",
    "                    cur = \"\";\n",
    "                    nid += 1;\n",
    "                    wordIndex += 1;\n",
    "                    stack[len(stack) - 1][\"children\"].append(newNode);\n",
    "                    nodes.append(newNode)\n",
    "                    stack.pop();\n",
    "                else:\n",
    "                    if (len(stack) == 1):\n",
    "                        root = stack[0]\n",
    "                    stack.pop();\n",
    "            elif pStr[i] == '(':\n",
    "                continue\n",
    "            else:\n",
    "                cur = cur + pStr[i];\n",
    "        return nodes, root\n",
    "\n",
    "    def showDependencyGraph(self, nodes, parsed):\n",
    "        g = ipydagred3.Graph()\n",
    "        for node in nodes:\n",
    "            g.setNode(str(node[\"nodeID\"]),\n",
    "                      label=node[\"name\"],\n",
    "                      tooltip=tooltips[node[\"name\"]] if node[\"name\"] in tooltips else node[\"name\"],\n",
    "                      rx=5,\n",
    "                      ry=5,\n",
    "                      style=\"fill: \" + (\"white\" if len(node[\"children\"]) else \"#00bcd4\"));\n",
    "\n",
    "        def setEdge(parent):\n",
    "            for i in range(len(parent[\"children\"])):\n",
    "                g.setEdge(str(parent[\"nodeID\"]), str(parent[\"children\"][i][\"nodeID\"]))\n",
    "                setEdge(parent[\"children\"][i])\n",
    "\n",
    "        setEdge(parsed)\n",
    "        widget = ipydagred3.DagreD3Widget(graph=g)\n",
    "        return display(widget)\n",
    "\n",
    "    def getWordIndex(self, node):\n",
    "        return [self.getWordIndex(childNode) for childNode in node[\"children\"]] if node[\"nodeType\"] != \"Leaf\" else node[\"wordIndex\"]\n",
    "\n",
    "    def getSpan(self, node):\n",
    "        return\n",
    "    \n",
    "    def getString(self, node):\n",
    "        return ' '.join([self.getString(childNode) for childNode in node[\"children\"]]) if node[\"nodeType\"] != \"Leaf\" else node[\"name\"]\n",
    "\n",
    "    def search(self, _types, node):\n",
    "        fltn = lambda l: [item for sublist in l for item in sublist]\n",
    "        types = [child for child in node[\"children\"] if child[\"name\"] in _types]\n",
    "        others = fltn([self.search(_types, child) for child in node[\"children\"]])\n",
    "        return types+others\n",
    "\n",
    "    def find(self, _types, node, sent):\n",
    "        spans = []\n",
    "        strings = []\n",
    "        for _node in self.search(_types, node):\n",
    "            indexes = flatten([x for x in self.getWordIndex(_node)])\n",
    "            span = sent[min(indexes): max(indexes)+1]\n",
    "            spans.append(span)\n",
    "            strings.append(self.getString(_node))\n",
    "        return spans\n",
    "        \n",
    "\n",
    "# add constituencyParser to spacy pipeline\n",
    "constituencyParser = ConstituencyParser()\n",
    "nlp.add_pipe(constituencyParser, last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "\n",
      "SENTENCE  :  le petit chat joue dans le grand jardin vert.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuanzheng/.conda/envs/multi_coref/lib/python3.8/site-packages/torch/distributions/distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/w9/673mfrb56v7dvx7hhvw7s6240000gn/T/ipykernel_33717/299285256.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"=========================\\n\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"SENTENCE  : \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msent\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"SUBJECTs  : \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msearch_constituency\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"NP-SUJ\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"OBJECTs   : \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msearch_constituency\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"NP-OBJ\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"VERBS     : \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msearch_constituency\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"VN\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"le petit chat joue dans le grand jardin vert.\")\n",
    "sent = list(doc.sents)[0]\n",
    "print(\"=========================\\n\")\n",
    "print(\"SENTENCE  : \", sent)\n",
    "print(\"SUBJECTs  : \", sent._.search_constituency([\"NP-SUJ\"]))\n",
    "print(\"OBJECTs   : \", sent._.search_constituency([\"NP-OBJ\"]))\n",
    "print(\"VERBS     : \", sent._.search_constituency([\"VN\"]))\n",
    "sent._.show_constituency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}